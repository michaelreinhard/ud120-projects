{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final project restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "                                                \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from tester import test_classifier\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "df = pd.DataFrame(data_dict)\n",
    "df = df.transpose()\n",
    "df = df.drop('email_address', axis=1)\n",
    "df = df.astype(float)\n",
    "df = df.drop('TOTAL')\n",
    "df = df.drop(\"THE TRAVEL AGENCY IN THE PARK\")\n",
    "df = df.drop(\"loan_advances\", axis=1)\n",
    "features_list = list(df.columns)\n",
    "features_list.remove('poi')\n",
    "features = df[features_list]\n",
    "labels = df['poi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    cross_validation.train_test_split(features, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Pipeline: median imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier\n",
      "Imputer: median\n",
      "Normalize: StandardScaler\n",
      "PCA: dimensions unspecified\n",
      "\n",
      "Accuracy Score: 0.863636363636\n",
      "\n",
      "Recall: 0.4\n",
      "\n",
      "Precision: 0.4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "Pipeline = Pipeline([\n",
    "        ('imp', Imputer(missing_values='NaN', strategy='median')),\n",
    "        ('std', StandardScaler()),\n",
    "        ('pca', PCA(n_components=9)),\n",
    "        ('clf', DecisionTreeClassifier(random_state = 53))\n",
    "    ])\n",
    "\n",
    "pipeline = Pipeline.fit(features_train, labels_train)\n",
    "\n",
    "print \"\"\"DecisionTreeClassifier\\nImputer: median\\nNormalize: StandardScaler\\nPCA: dimensions unspecified\"\"\"\n",
    "print \"\\nAccuracy Score:\", pipeline.score(features_test, labels_test)\n",
    "\n",
    "clf_pipeline_pred = pipeline.predict(features_test)\n",
    "print \"\\nRecall:\", recall_score(labels_test, clf_pipeline_pred)\n",
    "print \"\\nPrecision:\", precision_score(labels_test, clf_pipeline_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "Ok, so this looks pretty good. I am going to go with this. 0.6 and 0.43. Not bad. Now I am going to go with this, tweeking the parameters on this model. Then I will worry about how to get this into the Udacity grader. \n",
    "\n",
    "First, I am not going to re-import all the modules. \n",
    "\n",
    "Second, I am not going to print out all the pipeline parameters. \n",
    "\n",
    "Third, I don't know what the PCA is doing because the pipeline does not assign a set number of dimensions for the pca to limit the data to. I really don't know how that is working. It is kind of strange. Without limiting it to a set number of dimensions I don't know why it would be having any effect at all? In fact, I am going to test that hypothesis by taking out the PCA and seeing if the result changes. \n",
    "\n",
    "The first thing that I discovered is that the pipeline object is not callable, or at least that is the error I got. So, I put the import statement back in the code and ran it again and everything was fine. Ok, so, I will keep that in the code evertime. I guess the big import statement at the top of the code was kind of a waste of time. \n",
    "\n",
    "Now, I am going to take out the PCA and see what, if any, difference it makes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree with no PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "DecisionTreeClassifier\n",
      "Imputer: median\n",
      "Normalize: StandardScaler\n",
      "PCA: NONE\n",
      "\n",
      "Accuracy Score: 0.75\n",
      "\n",
      "Recall: 0.0\n",
      "\n",
      "Precision: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "Pipeline = Pipeline([\n",
    "        ('imp', Imputer(missing_values='NaN', strategy='median')),\n",
    "        ('std', StandardScaler()),\n",
    "        ('clf', DecisionTreeClassifier(random_state = 53))\n",
    "    ])\n",
    "\n",
    "pipeline = Pipeline.fit(features_train, labels_train)\n",
    "\n",
    "# print \"\\nPipeline parameters:\"\n",
    "# pprint(pipeline.get_params())\n",
    "\n",
    "print \"\\n\"\n",
    "print \"\"\"DecisionTreeClassifier\\nImputer: median\\nNormalize: StandardScaler\\nPCA: NONE\"\"\"\n",
    "print \"\\nAccuracy Score:\", pipeline.score(features_test, labels_test)\n",
    "\n",
    "clf_pipeline_pred = pipeline.predict(features_test)\n",
    "print \"\\nRecall:\", recall_score(labels_test, clf_pipeline_pred)\n",
    "print \"\\nPrecision:\", precision_score(labels_test, clf_pipeline_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "Ok, without the PCA the accuracy goes down to 0.75 and the recall and precision drop to zero. So, the PCA has to stay in, I guess. \n",
    "\n",
    "Now I am going to run it with the pca in but with the scalar changed to Robust since the data is very sparse in some places. Also, I am going to stop printing out the parameters of the pipeline since I know what they are. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree with PCA, median and Robust Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier\n",
      "Imputer: median\n",
      "Normalize: RobustScaler\n",
      "PCA: dimensions unspecified\n",
      "\n",
      "Accuracy Score: 0.840909090909\n",
      "\n",
      "Recall: 0.2\n",
      "\n",
      "Precision: 0.25\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "Pipeline = Pipeline([\n",
    "        ('imp', Imputer(missing_values='NaN', strategy='median')),\n",
    "        ('std', RobustScaler()),\n",
    "        ('pca', PCA()),\n",
    "        ('clf', DecisionTreeClassifier(random_state = 53))\n",
    "    ])\n",
    "\n",
    "pipeline = Pipeline.fit(features_train, labels_train)\n",
    "\n",
    "\n",
    "print \"\"\"DecisionTreeClassifier\\nImputer: median\\nNormalize: RobustScaler\\nPCA: dimensions unspecified\"\"\"\n",
    "print \"\\nAccuracy Score:\", pipeline.score(features_test, labels_test)\n",
    "\n",
    "clf_pipeline_pred = pipeline.predict(features_test)\n",
    "print \"\\nRecall:\", recall_score(labels_test, clf_pipeline_pred)\n",
    "print \"\\nPrecision:\", precision_score(labels_test, clf_pipeline_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "Massive drop in preformance with the Robust scaler. No more of that. Now I am going to try a different strategy with imputation, imputing the most_frequent value instead of the median and going back to the StadardScaler. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Pipeline with most_frequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier\n",
      "Imputer: most_frequent\n",
      "Normalize: StandardScaler\n",
      "PCA: dimensions unspecified\n",
      "\n",
      "Accuracy Score: 0.886363636364\n",
      "\n",
      "Recall: 0.4\n",
      "\n",
      "Precision: 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "Pipeline = Pipeline([\n",
    "        ('imp', Imputer(missing_values='NaN', strategy='most_frequent')),\n",
    "        ('std', StandardScaler()),\n",
    "        ('pca', PCA()),\n",
    "        ('clf', DecisionTreeClassifier(random_state = 53))\n",
    "    ])\n",
    "\n",
    "pipeline = Pipeline.fit(features_train, labels_train)\n",
    "print \"\"\"DecisionTreeClassifier\\nImputer: most_frequent\\nNormalize: StandardScaler\\nPCA: dimensions unspecified\"\"\"\n",
    "print \"\\nAccuracy Score:\", pipeline.score(features_test, labels_test)\n",
    "\n",
    "clf_pipeline_pred = pipeline.predict(features_test)\n",
    "print \"\\nRecall:\", recall_score(labels_test, clf_pipeline_pred)\n",
    "print \"\\nPrecision:\", precision_score(labels_test, clf_pipeline_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "Ok, that is good, but it looks like the first model is still better. The first model with mean imputation for the missing values had the recall score up at 0.6 while the precision score was a bit lower at 0.43. Since recall is the more important dimension we will stick with that. \n",
    "\n",
    "Now that we have a good score on the recall with the decision trees lets look and see what we can do when we try out different numbers of principle components. To do that I am going to have to introduce a grid search. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search with PCA-2 to 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters from the grid search:{'pca__n_components': 9}\n",
      " None\n",
      "\n",
      "Best estimator accuracy: 0.863636363636\n",
      "\n",
      "\n",
      "Recall Score: 0.4\n",
      "\n",
      "\n",
      "Precision Score: 0.4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "Pipeline = Pipeline([\n",
    "        ('imp', Imputer(missing_values='NaN', strategy='median')),\n",
    "        ('std', StandardScaler()),\n",
    "        ('pca', PCA()),\n",
    "        ('clf', DecisionTreeClassifier(random_state = 53))\n",
    "    ])\n",
    "\n",
    "x = [x for x in range(2,19)]\n",
    "param_grid = {'pca__n_components': x}\n",
    "\n",
    "# pipeline = Pipeline.fit(features_train, labels_train)\n",
    "\n",
    "gridCV_object = GridSearchCV(estimator = Pipeline, \n",
    "                             param_grid = param_grid)\n",
    "\n",
    "gridCV_object.fit(features_train, labels_train)\n",
    "\n",
    "print \"Best parameters from the grid search:\", pprint(gridCV_object.best_params_)\n",
    "\n",
    "clf_gridCV = gridCV_object.best_estimator_\n",
    "\n",
    "print \"\\nBest estimator accuracy:\", clf_gridCV.score(features_test, labels_test)\n",
    "\n",
    "clf_gridCV_pred = clf_gridCV.predict(features_test)\n",
    "\n",
    "print \"\\n\\nRecall Score:\", recall_score(labels_test, clf_gridCV_pred)\n",
    "print \"\\n\\nPrecision Score:\", precision_score(labels_test, clf_gridCV_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion: adding grid_search PCA\n",
    "This is not really improving. It has gone down instead of up. The precision score has actually gone down from 0.5 to 0.4. It really sucks. I don't know why I am doing this. And I am sure that once I stick it into the Udacity grader it will get worse. It\n",
    "\n",
    "The next thing I will try is putting the imputation in the param_grid. This should not make any difference but I will see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search for PCA and Imputation Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters from the grid search:{'imp__strategy': 'most_frequent', 'pca__n_components': 5}\n",
      " None\n",
      "\n",
      "Best estimator accuracy: 0.818181818182\n",
      "\n",
      "\n",
      "Recall Score: 0.2\n",
      "\n",
      "\n",
      "Precision Score: 0.2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "Pipeline = Pipeline([\n",
    "        ('imp', Imputer()),\n",
    "        ('std', StandardScaler()),\n",
    "        ('pca', PCA()),\n",
    "        ('clf', DecisionTreeClassifier(random_state = 53))\n",
    "    ])\n",
    "\n",
    "x = [x for x in range(2,19)]\n",
    "param_grid = {'imp__strategy':['median', 'most_frequent'],\n",
    "              'pca__n_components': x}\n",
    "\n",
    "# pipeline = Pipeline.fit(features_train, labels_train)\n",
    "\n",
    "gridCV_object = GridSearchCV(estimator = Pipeline, \n",
    "                             param_grid = param_grid)\n",
    "\n",
    "gridCV_object.fit(features_train, labels_train)\n",
    "\n",
    "print \"Best parameters from the grid search:\", pprint(gridCV_object.best_params_)\n",
    "\n",
    "clf_gridCV = gridCV_object.best_estimator_\n",
    "\n",
    "print \"\\nBest estimator accuracy:\", clf_gridCV.score(features_test, labels_test)\n",
    "\n",
    "clf_gridCV_pred = clf_gridCV.predict(features_test)\n",
    "\n",
    "print \"\\n\\nRecall Score:\", recall_score(labels_test, clf_gridCV_pred)\n",
    "print \"\\n\\nPrecision Score:\", precision_score(labels_test, clf_gridCV_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "Ok, this is driving me insane. I have put in the possibility of using median or most_frequent as an imputation strategy and it chooses most_frequent and actually does worse on all three outcomes. This is totally rediculous. It is getting worse the more choices I give it! \n",
    "\n",
    "I have been on the discussion board and I have seen the coaches say that you can't make it do worse by giving it more choices. I mean, how could it? Also, I have set the random_state variable to a fixed number, 53, so that can't be it. What is going on? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Feature-Data Reimport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "df = pd.DataFrame(data_dict)\n",
    "df = df.transpose()\n",
    "df = df.drop('email_address', axis=1)\n",
    "df = df.astype(float)\n",
    "df = df.drop('TOTAL')\n",
    "df = df.drop(\"THE TRAVEL AGENCY IN THE PARK\")\n",
    "df = df.drop(\"loan_advances\", axis=1)\n",
    "#new feature\n",
    "df['deferred_ratio'] = df['deferred_income']/(df['total_payments'] + 1)\n",
    "features_list = list(df.columns)\n",
    "features_list.remove('poi')\n",
    "features = df[features_list]\n",
    "labels = df['poi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    cross_validation.train_test_split(features, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Pipeline-median imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier\n",
      "Imputer: median\n",
      "Normalize: StandardScaler\n",
      "PCA: dimensions unspecified\n",
      "\n",
      "Accuracy Score: 0.795454545455\n",
      "\n",
      "Recall: 0.4\n",
      "\n",
      "Precision: 0.25\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "Pipeline = Pipeline([\n",
    "        ('imp', Imputer(missing_values='NaN', strategy='median')),\n",
    "        ('std', StandardScaler()),\n",
    "        ('pca', PCA()),\n",
    "        ('clf', DecisionTreeClassifier(random_state = 53))\n",
    "    ])\n",
    "\n",
    "pipeline = Pipeline.fit(features_train, labels_train)\n",
    "\n",
    "print \"\"\"DecisionTreeClassifier\\nImputer: median\\nNormalize: StandardScaler\\nPCA: dimensions unspecified\"\"\"\n",
    "print \"\\nAccuracy Score:\", pipeline.score(features_test, labels_test)\n",
    "\n",
    "clf_pipeline_pred = pipeline.predict(features_test)\n",
    "print \"\\nRecall:\", recall_score(labels_test, clf_pipeline_pred)\n",
    "print \"\\nPrecision:\", precision_score(labels_test, clf_pipeline_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "So one thing I have found out is that the new feature I created of deferred compensation actually makes things worse. So, I am going to get rid of it. The question is do I want to get rid of it by reimporting the data set or taking the column of data out. \n",
    "\n",
    "I am just going to take it out and recalculate the training and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df = df.drop(\"deferred_ratio\", axis=1)\n",
    "features_list = list(df.columns)\n",
    "features_list.remove('poi')\n",
    "features = df[features_list]\n",
    "labels = df['poi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    cross_validation.train_test_split(features, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier\n",
      "Imputer: median\n",
      "Normalize: StandardScaler\n",
      "PCA: dimensions unspecified\n",
      "\n",
      "Accuracy Score: 0.795454545455\n",
      "\n",
      "Recall: 0.4\n",
      "\n",
      "Precision: 0.25\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "Pipeline = Pipeline([\n",
    "        ('imp', Imputer(missing_values='NaN', strategy='median')),\n",
    "        ('std', StandardScaler()),\n",
    "        ('pca', PCA()),\n",
    "        ('clf', DecisionTreeClassifier(random_state = 53))\n",
    "    ])\n",
    "\n",
    "pipeline = Pipeline.fit(features_train, labels_train)\n",
    "\n",
    "print \"\"\"DecisionTreeClassifier\\nImputer: median\\nNormalize: StandardScaler\\nPCA: dimensions unspecified\"\"\"\n",
    "print \"\\nAccuracy Score:\", pipeline.score(features_test, labels_test)\n",
    "\n",
    "clf_pipeline_pred = pipeline.predict(features_test)\n",
    "print \"\\nRecall:\", recall_score(labels_test, clf_pipeline_pred)\n",
    "print \"\\nPrecision:\", precision_score(labels_test, clf_pipeline_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bonus</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>expenses</th>\n",
       "      <th>from_messages</th>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>other</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>salary</th>\n",
       "      <th>shared_receipt_with_poi</th>\n",
       "      <th>to_messages</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>total_stock_value</th>\n",
       "      <th>deferred_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>COX DAVID</th>\n",
       "      <td>800000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-41250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>117551</td>\n",
       "      <td>27861</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>494</td>\n",
       "      <td>378082</td>\n",
       "      <td>NaN</td>\n",
       "      <td>314288</td>\n",
       "      <td>71</td>\n",
       "      <td>102</td>\n",
       "      <td>1101393</td>\n",
       "      <td>495633</td>\n",
       "      <td>-0.037453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ECHOLS JOHN B</th>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>601438</td>\n",
       "      <td>21530</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2234774</td>\n",
       "      <td>53775</td>\n",
       "      <td>407503</td>\n",
       "      <td>NaN</td>\n",
       "      <td>182245</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2692324</td>\n",
       "      <td>1008941</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MARTIN AMANDA K</th>\n",
       "      <td>NaN</td>\n",
       "      <td>85430</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2070306</td>\n",
       "      <td>8211</td>\n",
       "      <td>230</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>5145434</td>\n",
       "      <td>2818454</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>349487</td>\n",
       "      <td>477</td>\n",
       "      <td>1522</td>\n",
       "      <td>8407016</td>\n",
       "      <td>2070306</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHRISTODOULOU DIOMEDES</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5127155</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>950730</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6077885</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLINE KENNETH W</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>662086</td>\n",
       "      <td>-472568</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>189518</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHAN RONNIE</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-98784</td>\n",
       "      <td>98784</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32460</td>\n",
       "      <td>-32460</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SCRIMSHAW MATTHEW</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>759557</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>759557</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HUGHES JAMES A</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>754966</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34</td>\n",
       "      <td>35</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>363428</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>589</td>\n",
       "      <td>719</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1118394</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FITZGERALD JAY L</th>\n",
       "      <td>350000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>664461</td>\n",
       "      <td>23870</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>556416</td>\n",
       "      <td>285414</td>\n",
       "      <td>956775</td>\n",
       "      <td>NaN</td>\n",
       "      <td>199157</td>\n",
       "      <td>723</td>\n",
       "      <td>936</td>\n",
       "      <td>1414857</td>\n",
       "      <td>1621236</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BOWEN JR RAYMOND M</th>\n",
       "      <td>1350000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-833</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65907</td>\n",
       "      <td>27</td>\n",
       "      <td>140</td>\n",
       "      <td>15</td>\n",
       "      <td>974293</td>\n",
       "      <td>1621</td>\n",
       "      <td>252055</td>\n",
       "      <td>NaN</td>\n",
       "      <td>278601</td>\n",
       "      <td>1593</td>\n",
       "      <td>1858</td>\n",
       "      <td>2669589</td>\n",
       "      <td>252055</td>\n",
       "      <td>-0.000312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEYER ROCKFORD G</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1848227</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>493489</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>462384</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "      <td>232</td>\n",
       "      <td>1848227</td>\n",
       "      <td>955873</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DURAN WILLIAM D</th>\n",
       "      <td>750000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1451869</td>\n",
       "      <td>25785</td>\n",
       "      <td>12</td>\n",
       "      <td>106</td>\n",
       "      <td>3</td>\n",
       "      <td>1105218</td>\n",
       "      <td>1568</td>\n",
       "      <td>189041</td>\n",
       "      <td>NaN</td>\n",
       "      <td>210692</td>\n",
       "      <td>599</td>\n",
       "      <td>904</td>\n",
       "      <td>2093263</td>\n",
       "      <td>1640910</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CAUSEY RICHARD A</th>\n",
       "      <td>1000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-235000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30674</td>\n",
       "      <td>49</td>\n",
       "      <td>58</td>\n",
       "      <td>12</td>\n",
       "      <td>350000</td>\n",
       "      <td>307895</td>\n",
       "      <td>2502063</td>\n",
       "      <td>NaN</td>\n",
       "      <td>415189</td>\n",
       "      <td>1585</td>\n",
       "      <td>1892</td>\n",
       "      <td>1868758</td>\n",
       "      <td>2502063</td>\n",
       "      <td>-0.125752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HUMPHREY GENE E</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2964506</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2282768</td>\n",
       "      <td>4994</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>130724</td>\n",
       "      <td>119</td>\n",
       "      <td>128</td>\n",
       "      <td>3100224</td>\n",
       "      <td>2282768</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LEFF DANIEL P</th>\n",
       "      <td>1000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>63</td>\n",
       "      <td>67</td>\n",
       "      <td>14</td>\n",
       "      <td>1387399</td>\n",
       "      <td>3083</td>\n",
       "      <td>360528</td>\n",
       "      <td>NaN</td>\n",
       "      <td>273746</td>\n",
       "      <td>2672</td>\n",
       "      <td>2822</td>\n",
       "      <td>2664228</td>\n",
       "      <td>360528</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WODRASKA JOHN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>189583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>189583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FREVERT MARK A</th>\n",
       "      <td>2000000</td>\n",
       "      <td>6426990</td>\n",
       "      <td>-3367011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10433518</td>\n",
       "      <td>86987</td>\n",
       "      <td>21</td>\n",
       "      <td>242</td>\n",
       "      <td>6</td>\n",
       "      <td>1617011</td>\n",
       "      <td>7427621</td>\n",
       "      <td>4188667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1060932</td>\n",
       "      <td>2979</td>\n",
       "      <td>3275</td>\n",
       "      <td>17252530</td>\n",
       "      <td>14622185</td>\n",
       "      <td>-0.195160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEFFNER JOSEPH M</th>\n",
       "      <td>600000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17378</td>\n",
       "      <td>41626</td>\n",
       "      <td>74</td>\n",
       "      <td>115</td>\n",
       "      <td>4</td>\n",
       "      <td>335349</td>\n",
       "      <td>25553</td>\n",
       "      <td>141833</td>\n",
       "      <td>NaN</td>\n",
       "      <td>206121</td>\n",
       "      <td>552</td>\n",
       "      <td>714</td>\n",
       "      <td>1208649</td>\n",
       "      <td>159211</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RICE KENNETH D</th>\n",
       "      <td>1750000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3504386</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19794175</td>\n",
       "      <td>46950</td>\n",
       "      <td>18</td>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>1617011</td>\n",
       "      <td>174839</td>\n",
       "      <td>2748364</td>\n",
       "      <td>NaN</td>\n",
       "      <td>420636</td>\n",
       "      <td>864</td>\n",
       "      <td>905</td>\n",
       "      <td>505050</td>\n",
       "      <td>22542539</td>\n",
       "      <td>-6.938677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KOENIG MARK E</th>\n",
       "      <td>700000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>671737</td>\n",
       "      <td>127017</td>\n",
       "      <td>61</td>\n",
       "      <td>53</td>\n",
       "      <td>15</td>\n",
       "      <td>300000</td>\n",
       "      <td>150458</td>\n",
       "      <td>1248318</td>\n",
       "      <td>NaN</td>\n",
       "      <td>309946</td>\n",
       "      <td>2271</td>\n",
       "      <td>2374</td>\n",
       "      <td>1587421</td>\n",
       "      <td>1920055</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WHALLEY LAWRENCE G</th>\n",
       "      <td>3000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3282960</td>\n",
       "      <td>57838</td>\n",
       "      <td>556</td>\n",
       "      <td>186</td>\n",
       "      <td>24</td>\n",
       "      <td>808346</td>\n",
       "      <td>301026</td>\n",
       "      <td>2796177</td>\n",
       "      <td>NaN</td>\n",
       "      <td>510364</td>\n",
       "      <td>3920</td>\n",
       "      <td>6019</td>\n",
       "      <td>4677574</td>\n",
       "      <td>6079137</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DETMERING TIMOTHY J</th>\n",
       "      <td>425000</td>\n",
       "      <td>875307</td>\n",
       "      <td>-775241</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2027865</td>\n",
       "      <td>52255</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>415657</td>\n",
       "      <td>1105</td>\n",
       "      <td>315068</td>\n",
       "      <td>-315068</td>\n",
       "      <td>210500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1204583</td>\n",
       "      <td>2027865</td>\n",
       "      <td>-0.643576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LEMAISTRE CHARLES</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-25000</td>\n",
       "      <td>112492</td>\n",
       "      <td>412878</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>87492</td>\n",
       "      <td>412878</td>\n",
       "      <td>-0.285737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YEAGER F SCOTT</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8308552</td>\n",
       "      <td>53947</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>147950</td>\n",
       "      <td>3576206</td>\n",
       "      <td>NaN</td>\n",
       "      <td>158403</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>360300</td>\n",
       "      <td>11884758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HAYSLETT RODERICK J</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1061</td>\n",
       "      <td>35</td>\n",
       "      <td>38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>346663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>571</td>\n",
       "      <td>2649</td>\n",
       "      <td>NaN</td>\n",
       "      <td>346663</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WINOKUR JR. HERBERT S</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-25000</td>\n",
       "      <td>108579</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1413</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84992</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.294142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WESTFAHL RICHARD K</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-10800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>256191</td>\n",
       "      <td>401130</td>\n",
       "      <td>384930</td>\n",
       "      <td>NaN</td>\n",
       "      <td>63744</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>762135</td>\n",
       "      <td>384930</td>\n",
       "      <td>-0.014171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GILLIS JOHN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9803</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75838</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85641</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BAZELIDES PHILIP J</th>\n",
       "      <td>NaN</td>\n",
       "      <td>684694</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1599641</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93750</td>\n",
       "      <td>874</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80818</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>860136</td>\n",
       "      <td>1599641</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOWRY CHARLES P</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>372205</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>153686</td>\n",
       "      <td>-153686</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>372205</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MCCLELLAN GEORGE</th>\n",
       "      <td>900000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-125000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>506765</td>\n",
       "      <td>228763</td>\n",
       "      <td>49</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51587</td>\n",
       "      <td>441096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>263413</td>\n",
       "      <td>1469</td>\n",
       "      <td>1744</td>\n",
       "      <td>1318763</td>\n",
       "      <td>947861</td>\n",
       "      <td>-0.094786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAYLOR MITCHELL S</th>\n",
       "      <td>600000</td>\n",
       "      <td>227449</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3181250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>563798</td>\n",
       "      <td>NaN</td>\n",
       "      <td>265214</td>\n",
       "      <td>300</td>\n",
       "      <td>533</td>\n",
       "      <td>1092663</td>\n",
       "      <td>3745048</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CARTER REBECCA C</th>\n",
       "      <td>300000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-159792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "      <td>29</td>\n",
       "      <td>7</td>\n",
       "      <td>75000</td>\n",
       "      <td>540</td>\n",
       "      <td>307301</td>\n",
       "      <td>-307301</td>\n",
       "      <td>261809</td>\n",
       "      <td>196</td>\n",
       "      <td>312</td>\n",
       "      <td>477557</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.334602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GRAY RODNEY</th>\n",
       "      <td>NaN</td>\n",
       "      <td>93585</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>365625</td>\n",
       "      <td>680833</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6615</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1146658</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KITCHEN LOUISE</th>\n",
       "      <td>3100000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81042</td>\n",
       "      <td>5774</td>\n",
       "      <td>1728</td>\n",
       "      <td>251</td>\n",
       "      <td>194</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93925</td>\n",
       "      <td>466101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>271442</td>\n",
       "      <td>3669</td>\n",
       "      <td>8305</td>\n",
       "      <td>3471141</td>\n",
       "      <td>547143</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DERRICK JR. JAMES V</th>\n",
       "      <td>800000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1284000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8831913</td>\n",
       "      <td>51124</td>\n",
       "      <td>909</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>484000</td>\n",
       "      <td>7482</td>\n",
       "      <td>1787380</td>\n",
       "      <td>-1787380</td>\n",
       "      <td>492375</td>\n",
       "      <td>1401</td>\n",
       "      <td>2181</td>\n",
       "      <td>550981</td>\n",
       "      <td>8831913</td>\n",
       "      <td>-2.330385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WHITE JR THOMAS E</th>\n",
       "      <td>450000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1297049</td>\n",
       "      <td>81353</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1085463</td>\n",
       "      <td>13847074</td>\n",
       "      <td>NaN</td>\n",
       "      <td>317543</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1934359</td>\n",
       "      <td>15144123</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HANNON KEVIN P</th>\n",
       "      <td>1500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3117011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5538001</td>\n",
       "      <td>34039</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>21</td>\n",
       "      <td>1617011</td>\n",
       "      <td>11350</td>\n",
       "      <td>853064</td>\n",
       "      <td>NaN</td>\n",
       "      <td>243293</td>\n",
       "      <td>1035</td>\n",
       "      <td>1045</td>\n",
       "      <td>288682</td>\n",
       "      <td>6391065</td>\n",
       "      <td>-10.797349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HERMANN ROBERT J</th>\n",
       "      <td>700000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-280000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>187500</td>\n",
       "      <td>48357</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>150000</td>\n",
       "      <td>416441</td>\n",
       "      <td>480632</td>\n",
       "      <td>NaN</td>\n",
       "      <td>262663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1297461</td>\n",
       "      <td>668132</td>\n",
       "      <td>-0.215806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TILNEY ELIZABETH A</th>\n",
       "      <td>300000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-575000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>591250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>275000</td>\n",
       "      <td>152055</td>\n",
       "      <td>576792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247338</td>\n",
       "      <td>379</td>\n",
       "      <td>460</td>\n",
       "      <td>399393</td>\n",
       "      <td>1168042</td>\n",
       "      <td>-1.439681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DONAHUE JR JEFFREY M</th>\n",
       "      <td>800000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-300000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>765920</td>\n",
       "      <td>96268</td>\n",
       "      <td>22</td>\n",
       "      <td>188</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>891</td>\n",
       "      <td>315068</td>\n",
       "      <td>NaN</td>\n",
       "      <td>278601</td>\n",
       "      <td>772</td>\n",
       "      <td>865</td>\n",
       "      <td>875760</td>\n",
       "      <td>1080988</td>\n",
       "      <td>-0.342559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CUMBERLAND MICHAEL S</th>\n",
       "      <td>325000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22344</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>275000</td>\n",
       "      <td>713</td>\n",
       "      <td>207940</td>\n",
       "      <td>NaN</td>\n",
       "      <td>184899</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>807956</td>\n",
       "      <td>207940</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PIRO JIM</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47304</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47304</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WASAFF GEORGE</th>\n",
       "      <td>325000</td>\n",
       "      <td>831299</td>\n",
       "      <td>-583325</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1668260</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>200000</td>\n",
       "      <td>1425</td>\n",
       "      <td>388167</td>\n",
       "      <td>NaN</td>\n",
       "      <td>259996</td>\n",
       "      <td>337</td>\n",
       "      <td>400</td>\n",
       "      <td>1034395</td>\n",
       "      <td>2056427</td>\n",
       "      <td>-0.563928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BADUM JAMES P</th>\n",
       "      <td>NaN</td>\n",
       "      <td>178980</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>257817</td>\n",
       "      <td>3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>182466</td>\n",
       "      <td>257817</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GIBBS DANA R</th>\n",
       "      <td>NaN</td>\n",
       "      <td>504610</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2218275</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>461912</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>169</td>\n",
       "      <td>966522</td>\n",
       "      <td>2218275</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>URQUHART JOHN A</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-36666</td>\n",
       "      <td>36666</td>\n",
       "      <td>NaN</td>\n",
       "      <td>228656</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>228656</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.160354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAI LOU L</th>\n",
       "      <td>1000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15364167</td>\n",
       "      <td>32047</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1829457</td>\n",
       "      <td>8453763</td>\n",
       "      <td>NaN</td>\n",
       "      <td>261879</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3123383</td>\n",
       "      <td>23817930</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MURRAY JULIA H</th>\n",
       "      <td>400000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>400478</td>\n",
       "      <td>57580</td>\n",
       "      <td>45</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>125000</td>\n",
       "      <td>330</td>\n",
       "      <td>196983</td>\n",
       "      <td>NaN</td>\n",
       "      <td>229284</td>\n",
       "      <td>395</td>\n",
       "      <td>2192</td>\n",
       "      <td>812194</td>\n",
       "      <td>597461</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SHANKMAN JEFFREY A</th>\n",
       "      <td>2000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1441898</td>\n",
       "      <td>178979</td>\n",
       "      <td>2681</td>\n",
       "      <td>94</td>\n",
       "      <td>83</td>\n",
       "      <td>554422</td>\n",
       "      <td>1191</td>\n",
       "      <td>630137</td>\n",
       "      <td>NaN</td>\n",
       "      <td>304110</td>\n",
       "      <td>1730</td>\n",
       "      <td>3221</td>\n",
       "      <td>3038702</td>\n",
       "      <td>2072035</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MCCARTY DANNY J</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>664375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>215</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>94556</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>508</td>\n",
       "      <td>1433</td>\n",
       "      <td>NaN</td>\n",
       "      <td>758931</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KISHKILL JOSEPH G</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-51042</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>116335</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>465357</td>\n",
       "      <td>1034346</td>\n",
       "      <td>NaN</td>\n",
       "      <td>174246</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>704896</td>\n",
       "      <td>1034346</td>\n",
       "      <td>-0.072411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SHERRIFF JOHN R</th>\n",
       "      <td>1500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1835558</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92</td>\n",
       "      <td>28</td>\n",
       "      <td>23</td>\n",
       "      <td>554422</td>\n",
       "      <td>1852186</td>\n",
       "      <td>1293424</td>\n",
       "      <td>NaN</td>\n",
       "      <td>428780</td>\n",
       "      <td>2103</td>\n",
       "      <td>3187</td>\n",
       "      <td>4335388</td>\n",
       "      <td>3128982</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YEAP SOON</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>192758</td>\n",
       "      <td>55097</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55097</td>\n",
       "      <td>192758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CALGER CHRISTOPHER F</th>\n",
       "      <td>1250000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-262500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35818</td>\n",
       "      <td>144</td>\n",
       "      <td>199</td>\n",
       "      <td>25</td>\n",
       "      <td>375304</td>\n",
       "      <td>486</td>\n",
       "      <td>126027</td>\n",
       "      <td>NaN</td>\n",
       "      <td>240189</td>\n",
       "      <td>2188</td>\n",
       "      <td>2598</td>\n",
       "      <td>1639297</td>\n",
       "      <td>126027</td>\n",
       "      <td>-0.160130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JAEDICKE ROBERT</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-25000</td>\n",
       "      <td>108750</td>\n",
       "      <td>431750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44093</td>\n",
       "      <td>-44093</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>83750</td>\n",
       "      <td>431750</td>\n",
       "      <td>-0.298504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PIPER GREGORY F</th>\n",
       "      <td>400000</td>\n",
       "      <td>1130036</td>\n",
       "      <td>-33333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>880290</td>\n",
       "      <td>43057</td>\n",
       "      <td>222</td>\n",
       "      <td>61</td>\n",
       "      <td>48</td>\n",
       "      <td>NaN</td>\n",
       "      <td>778</td>\n",
       "      <td>409554</td>\n",
       "      <td>-409554</td>\n",
       "      <td>197091</td>\n",
       "      <td>742</td>\n",
       "      <td>1238</td>\n",
       "      <td>1737629</td>\n",
       "      <td>880290</td>\n",
       "      <td>-0.019183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BLAKE JR. NORMAN P</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-113784</td>\n",
       "      <td>113784</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1279</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1279</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-88.893750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MENDELSOHN JOHN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-103750</td>\n",
       "      <td>103750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>148</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>148</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-696.308725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OVERDYKE JR JERE C</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5266578</td>\n",
       "      <td>18834</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>135836</td>\n",
       "      <td>176</td>\n",
       "      <td>2041016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>94941</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>249787</td>\n",
       "      <td>7307594</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows  19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          bonus  deferral_payments  deferred_income  \\\n",
       "COX DAVID                800000                NaN           -41250   \n",
       "ECHOLS JOHN B            200000                NaN              NaN   \n",
       "MARTIN AMANDA K             NaN              85430              NaN   \n",
       "CHRISTODOULOU DIOMEDES      NaN                NaN              NaN   \n",
       "CLINE KENNETH W             NaN                NaN              NaN   \n",
       "CHAN RONNIE                 NaN                NaN           -98784   \n",
       "SCRIMSHAW MATTHEW           NaN                NaN              NaN   \n",
       "HUGHES JAMES A              NaN                NaN              NaN   \n",
       "FITZGERALD JAY L         350000                NaN              NaN   \n",
       "BOWEN JR RAYMOND M      1350000                NaN             -833   \n",
       "MEYER ROCKFORD G            NaN            1848227              NaN   \n",
       "DURAN WILLIAM D          750000                NaN              NaN   \n",
       "CAUSEY RICHARD A        1000000                NaN          -235000   \n",
       "HUMPHREY GENE E             NaN            2964506              NaN   \n",
       "LEFF DANIEL P           1000000                NaN              NaN   \n",
       "WODRASKA JOHN               NaN                NaN              NaN   \n",
       "FREVERT MARK A          2000000            6426990         -3367011   \n",
       "DEFFNER JOSEPH M         600000                NaN              NaN   \n",
       "RICE KENNETH D          1750000                NaN         -3504386   \n",
       "KOENIG MARK E            700000                NaN              NaN   \n",
       "WHALLEY LAWRENCE G      3000000                NaN              NaN   \n",
       "DETMERING TIMOTHY J      425000             875307          -775241   \n",
       "LEMAISTRE CHARLES           NaN                NaN           -25000   \n",
       "YEAGER F SCOTT              NaN                NaN              NaN   \n",
       "HAYSLETT RODERICK J         NaN                NaN              NaN   \n",
       "WINOKUR JR. HERBERT S       NaN                NaN           -25000   \n",
       "WESTFAHL RICHARD K          NaN                NaN           -10800   \n",
       "GILLIS JOHN                 NaN                NaN              NaN   \n",
       "BAZELIDES PHILIP J          NaN             684694              NaN   \n",
       "LOWRY CHARLES P             NaN                NaN              NaN   \n",
       "...                         ...                ...              ...   \n",
       "MCCLELLAN GEORGE         900000                NaN          -125000   \n",
       "TAYLOR MITCHELL S        600000             227449              NaN   \n",
       "CARTER REBECCA C         300000                NaN          -159792   \n",
       "GRAY RODNEY                 NaN              93585              NaN   \n",
       "KITCHEN LOUISE          3100000                NaN              NaN   \n",
       "DERRICK JR. JAMES V      800000                NaN         -1284000   \n",
       "WHITE JR THOMAS E        450000                NaN              NaN   \n",
       "HANNON KEVIN P          1500000                NaN         -3117011   \n",
       "HERMANN ROBERT J         700000                NaN          -280000   \n",
       "TILNEY ELIZABETH A       300000                NaN          -575000   \n",
       "DONAHUE JR JEFFREY M     800000                NaN          -300000   \n",
       "CUMBERLAND MICHAEL S     325000                NaN              NaN   \n",
       "PIRO JIM                    NaN                NaN              NaN   \n",
       "WASAFF GEORGE            325000             831299          -583325   \n",
       "BADUM JAMES P               NaN             178980              NaN   \n",
       "GIBBS DANA R                NaN             504610              NaN   \n",
       "URQUHART JOHN A             NaN                NaN           -36666   \n",
       "PAI LOU L               1000000                NaN              NaN   \n",
       "MURRAY JULIA H           400000                NaN              NaN   \n",
       "SHANKMAN JEFFREY A      2000000                NaN              NaN   \n",
       "MCCARTY DANNY J             NaN                NaN              NaN   \n",
       "KISHKILL JOSEPH G           NaN                NaN           -51042   \n",
       "SHERRIFF JOHN R         1500000                NaN              NaN   \n",
       "YEAP SOON                   NaN                NaN              NaN   \n",
       "CALGER CHRISTOPHER F    1250000                NaN          -262500   \n",
       "JAEDICKE ROBERT             NaN                NaN           -25000   \n",
       "PIPER GREGORY F          400000            1130036           -33333   \n",
       "BLAKE JR. NORMAN P          NaN                NaN          -113784   \n",
       "MENDELSOHN JOHN             NaN                NaN          -103750   \n",
       "OVERDYKE JR JERE C          NaN                NaN              NaN   \n",
       "\n",
       "                        director_fees  exercised_stock_options  expenses  \\\n",
       "COX DAVID                         NaN                   117551     27861   \n",
       "ECHOLS JOHN B                     NaN                   601438     21530   \n",
       "MARTIN AMANDA K                   NaN                  2070306      8211   \n",
       "CHRISTODOULOU DIOMEDES            NaN                  5127155       NaN   \n",
       "CLINE KENNETH W                   NaN                      NaN       NaN   \n",
       "CHAN RONNIE                     98784                      NaN       NaN   \n",
       "SCRIMSHAW MATTHEW                 NaN                   759557       NaN   \n",
       "HUGHES JAMES A                    NaN                   754966       NaN   \n",
       "FITZGERALD JAY L                  NaN                   664461     23870   \n",
       "BOWEN JR RAYMOND M                NaN                      NaN     65907   \n",
       "MEYER ROCKFORD G                  NaN                   493489       NaN   \n",
       "DURAN WILLIAM D                   NaN                  1451869     25785   \n",
       "CAUSEY RICHARD A                  NaN                      NaN     30674   \n",
       "HUMPHREY GENE E                   NaN                  2282768      4994   \n",
       "LEFF DANIEL P                     NaN                      NaN       NaN   \n",
       "WODRASKA JOHN                     NaN                      NaN       NaN   \n",
       "FREVERT MARK A                    NaN                 10433518     86987   \n",
       "DEFFNER JOSEPH M                  NaN                    17378     41626   \n",
       "RICE KENNETH D                    NaN                 19794175     46950   \n",
       "KOENIG MARK E                     NaN                   671737    127017   \n",
       "WHALLEY LAWRENCE G                NaN                  3282960     57838   \n",
       "DETMERING TIMOTHY J               NaN                  2027865     52255   \n",
       "LEMAISTRE CHARLES              112492                   412878       NaN   \n",
       "YEAGER F SCOTT                    NaN                  8308552     53947   \n",
       "HAYSLETT RODERICK J               NaN                      NaN       NaN   \n",
       "WINOKUR JR. HERBERT S          108579                      NaN      1413   \n",
       "WESTFAHL RICHARD K                NaN                      NaN     51870   \n",
       "GILLIS JOHN                       NaN                     9803       NaN   \n",
       "BAZELIDES PHILIP J                NaN                  1599641       NaN   \n",
       "LOWRY CHARLES P                   NaN                   372205       NaN   \n",
       "...                               ...                      ...       ...   \n",
       "MCCLELLAN GEORGE                  NaN                   506765    228763   \n",
       "TAYLOR MITCHELL S                 NaN                  3181250       NaN   \n",
       "CARTER REBECCA C                  NaN                      NaN       NaN   \n",
       "GRAY RODNEY                       NaN                      NaN       NaN   \n",
       "KITCHEN LOUISE                    NaN                    81042      5774   \n",
       "DERRICK JR. JAMES V               NaN                  8831913     51124   \n",
       "WHITE JR THOMAS E                 NaN                  1297049     81353   \n",
       "HANNON KEVIN P                    NaN                  5538001     34039   \n",
       "HERMANN ROBERT J                  NaN                   187500     48357   \n",
       "TILNEY ELIZABETH A                NaN                   591250       NaN   \n",
       "DONAHUE JR JEFFREY M              NaN                   765920     96268   \n",
       "CUMBERLAND MICHAEL S              NaN                      NaN     22344   \n",
       "PIRO JIM                          NaN                      NaN       NaN   \n",
       "WASAFF GEORGE                     NaN                  1668260       NaN   \n",
       "BADUM JAMES P                     NaN                   257817      3486   \n",
       "GIBBS DANA R                      NaN                  2218275       NaN   \n",
       "URQUHART JOHN A                 36666                      NaN    228656   \n",
       "PAI LOU L                         NaN                 15364167     32047   \n",
       "MURRAY JULIA H                    NaN                   400478     57580   \n",
       "SHANKMAN JEFFREY A                NaN                  1441898    178979   \n",
       "MCCARTY DANNY J                   NaN                   664375       NaN   \n",
       "KISHKILL JOSEPH G                 NaN                      NaN    116335   \n",
       "SHERRIFF JOHN R                   NaN                  1835558       NaN   \n",
       "YEAP SOON                         NaN                   192758     55097   \n",
       "CALGER CHRISTOPHER F              NaN                      NaN     35818   \n",
       "JAEDICKE ROBERT                108750                   431750       NaN   \n",
       "PIPER GREGORY F                   NaN                   880290     43057   \n",
       "BLAKE JR. NORMAN P             113784                      NaN      1279   \n",
       "MENDELSOHN JOHN                103750                      NaN       148   \n",
       "OVERDYKE JR JERE C                NaN                  5266578     18834   \n",
       "\n",
       "                        from_messages  from_poi_to_this_person  \\\n",
       "COX DAVID                          33                        0   \n",
       "ECHOLS JOHN B                     NaN                      NaN   \n",
       "MARTIN AMANDA K                   230                        8   \n",
       "CHRISTODOULOU DIOMEDES            NaN                      NaN   \n",
       "CLINE KENNETH W                   NaN                      NaN   \n",
       "CHAN RONNIE                       NaN                      NaN   \n",
       "SCRIMSHAW MATTHEW                 NaN                      NaN   \n",
       "HUGHES JAMES A                     34                       35   \n",
       "FITZGERALD JAY L                   16                        1   \n",
       "BOWEN JR RAYMOND M                 27                      140   \n",
       "MEYER ROCKFORD G                   28                        0   \n",
       "DURAN WILLIAM D                    12                      106   \n",
       "CAUSEY RICHARD A                   49                       58   \n",
       "HUMPHREY GENE E                    17                       10   \n",
       "LEFF DANIEL P                      63                       67   \n",
       "WODRASKA JOHN                     NaN                      NaN   \n",
       "FREVERT MARK A                     21                      242   \n",
       "DEFFNER JOSEPH M                   74                      115   \n",
       "RICE KENNETH D                     18                       42   \n",
       "KOENIG MARK E                      61                       53   \n",
       "WHALLEY LAWRENCE G                556                      186   \n",
       "DETMERING TIMOTHY J               NaN                      NaN   \n",
       "LEMAISTRE CHARLES                 NaN                      NaN   \n",
       "YEAGER F SCOTT                    NaN                      NaN   \n",
       "HAYSLETT RODERICK J              1061                       35   \n",
       "WINOKUR JR. HERBERT S             NaN                      NaN   \n",
       "WESTFAHL RICHARD K                NaN                      NaN   \n",
       "GILLIS JOHN                       NaN                      NaN   \n",
       "BAZELIDES PHILIP J                NaN                      NaN   \n",
       "LOWRY CHARLES P                   NaN                      NaN   \n",
       "...                               ...                      ...   \n",
       "MCCLELLAN GEORGE                   49                       52   \n",
       "TAYLOR MITCHELL S                  29                        0   \n",
       "CARTER REBECCA C                   15                       29   \n",
       "GRAY RODNEY                       NaN                      NaN   \n",
       "KITCHEN LOUISE                   1728                      251   \n",
       "DERRICK JR. JAMES V               909                       64   \n",
       "WHITE JR THOMAS E                 NaN                      NaN   \n",
       "HANNON KEVIN P                     32                       32   \n",
       "HERMANN ROBERT J                  NaN                      NaN   \n",
       "TILNEY ELIZABETH A                 19                       10   \n",
       "DONAHUE JR JEFFREY M               22                      188   \n",
       "CUMBERLAND MICHAEL S              NaN                      NaN   \n",
       "PIRO JIM                           16                        0   \n",
       "WASAFF GEORGE                      30                       22   \n",
       "BADUM JAMES P                     NaN                      NaN   \n",
       "GIBBS DANA R                       12                        0   \n",
       "URQUHART JOHN A                   NaN                      NaN   \n",
       "PAI LOU L                         NaN                      NaN   \n",
       "MURRAY JULIA H                     45                       11   \n",
       "SHANKMAN JEFFREY A               2681                       94   \n",
       "MCCARTY DANNY J                   215                       25   \n",
       "KISHKILL JOSEPH G                 NaN                      NaN   \n",
       "SHERRIFF JOHN R                    92                       28   \n",
       "YEAP SOON                         NaN                      NaN   \n",
       "CALGER CHRISTOPHER F              144                      199   \n",
       "JAEDICKE ROBERT                   NaN                      NaN   \n",
       "PIPER GREGORY F                   222                       61   \n",
       "BLAKE JR. NORMAN P                NaN                      NaN   \n",
       "MENDELSOHN JOHN                   NaN                      NaN   \n",
       "OVERDYKE JR JERE C                NaN                      NaN   \n",
       "\n",
       "                        from_this_person_to_poi  long_term_incentive    other  \\\n",
       "COX DAVID                                     4                  NaN      494   \n",
       "ECHOLS JOHN B                               NaN              2234774    53775   \n",
       "MARTIN AMANDA K                               0              5145434  2818454   \n",
       "CHRISTODOULOU DIOMEDES                      NaN                  NaN      NaN   \n",
       "CLINE KENNETH W                             NaN                  NaN      NaN   \n",
       "CHAN RONNIE                                 NaN                  NaN      NaN   \n",
       "SCRIMSHAW MATTHEW                           NaN                  NaN      NaN   \n",
       "HUGHES JAMES A                                5                  NaN      NaN   \n",
       "FITZGERALD JAY L                              8               556416   285414   \n",
       "BOWEN JR RAYMOND M                           15               974293     1621   \n",
       "MEYER ROCKFORD G                              0                  NaN      NaN   \n",
       "DURAN WILLIAM D                               3              1105218     1568   \n",
       "CAUSEY RICHARD A                             12               350000   307895   \n",
       "HUMPHREY GENE E                              17                  NaN      NaN   \n",
       "LEFF DANIEL P                                14              1387399     3083   \n",
       "WODRASKA JOHN                               NaN                  NaN   189583   \n",
       "FREVERT MARK A                                6              1617011  7427621   \n",
       "DEFFNER JOSEPH M                              4               335349    25553   \n",
       "RICE KENNETH D                                4              1617011   174839   \n",
       "KOENIG MARK E                                15               300000   150458   \n",
       "WHALLEY LAWRENCE G                           24               808346   301026   \n",
       "DETMERING TIMOTHY J                         NaN               415657     1105   \n",
       "LEMAISTRE CHARLES                           NaN                  NaN      NaN   \n",
       "YEAGER F SCOTT                              NaN                  NaN   147950   \n",
       "HAYSLETT RODERICK J                          38                  NaN      NaN   \n",
       "WINOKUR JR. HERBERT S                       NaN                  NaN      NaN   \n",
       "WESTFAHL RICHARD K                          NaN               256191   401130   \n",
       "GILLIS JOHN                                 NaN                  NaN      NaN   \n",
       "BAZELIDES PHILIP J                          NaN                93750      874   \n",
       "LOWRY CHARLES P                             NaN                  NaN      NaN   \n",
       "...                                         ...                  ...      ...   \n",
       "MCCLELLAN GEORGE                              0                  NaN    51587   \n",
       "TAYLOR MITCHELL S                             0                  NaN      NaN   \n",
       "CARTER REBECCA C                              7                75000      540   \n",
       "GRAY RODNEY                                 NaN               365625   680833   \n",
       "KITCHEN LOUISE                              194                  NaN    93925   \n",
       "DERRICK JR. JAMES V                          20               484000     7482   \n",
       "WHITE JR THOMAS E                           NaN                  NaN  1085463   \n",
       "HANNON KEVIN P                               21              1617011    11350   \n",
       "HERMANN ROBERT J                            NaN               150000   416441   \n",
       "TILNEY ELIZABETH A                           11               275000   152055   \n",
       "DONAHUE JR JEFFREY M                         11                  NaN      891   \n",
       "CUMBERLAND MICHAEL S                        NaN               275000      713   \n",
       "PIRO JIM                                      1                  NaN      NaN   \n",
       "WASAFF GEORGE                                 7               200000     1425   \n",
       "BADUM JAMES P                               NaN                  NaN      NaN   \n",
       "GIBBS DANA R                                  0               461912      NaN   \n",
       "URQUHART JOHN A                             NaN                  NaN      NaN   \n",
       "PAI LOU L                                   NaN                  NaN  1829457   \n",
       "MURRAY JULIA H                                2               125000      330   \n",
       "SHANKMAN JEFFREY A                           83               554422     1191   \n",
       "MCCARTY DANNY J                               2                  NaN      NaN   \n",
       "KISHKILL JOSEPH G                           NaN                  NaN   465357   \n",
       "SHERRIFF JOHN R                              23               554422  1852186   \n",
       "YEAP SOON                                   NaN                  NaN      NaN   \n",
       "CALGER CHRISTOPHER F                         25               375304      486   \n",
       "JAEDICKE ROBERT                             NaN                  NaN      NaN   \n",
       "PIPER GREGORY F                              48                  NaN      778   \n",
       "BLAKE JR. NORMAN P                          NaN                  NaN      NaN   \n",
       "MENDELSOHN JOHN                             NaN                  NaN      NaN   \n",
       "OVERDYKE JR JERE C                          NaN               135836      176   \n",
       "\n",
       "                        restricted_stock  restricted_stock_deferred   salary  \\\n",
       "COX DAVID                         378082                        NaN   314288   \n",
       "ECHOLS JOHN B                     407503                        NaN   182245   \n",
       "MARTIN AMANDA K                      NaN                        NaN   349487   \n",
       "CHRISTODOULOU DIOMEDES            950730                        NaN      NaN   \n",
       "CLINE KENNETH W                   662086                    -472568      NaN   \n",
       "CHAN RONNIE                        32460                     -32460      NaN   \n",
       "SCRIMSHAW MATTHEW                    NaN                        NaN      NaN   \n",
       "HUGHES JAMES A                    363428                        NaN      NaN   \n",
       "FITZGERALD JAY L                  956775                        NaN   199157   \n",
       "BOWEN JR RAYMOND M                252055                        NaN   278601   \n",
       "MEYER ROCKFORD G                  462384                        NaN      NaN   \n",
       "DURAN WILLIAM D                   189041                        NaN   210692   \n",
       "CAUSEY RICHARD A                 2502063                        NaN   415189   \n",
       "HUMPHREY GENE E                      NaN                        NaN   130724   \n",
       "LEFF DANIEL P                     360528                        NaN   273746   \n",
       "WODRASKA JOHN                        NaN                        NaN      NaN   \n",
       "FREVERT MARK A                   4188667                        NaN  1060932   \n",
       "DEFFNER JOSEPH M                  141833                        NaN   206121   \n",
       "RICE KENNETH D                   2748364                        NaN   420636   \n",
       "KOENIG MARK E                    1248318                        NaN   309946   \n",
       "WHALLEY LAWRENCE G               2796177                        NaN   510364   \n",
       "DETMERING TIMOTHY J               315068                    -315068   210500   \n",
       "LEMAISTRE CHARLES                    NaN                        NaN      NaN   \n",
       "YEAGER F SCOTT                   3576206                        NaN   158403   \n",
       "HAYSLETT RODERICK J               346663                        NaN      NaN   \n",
       "WINOKUR JR. HERBERT S                NaN                        NaN      NaN   \n",
       "WESTFAHL RICHARD K                384930                        NaN    63744   \n",
       "GILLIS JOHN                        75838                        NaN      NaN   \n",
       "BAZELIDES PHILIP J                   NaN                        NaN    80818   \n",
       "LOWRY CHARLES P                   153686                    -153686      NaN   \n",
       "...                                  ...                        ...      ...   \n",
       "MCCLELLAN GEORGE                  441096                        NaN   263413   \n",
       "TAYLOR MITCHELL S                 563798                        NaN   265214   \n",
       "CARTER REBECCA C                  307301                    -307301   261809   \n",
       "GRAY RODNEY                          NaN                        NaN     6615   \n",
       "KITCHEN LOUISE                    466101                        NaN   271442   \n",
       "DERRICK JR. JAMES V              1787380                   -1787380   492375   \n",
       "WHITE JR THOMAS E               13847074                        NaN   317543   \n",
       "HANNON KEVIN P                    853064                        NaN   243293   \n",
       "HERMANN ROBERT J                  480632                        NaN   262663   \n",
       "TILNEY ELIZABETH A                576792                        NaN   247338   \n",
       "DONAHUE JR JEFFREY M              315068                        NaN   278601   \n",
       "CUMBERLAND MICHAEL S              207940                        NaN   184899   \n",
       "PIRO JIM                           47304                        NaN      NaN   \n",
       "WASAFF GEORGE                     388167                        NaN   259996   \n",
       "BADUM JAMES P                        NaN                        NaN      NaN   \n",
       "GIBBS DANA R                         NaN                        NaN      NaN   \n",
       "URQUHART JOHN A                      NaN                        NaN      NaN   \n",
       "PAI LOU L                        8453763                        NaN   261879   \n",
       "MURRAY JULIA H                    196983                        NaN   229284   \n",
       "SHANKMAN JEFFREY A                630137                        NaN   304110   \n",
       "MCCARTY DANNY J                    94556                        NaN      NaN   \n",
       "KISHKILL JOSEPH G                1034346                        NaN   174246   \n",
       "SHERRIFF JOHN R                  1293424                        NaN   428780   \n",
       "YEAP SOON                            NaN                        NaN      NaN   \n",
       "CALGER CHRISTOPHER F              126027                        NaN   240189   \n",
       "JAEDICKE ROBERT                    44093                     -44093      NaN   \n",
       "PIPER GREGORY F                   409554                    -409554   197091   \n",
       "BLAKE JR. NORMAN P                   NaN                        NaN      NaN   \n",
       "MENDELSOHN JOHN                      NaN                        NaN      NaN   \n",
       "OVERDYKE JR JERE C               2041016                        NaN    94941   \n",
       "\n",
       "                        shared_receipt_with_poi  to_messages  total_payments  \\\n",
       "COX DAVID                                    71          102         1101393   \n",
       "ECHOLS JOHN B                               NaN          NaN         2692324   \n",
       "MARTIN AMANDA K                             477         1522         8407016   \n",
       "CHRISTODOULOU DIOMEDES                      NaN          NaN             NaN   \n",
       "CLINE KENNETH W                             NaN          NaN             NaN   \n",
       "CHAN RONNIE                                 NaN          NaN             NaN   \n",
       "SCRIMSHAW MATTHEW                           NaN          NaN             NaN   \n",
       "HUGHES JAMES A                              589          719             NaN   \n",
       "FITZGERALD JAY L                            723          936         1414857   \n",
       "BOWEN JR RAYMOND M                         1593         1858         2669589   \n",
       "MEYER ROCKFORD G                             22          232         1848227   \n",
       "DURAN WILLIAM D                             599          904         2093263   \n",
       "CAUSEY RICHARD A                           1585         1892         1868758   \n",
       "HUMPHREY GENE E                             119          128         3100224   \n",
       "LEFF DANIEL P                              2672         2822         2664228   \n",
       "WODRASKA JOHN                               NaN          NaN          189583   \n",
       "FREVERT MARK A                             2979         3275        17252530   \n",
       "DEFFNER JOSEPH M                            552          714         1208649   \n",
       "RICE KENNETH D                              864          905          505050   \n",
       "KOENIG MARK E                              2271         2374         1587421   \n",
       "WHALLEY LAWRENCE G                         3920         6019         4677574   \n",
       "DETMERING TIMOTHY J                         NaN          NaN         1204583   \n",
       "LEMAISTRE CHARLES                           NaN          NaN           87492   \n",
       "YEAGER F SCOTT                              NaN          NaN          360300   \n",
       "HAYSLETT RODERICK J                         571         2649             NaN   \n",
       "WINOKUR JR. HERBERT S                       NaN          NaN           84992   \n",
       "WESTFAHL RICHARD K                          NaN          NaN          762135   \n",
       "GILLIS JOHN                                 NaN          NaN             NaN   \n",
       "BAZELIDES PHILIP J                          NaN          NaN          860136   \n",
       "LOWRY CHARLES P                             NaN          NaN             NaN   \n",
       "...                                         ...          ...             ...   \n",
       "MCCLELLAN GEORGE                           1469         1744         1318763   \n",
       "TAYLOR MITCHELL S                           300          533         1092663   \n",
       "CARTER REBECCA C                            196          312          477557   \n",
       "GRAY RODNEY                                 NaN          NaN         1146658   \n",
       "KITCHEN LOUISE                             3669         8305         3471141   \n",
       "DERRICK JR. JAMES V                        1401         2181          550981   \n",
       "WHITE JR THOMAS E                           NaN          NaN         1934359   \n",
       "HANNON KEVIN P                             1035         1045          288682   \n",
       "HERMANN ROBERT J                            NaN          NaN         1297461   \n",
       "TILNEY ELIZABETH A                          379          460          399393   \n",
       "DONAHUE JR JEFFREY M                        772          865          875760   \n",
       "CUMBERLAND MICHAEL S                        NaN          NaN          807956   \n",
       "PIRO JIM                                      3           58             NaN   \n",
       "WASAFF GEORGE                               337          400         1034395   \n",
       "BADUM JAMES P                               NaN          NaN          182466   \n",
       "GIBBS DANA R                                 23          169          966522   \n",
       "URQUHART JOHN A                             NaN          NaN          228656   \n",
       "PAI LOU L                                   NaN          NaN         3123383   \n",
       "MURRAY JULIA H                              395         2192          812194   \n",
       "SHANKMAN JEFFREY A                         1730         3221         3038702   \n",
       "MCCARTY DANNY J                             508         1433             NaN   \n",
       "KISHKILL JOSEPH G                           NaN          NaN          704896   \n",
       "SHERRIFF JOHN R                            2103         3187         4335388   \n",
       "YEAP SOON                                   NaN          NaN           55097   \n",
       "CALGER CHRISTOPHER F                       2188         2598         1639297   \n",
       "JAEDICKE ROBERT                             NaN          NaN           83750   \n",
       "PIPER GREGORY F                             742         1238         1737629   \n",
       "BLAKE JR. NORMAN P                          NaN          NaN            1279   \n",
       "MENDELSOHN JOHN                             NaN          NaN             148   \n",
       "OVERDYKE JR JERE C                          NaN          NaN          249787   \n",
       "\n",
       "                        total_stock_value  deferred_ratio  \n",
       "COX DAVID                          495633       -0.037453  \n",
       "ECHOLS JOHN B                     1008941             NaN  \n",
       "MARTIN AMANDA K                   2070306             NaN  \n",
       "CHRISTODOULOU DIOMEDES            6077885             NaN  \n",
       "CLINE KENNETH W                    189518             NaN  \n",
       "CHAN RONNIE                           NaN             NaN  \n",
       "SCRIMSHAW MATTHEW                  759557             NaN  \n",
       "HUGHES JAMES A                    1118394             NaN  \n",
       "FITZGERALD JAY L                  1621236             NaN  \n",
       "BOWEN JR RAYMOND M                 252055       -0.000312  \n",
       "MEYER ROCKFORD G                   955873             NaN  \n",
       "DURAN WILLIAM D                   1640910             NaN  \n",
       "CAUSEY RICHARD A                  2502063       -0.125752  \n",
       "HUMPHREY GENE E                   2282768             NaN  \n",
       "LEFF DANIEL P                      360528             NaN  \n",
       "WODRASKA JOHN                         NaN             NaN  \n",
       "FREVERT MARK A                   14622185       -0.195160  \n",
       "DEFFNER JOSEPH M                   159211             NaN  \n",
       "RICE KENNETH D                   22542539       -6.938677  \n",
       "KOENIG MARK E                     1920055             NaN  \n",
       "WHALLEY LAWRENCE G                6079137             NaN  \n",
       "DETMERING TIMOTHY J               2027865       -0.643576  \n",
       "LEMAISTRE CHARLES                  412878       -0.285737  \n",
       "YEAGER F SCOTT                   11884758             NaN  \n",
       "HAYSLETT RODERICK J                346663             NaN  \n",
       "WINOKUR JR. HERBERT S                 NaN       -0.294142  \n",
       "WESTFAHL RICHARD K                 384930       -0.014171  \n",
       "GILLIS JOHN                         85641             NaN  \n",
       "BAZELIDES PHILIP J                1599641             NaN  \n",
       "LOWRY CHARLES P                    372205             NaN  \n",
       "...                                   ...             ...  \n",
       "MCCLELLAN GEORGE                   947861       -0.094786  \n",
       "TAYLOR MITCHELL S                 3745048             NaN  \n",
       "CARTER REBECCA C                      NaN       -0.334602  \n",
       "GRAY RODNEY                           NaN             NaN  \n",
       "KITCHEN LOUISE                     547143             NaN  \n",
       "DERRICK JR. JAMES V               8831913       -2.330385  \n",
       "WHITE JR THOMAS E                15144123             NaN  \n",
       "HANNON KEVIN P                    6391065      -10.797349  \n",
       "HERMANN ROBERT J                   668132       -0.215806  \n",
       "TILNEY ELIZABETH A                1168042       -1.439681  \n",
       "DONAHUE JR JEFFREY M              1080988       -0.342559  \n",
       "CUMBERLAND MICHAEL S               207940             NaN  \n",
       "PIRO JIM                            47304             NaN  \n",
       "WASAFF GEORGE                     2056427       -0.563928  \n",
       "BADUM JAMES P                      257817             NaN  \n",
       "GIBBS DANA R                      2218275             NaN  \n",
       "URQUHART JOHN A                       NaN       -0.160354  \n",
       "PAI LOU L                        23817930             NaN  \n",
       "MURRAY JULIA H                     597461             NaN  \n",
       "SHANKMAN JEFFREY A                2072035             NaN  \n",
       "MCCARTY DANNY J                    758931             NaN  \n",
       "KISHKILL JOSEPH G                 1034346       -0.072411  \n",
       "SHERRIFF JOHN R                   3128982             NaN  \n",
       "YEAP SOON                          192758             NaN  \n",
       "CALGER CHRISTOPHER F               126027       -0.160130  \n",
       "JAEDICKE ROBERT                    431750       -0.298504  \n",
       "PIPER GREGORY F                    880290       -0.019183  \n",
       "BLAKE JR. NORMAN P                    NaN      -88.893750  \n",
       "MENDELSOHN JOHN                       NaN     -696.308725  \n",
       "OVERDYKE JR JERE C                7307594             NaN  \n",
       "\n",
       "[100 rows x 19 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "So now it is getting way better. I don't understand how it has gotten better. Recall is at 0.6, which is where it was before at its but, but both accuracy and precision are better than ever. Accuracy is at 0.93 and precision is at 0.75. So I will keep it but I don't see how it has happened. \n",
    "\n",
    "Now we can see if the improvement is transmitted to the other models. \n",
    "\n",
    "UPDATE: So it turns out that the 'poi' was in the features_train. It is not surprising that you can make pretty good predictions if you have the dependent variable in the data set. Now I ran the code above again and got rid of the dependent variable in the features set and got the more believable numbers from the model, with recall and precision down at .6 and .43 respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree, most_frequent imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier\n",
      "Imputer: most_frequent\n",
      "Normalize: StandardScaler\n",
      "PCA: dimensions unspecified\n",
      "\n",
      "Accuracy Score: 0.840909090909\n",
      "\n",
      "Recall: 0.4\n",
      "\n",
      "Precision: 0.333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "Pipeline = Pipeline([\n",
    "        ('imp', Imputer(missing_values='NaN', strategy='most_frequent')),\n",
    "        ('std', StandardScaler()),\n",
    "        ('pca', PCA()),\n",
    "        ('clf', DecisionTreeClassifier(random_state = 53))\n",
    "    ])\n",
    "\n",
    "pipeline = Pipeline.fit(features_train, labels_train)\n",
    "print \"\"\"DecisionTreeClassifier\\nImputer: most_frequent\\nNormalize: StandardScaler\\nPCA: dimensions unspecified\"\"\"\n",
    "print \"\\nAccuracy Score:\", pipeline.score(features_test, labels_test)\n",
    "\n",
    "clf_pipeline_pred = pipeline.predict(features_test)\n",
    "print \"\\nRecall:\", recall_score(labels_test, clf_pipeline_pred)\n",
    "print \"\\nPrecision:\", precision_score(labels_test, clf_pipeline_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discusion \n",
    "The Accuracy is still as high as before, but recall has dropped while precision is up to 1.0. That means that there are no false what's, no mis-classified positives, no false positives. So that is something. \n",
    "\n",
    "The increase in precision is not worth the drop in recall so it is not worth it. So we will go back to the median imputation. \n",
    "\n",
    "Now the next step is to confirm this by putting the grid_search back in and testing the two methods of imputation together. \n",
    "\n",
    "UPDATE: since getting rid of the dependent variable in the data set also improves the preformance of this model. \n",
    "\n",
    "This is a really interesting result. It is not surprising that the inclusion of the dependent variable makes the results unrealistically good. It is surprising that the inclusion of the dependent variable makes the model preform worse. I wonder if that is some property of the decision tree model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search: Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters from the grid search:{'imp__strategy': 'most_frequent'}\n",
      " None\n",
      "\n",
      "Best estimator accuracy: 0.840909090909\n",
      "\n",
      "\n",
      "Recall Score: 0.4\n",
      "\n",
      "\n",
      "Precision Score: 0.333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "Pipeline = Pipeline([\n",
    "        ('imp', Imputer()),\n",
    "        ('std', StandardScaler()),\n",
    "        ('pca', PCA()),\n",
    "        ('clf', DecisionTreeClassifier(random_state = 53))\n",
    "    ])\n",
    "\n",
    "x = [x for x in range(2,19)]\n",
    "param_grid = {'imp__strategy':['median', 'most_frequent']}\n",
    "\n",
    "# pipeline = Pipeline.fit(features_train, labels_train)\n",
    "\n",
    "gridCV_object = GridSearchCV(estimator = Pipeline, \n",
    "                             param_grid = param_grid)\n",
    "\n",
    "gridCV_object.fit(features_train, labels_train)\n",
    "\n",
    "print \"Best parameters from the grid search:\", pprint(gridCV_object.best_params_)\n",
    "\n",
    "clf_gridCV = gridCV_object.best_estimator_\n",
    "\n",
    "print \"\\nBest estimator accuracy:\", clf_gridCV.score(features_test, labels_test)\n",
    "\n",
    "clf_gridCV_pred = clf_gridCV.predict(features_test)\n",
    "\n",
    "print \"\\n\\nRecall Score:\", recall_score(labels_test, clf_gridCV_pred)\n",
    "print \"\\n\\nPrecision Score:\", precision_score(labels_test, clf_gridCV_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "So I get the same answer that I got when I did it without the grid_search. So at least I am not losing my mind. \n",
    "\n",
    "Now I am ready to see if I can get the grid search to work without losing the results I have already gotten. The big problem was that the pca actually preformed worse with the pca grid search than it did without it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree with PCA grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters from the grid search:{'imp__strategy': 'most_frequent', 'pca__n_components': 4}\n",
      " None\n",
      "\n",
      "Best Estimator Accuracy: 0.840909090909\n",
      "\n",
      "\n",
      "Recall Score: 0.2\n",
      "\n",
      "\n",
      "Precision Score: 0.25\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "Pipeline = Pipeline([\n",
    "        ('imp', Imputer()),\n",
    "        ('std', StandardScaler()),\n",
    "        ('pca', PCA()),\n",
    "        ('clf', DecisionTreeClassifier(random_state = 53))\n",
    "    ])\n",
    "\n",
    "x = [x for x in range(2,19)]\n",
    "param_grid = {'imp__strategy':['median', 'most_frequent'],\n",
    "              'pca__n_components': x}\n",
    "\n",
    "gridCV_object = GridSearchCV(estimator = Pipeline, \n",
    "                             param_grid = param_grid)\n",
    "\n",
    "gridCV_object.fit(features_train, labels_train)\n",
    "\n",
    "print \"Best parameters from the grid search:\", pprint(gridCV_object.best_params_)\n",
    "\n",
    "clf_gridCV = gridCV_object.best_estimator_\n",
    "\n",
    "print \"\\nBest Estimator Accuracy:\", clf_gridCV.score(features_test, labels_test)\n",
    "\n",
    "clf_gridCV_pred = clf_gridCV.predict(features_test)\n",
    "\n",
    "print \"\\n\\nRecall Score:\", recall_score(labels_test, clf_gridCV_pred)\n",
    "print \"\\n\\nPrecision Score:\", precision_score(labels_test, clf_gridCV_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "So this is pretty good. I have the thing set up so that the accuracy is over 90% and recall is at 0.6. \n",
    "\n",
    "The thing is that this may all be because of a lucky draw from the train-test split. It may come out terrible in the grader. It may turn out to be just lousy when it goes though the 1000 folds of the Udacity grading program. So the next step is to try it in the auto grader and see if the results hold up. The first step is to prep the data. \n",
    "\n",
    "UPDATE: Now that it has the right data set it is kind of strange but the model actually preforms less well with the pca included. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Submitting to the Udacity Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "df = pd.DataFrame(data_dict)\n",
    "df = df.transpose()\n",
    "df = df.drop('email_address', axis=1)\n",
    "df = df.astype(float)\n",
    "df = df.drop('TOTAL')\n",
    "df = df.drop(\"THE TRAVEL AGENCY IN THE PARK\")\n",
    "df = df.drop(\"loan_advances\", axis=1)\n",
    "df['pct_from_poi'] = df['from_poi_to_this_person']/(df['from_messages'] + 1)\n",
    "df['pct_to_poi'] = df['from_this_person_to_poi']/(df['from_messages'] + 1)\n",
    "df['to_from'] = df['pct_from_poi']*df['pct_from_poi']\n",
    "features_list = list(df.columns)\n",
    "features_list.remove('poi')\n",
    "features_list.remove('from_poi_to_this_person')\n",
    "features_list.remove('from_this_person_to_poi')\n",
    "features_list.remove('from_messages')\n",
    "features = df[features_list]\n",
    "labels = df['poi']\n",
    "\n",
    "from sklearn import cross_validation\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    cross_validation.train_test_split(features, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_list[0] = 'poi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['poi',\n",
       " 'deferral_payments',\n",
       " 'deferred_income',\n",
       " 'director_fees',\n",
       " 'exercised_stock_options',\n",
       " 'expenses',\n",
       " 'long_term_incentive',\n",
       " 'other',\n",
       " 'restricted_stock',\n",
       " 'restricted_stock_deferred',\n",
       " 'salary',\n",
       " 'shared_receipt_with_poi',\n",
       " 'to_messages',\n",
       " 'total_payments',\n",
       " 'total_stock_value',\n",
       " 'pct_from_poi',\n",
       " 'pct_to_poi',\n",
       " 'to_from']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is no good. I have no gotten rid of 'bonus' and replaced it with 'poi'. And I don't think that 'poi' should be in the features list anyway. Then again, what is the deal with the requirement that the dependent variable be the first in the data frame for the grading software to work? Am I imagining that I heard that? \n",
    "\n",
    "It seems like data_dict should have the dependent and dependent variables in it, but that the features list should be supplied so that the program can sort them out itself. \n",
    "\n",
    "Anyway, I have to go back and find where I was able to get the grader to work. I should go back and see if I can stick this model into that notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_1 = df.transpose()\n",
    "data_dict = df_1.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters from the grid search:{'pca__n_components': 3}\n",
      " None\n",
      "\n",
      "Best Estimator Accuracy: 0.75\n",
      "\n",
      "\n",
      "Recall Score: 0.0\n",
      "\n",
      "\n",
      "Precision Score: 0.0\n",
      "\n",
      "\n",
      "And these are the results going through the test classifier:\n",
      "\n",
      "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('std', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=3, whiten=False)), ('clf', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=53, splitter='best'))])\n",
      "\tAccuracy: 0.80960\tPrecision: 0.27450\tRecall: 0.26050\tF1: 0.26732\tF2: 0.26318\n",
      "\tTotal predictions: 15000\tTrue positives:  521\tFalse positives: 1377\tFalse negatives: 1479\tTrue negatives: 11623\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tester import test_classifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "clf = Pipeline([\n",
    "        ('imp', Imputer(missing_values='NaN', strategy='median')),\n",
    "        ('std', StandardScaler()),\n",
    "        ('pca', PCA(n_components = 3)),\n",
    "        ('clf', DecisionTreeClassifier(random_state = 53))\n",
    "    ])\n",
    "\n",
    "Pipeline = Pipeline([\n",
    "        ('imp', Imputer(missing_values='NaN', strategy='median')),\n",
    "        ('std', StandardScaler()),\n",
    "        ('pca', PCA()),\n",
    "        ('clf', DecisionTreeClassifier(random_state = 53))\n",
    "    ])\n",
    "\n",
    "x = [x for x in range(2,19)]\n",
    "param_grid = {'pca__n_components': x}\n",
    "\n",
    "gridCV_object = GridSearchCV(estimator = Pipeline, \n",
    "                             param_grid = param_grid)\n",
    "\n",
    "gridCV_object.fit(features_train, labels_train)\n",
    "\n",
    "print \"Best parameters from the grid search:\", pprint(gridCV_object.best_params_)\n",
    "\n",
    "clf_gridCV = gridCV_object.best_estimator_\n",
    "\n",
    "print \"\\nBest Estimator Accuracy:\", clf_gridCV.score(features_test, labels_test)\n",
    "\n",
    "clf_gridCV_pred = clf_gridCV.predict(features_test)\n",
    "\n",
    "print \"\\n\\nRecall Score:\", recall_score(labels_test, clf_gridCV_pred)\n",
    "print \"\\n\\nPrecision Score:\", precision_score(labels_test, clf_gridCV_pred)\n",
    "\n",
    "print \"\\n\\nAnd these are the results going through the test classifier:\\n\"\n",
    "test_classifier(clf, data_dict, features_list, folds = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
