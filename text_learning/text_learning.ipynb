{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lesson 10: text_learning**\n",
    "\n",
    "The sklearn module for text learning is based on the bag of words model, called the CountVectorizer in sklearn, and lives in the feature_extraction.text library.\n",
    "\n",
    "The CountVectorizer is based on the bag of words model, which allows us to analyze documents of different sizes along a common set of dimensions, specifically the words that occur in any of the documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer is just the name for the bag of words in sklearn. \n",
    "\n",
    "To get them into CountVectorizer you have to turn them into a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "string1 = \"My name is Michael. (Love, Love, Love) (Hate, hate hate) Here are some words that we might find in an email\"\n",
    "string2 = \"My name is Michael. The new Star Wars sucks, but it doesn't suck as bad as the others.\"\n",
    "string3 = \"My name is Michael. When in the course of human events, it becomes necessary...\"\n",
    "\n",
    "email_list = [string1, string2, string3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we turn the list into a bag of words, so this means that every document is a row and every possible word, i.e., any word that occurs at least once in the corpus, is a column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My name is Michael. (Love, Love, Love) (Hate, hate hate) Here are some words that we might find in an email', \"My name is Michael. The new Star Wars sucks, but it doesn't suck as bad as the others.\", 'My name is Michael. When in the course of human events, it becomes necessary...']\n"
     ]
    }
   ],
   "source": [
    "print email_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bag_of_words = vectorizer.fit(email_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bag_of_words = vectorizer.transform(email_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words[2,15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 11)\t3\n",
      "  (0, 12)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 15)\t1\n",
      "  (0, 17)\t3\n",
      "  (0, 18)\t1\n",
      "  (0, 19)\t1\n",
      "  (0, 20)\t1\n",
      "  (0, 21)\t1\n",
      "  (0, 26)\t1\n",
      "  (0, 30)\t1\n",
      "  (0, 33)\t1\n",
      "  (0, 35)\t1\n",
      "  (1, 2)\t2\n",
      "  (1, 3)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 15)\t1\n",
      "  (1, 16)\t1\n",
      "  (1, 18)\t1\n",
      "  (1, 20)\t1\n",
      "  (1, 21)\t1\n",
      "  (1, 23)\t1\n",
      "  (1, 25)\t1\n",
      "  (1, 27)\t1\n",
      "  (1, 28)\t1\n",
      "  (1, 29)\t1\n",
      "  (1, 31)\t2\n",
      "  (1, 32)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 9)\t1\n",
      "  (2, 13)\t1\n",
      "  (2, 14)\t1\n",
      "  (2, 15)\t1\n",
      "  (2, 16)\t1\n",
      "  (2, 18)\t1\n",
      "  (2, 20)\t1\n",
      "  (2, 21)\t1\n",
      "  (2, 22)\t1\n",
      "  (2, 24)\t1\n",
      "  (2, 31)\t1\n",
      "  (2, 34)\t1\n"
     ]
    }
   ],
   "source": [
    "print bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the tuples are the document/word pair. The column of mostly ones is the frequency. \n",
    "\n",
    "So, the row that has the values (0, 16) 1 [this is in the Udacity Video] means that the 16th word in the 0th document occurred once. \n",
    "\n",
    "There are still some things I don't understand, though. I have included the same sentence, \"My name is Michael\", three times in the data set but there are no words with the frequency 3 in the data set. Also, all three strings begin with the same four words but in the data set the documents do not begin the same way. \n",
    "\n",
    "Ahh, the frequency number is the number of times it occurred in that document. If I want to see a 3 I have to repeat the same word three times in the same document. I'll just confirm that by changing string1 above. \n",
    "\n",
    "Ok, really cool. The words 'love' and 'hate' occur three times. I show the two 3s in the vectorized document and I show that the vectorizer is case insensitive. Also, we see that the number applied to the word has nothing to do with where it occurs in the document.\n",
    "\n",
    "We can find out what number a particular word has by using the get method on the vocabulary method on the count vectorizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_.get(\"love\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the word from the number? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stopwords in NLTK**\n",
    "\n",
    "*Get a list of stopwords from the Natural Language Took Kit*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.download() # don't have to run this everytime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sw = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'i'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "So now we know how many stopwords there are in English. \n",
    "\n",
    "Now not all unique words are actually different, that is why we need the stemmer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the snowball stemmer. It is like a sklearn module where you create a model or machine that has an existence independent of the data that it is applied to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'respons'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"responsivity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Order of Operations in Text Processing**\n",
    "\n",
    "Always do your stemming before you do your bag of words text processing. After all, the whole point of stemming is to save time and count the right things. You don't want to count irrelevant variations on words as entirely different things. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weighting Terms by Frequency**\n",
    "\n",
    "The idea is to weight words inversely by frequency. \n",
    "\n",
    "*Tf* is for term frequency\n",
    "\n",
    "*Idf* is the frequency of the term in the corpus as a whole. You want to give a higher weight to the rare words in the corpus. They are what make your document stand out. When words that are uncommon in the corpus as a whole are frequent in a document those words are especially salient to determining the unique content of the document. So the whole process is; TfIdf. \n",
    "\n",
    "Tf is just the standard weighting of how often a word occurs in a document. But the Idf is the weighting of words that do NOT occur often in the corpus as a whole. So TfIdf weights the rare words that occur a lot in the document more highly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Mini-Project**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now we are going to start doing some of the pre-processing ourselves.\n",
    "\n",
    "In the previous projects we recieved the data in the TfIdf form. Now we have to get them to that stage ourselves. \n",
    "\n",
    "We get two text files, one of email from Sara, one from Chris. We have to access the parseOutText() function that takes an opened email as an argument and returns a string containing all the stemmed words in the email. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The first task in the project is to run parseOutText() on an email. The function is defined in the script *parse_out_email_text.py*. The script calls the function on a sample email. \n",
    "\n",
    "The instructions are: \"We currently have this script set up so that it will print the text of the email to the screen, what is the text that you get when you run parseOutText()?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi everyon if you can read this messag your proper use parseouttext pleas proceed to the next part of the project \n"
     ]
    }
   ],
   "source": [
    "%run \"../tools/parse_out_email_text.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deploying Stemming**\n",
    "\n",
    "In parseOutText(), comment out the following line: \n",
    "\n",
    "words = text_string \n",
    "\n",
    "Augment parseOutText() so that the string it returns has all the words stemmed using a SnowballStemmer (use the nltk package, some examples that I found helpful can be found here: http://www.nltk.org/howto/stem.html ). Rerun parse_out_email_text.py, which will use your updated parseOutText() function--whatâ€™s your output now?\n",
    "\n",
    "Hint: you'll need to break the string down into individual words, stem each word, then recombine all the words into one string.\n",
    "\n",
    "*My Answer*\n",
    "\n",
    "So I am going to modify the *parse_out_email_test.py* in my notebook and try to run it from there. First I will concatenate the script's contents to my notebook.\n",
    "\n",
    "So I run in the window below: \n",
    "```\n",
    "%load \"../tools/parse_out_email_text.py\"\n",
    "```\n",
    "\n",
    "And the magic command will show commented out at the top of the window with the text of the script printed out in the window. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi everyon if you can read this messag your proper use parseouttext pleas proceed to the next part of the project \n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import string\n",
    "\n",
    "def parseOutText(f):\n",
    "    \"\"\" given an opened email file f, parse out all text below the\n",
    "        metadata block at the top\n",
    "        (in Part 2, you will also add stemming capabilities)\n",
    "        and return a string that contains all the words\n",
    "        in the email (space-separated) \n",
    "        \n",
    "        example use case:\n",
    "        f = open(\"email_file_name.txt\", \"r\")\n",
    "        text = parseOutText(f)\n",
    "        \n",
    "        Hint: you'll need to break the string down into \n",
    "        individual words, stem each word, then recombine all \n",
    "        the words into one string.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    f.seek(0)  ### go back to beginning of file (annoying)\n",
    "    all_text = f.read()\n",
    "\n",
    "    ### split off metadata\n",
    "    content = all_text.split(\"X-FileName:\")\n",
    "    words = \"\"\n",
    "    if len(content) > 1:\n",
    "        ### remove punctuation\n",
    "        text_string = content[1].translate(string.maketrans(\"\", \"\"), string.punctuation)\n",
    "\n",
    "        ### project part 2: comment out the line below\n",
    "        #words = text_string\n",
    "\n",
    "        ### split the text string into individual words, stem each word,\n",
    "        ### and append the stemmed word to words (make sure there's a single\n",
    "        ### space between each stemmed word)\n",
    "        \n",
    "        # get the stemmer\n",
    "        stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "        text_string = text_string.split()\n",
    "        for word in text_string:\n",
    "            words += stemmer.stem(word) + \" \"\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return words\n",
    "\n",
    "    \n",
    "\n",
    "def main():\n",
    "    ff = open(\"../text_learning/test_email.txt\", \"r\")\n",
    "    text = parseOutText(ff)\n",
    "    print text\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi everyon if you can read this messag your proper use parseouttext pleas proceed to the next part of the project \n"
     ]
    }
   ],
   "source": [
    "%run \"../tools/parse_out_email_text.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now it works both ways. That is good. \n",
    "\n",
    "**Vectorize Text**\n",
    "\n",
    "Now we are going to look at the vectorizing of text. We have the text stemmed. Now we want to turn those into numbers that sklearn work with. \n",
    "\n",
    "The file is *vectorize_text.py*. It will import the parseOutText() and then I alter the code to loop through and parse the text. We feed it one email at a time and return a stemmed text string.\n",
    "\n",
    "Then we go through the strings and take out the proper names. \n",
    "\n",
    "Then we append the updated text strings to the list (?) word_data. And we append the Chris (0) and sara (1) numbers to the from_data list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/Users/michaelreinhard/nano/machineLearning/ud120-projects/text_learning'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emails processed\n",
      "tjonesnsf stephan and sam need nymex calendar \n"
     ]
    }
   ],
   "source": [
    "# I have code that runs and seems to work but the output is not accepted by the grader for the signature scrubbing quiz in Lesson 10. Here is my code: \n",
    "\n",
    "# %load vectorize_text.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "\n",
    "sys.path.append( \"../tools/\" )\n",
    "from parse_out_email_text import parseOutText\n",
    "\n",
    "\"\"\"\n",
    "    Starter code to process the emails from Sara and Chris to extract\n",
    "    the features and get the documents ready for classification.\n",
    "\n",
    "    The list of all the emails from Sara are in the from_sara list\n",
    "    likewise for emails from Chris (from_chris)\n",
    "\n",
    "    The actual documents are in the Enron email dataset, which\n",
    "    you downloaded/unpacked in Part 0 of the first mini-project. If you have\n",
    "    not obtained the Enron email corpus, run startup.py in the tools folder.\n",
    "\n",
    "    The data is stored in lists and packed away in pickle files at the end.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from_sara  = open(\"from_sara.txt\", \"r\")\n",
    "from_chris = open(\"from_chris.txt\", \"r\")\n",
    "\n",
    "# trying to find out why no data gets to the parseOutText program\n",
    "# print from_sara\n",
    "# This returned a proper file object\n",
    "\n",
    "from_data = []\n",
    "word_data = []\n",
    "\n",
    "### temp_counter is a way to speed up the development--there are\n",
    "### thousands of emails from Sara and Chris, so running over all of them\n",
    "### can take a long time\n",
    "### temp_counter helps you only look at the first 200 emails in the list so you\n",
    "### can iterate your modifications quicker\n",
    "temp_counter = 0\n",
    "\n",
    "\n",
    "for name, from_person in [(\"sara\", from_sara), (\"chris\", from_chris)]:\n",
    "    for path in from_person:\n",
    "        ### only look at first 200 emails when developing\n",
    "        ### once everything is working, remove this line to run over full dataset\n",
    "        temp_counter += 1\n",
    "        if temp_counter >= 0:\n",
    "            path = os.path.join('..', path[:-1])\n",
    "            #print path\n",
    "            email = open(path, \"r\")\n",
    "            \n",
    "            \n",
    "            ### use parseOutText to extract the text from the opened email\n",
    "            #email = parseOutText(email)\n",
    "            \n",
    "\n",
    "            text = parseOutText(email)\n",
    "            #print text\n",
    "            ### use str.replace() to remove any instances of the words\n",
    "            \n",
    "            drop_names = [\"sara\", \"shackleton\", \"chris\", \"germani\"]\n",
    "            for i in drop_names:\n",
    "                text = text.replace(\"i\", \"\")\n",
    "            \n",
    "            ### append the text to word_data\n",
    "            \n",
    "            word_data.append(text)\n",
    "\n",
    "            ### append a 0 to from_data if email is from Sara, and 1 if email is from Chris\n",
    "\n",
    "            if name == \"sara\":\n",
    "                from_data.append(0)\n",
    "            else: \n",
    "                from_data.append(1)\n",
    "            \n",
    "            email.close()\n",
    "\n",
    "print \"emails processed\"\n",
    "print word_data[152]\n",
    "from_sara.close()\n",
    "from_chris.close()\n",
    "\n",
    "pickle.dump( word_data, open(\"your_word_data.pkl\", \"w\") )\n",
    "pickle.dump( from_data, open(\"your_email_authors.pkl\", \"w\") )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### in Part 4, do TfIdf vectorization here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**TfIdf It**\n",
    "\n",
    "Transform the word_data into a tf-idf matrix using the sklearn TfIdf transformation. \n",
    "\n",
    "Remove english stopwords.\n",
    "\n",
    "You can access the mapping between words and feature numbers using get_feature_names(), which returns a list of all the words in the vocabulary. \n",
    "\n",
    "How many different words are there?\n",
    "\n",
    "*Me*\n",
    "\n",
    "So, I am going to start with asking can I get access the the word_data object we have already created and figure out what it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'sbale2 nonprvlegedpst susan pleas send the forego lst to rchard thank sara shackleton enron wholesal servc 1400 smth street eb3801a houston tx 77002 ph 713 8535620 fax 713 6463490 '"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like a list of strings that contain the texts of emails. I suppose that the 2 at the end of the name is an '@' sign that doesn't get shifted in the transition from whatever we started with to unicode. \n",
    "\n",
    "I will confirm that it is a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(word_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we should import the CountVectorizer and the TfIdfTransformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words = sw)\n",
    "X = vectorizer.fit_transform(word_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<17578x37890 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 1193767 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = vectorizer.get_feature_names()\n",
    "# treats numbers as features too? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37890"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<17578x37890 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 1193767 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer = TfidfTransformer()\n",
    "transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
