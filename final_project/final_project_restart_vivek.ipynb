{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final project restart: Vivek\n",
    "Import the modules, assign variables and define functions.\n",
    "\n",
    "This is the version that I am using since I got some advice from Vivek on the forum. He is really great and added a great customized function for getting the scoring function to work better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "                                                \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn import cross_validation\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from tester import test_classifier\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import\n",
    "Defining functions that I will use for data import and prep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = \"final_project_dataset.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def import_data(data):\n",
    "    '''This are the things I will do to import the data everytime, \n",
    "    regardless of what variables I make.'''\n",
    "    with open(data, \"r\") as data_file:\n",
    "        data_dict = pickle.load(data_file)\n",
    "    df = pd.DataFrame(data_dict)\n",
    "    df = df.transpose()\n",
    "    df = df.drop('email_address', axis=1)\n",
    "    df = df.astype(float)\n",
    "    df = df.drop('TOTAL')\n",
    "    df = df.drop(\"THE TRAVEL AGENCY IN THE PARK\")\n",
    "    df = df.drop(\"LOCKHART EUGENE E\")\n",
    "    df = df.drop(\"loan_advances\", axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def import_data1(data):\n",
    "    '''This are the things I will do to import the data everytime, \n",
    "    regardless of what variables I make.'''\n",
    "    with open(data, \"r\") as data_file:\n",
    "        data_dict1 = pickle.load(data_file)\n",
    "    return data_dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df1 = import_data1(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df = import_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_features_labels_new(df):\n",
    "    '''This is where the features and labels are extracted to use as arguments\n",
    "    for sklearn\\'s cross_validation function. It is also where I will do add\n",
    "    any new variables.'''\n",
    "    \n",
    "    #add columns\n",
    "#     df['deferred_ratio'] = df['deferred_income']/(df['total_payments'] + 1)\n",
    "#     df['pct_from_poi'] = df['from_poi_to_this_person']/(df['from_messages'] + 1)\n",
    "#     df['pct_to_poi'] = df['from_this_person_to_poi']/(df['from_messages'] + 1)\n",
    "#     df['to_from'] = df['pct_from_poi']*df['pct_from_poi']\n",
    "    \n",
    "    #drop columns\n",
    "    df = df.drop('restricted_stock_deferred', axis=1)\n",
    "    df = df.drop('director_fees', axis=1)\n",
    "    df = df.drop('restricted_stock', axis=1)\n",
    "\n",
    "    \n",
    "    #drop rows\n",
    "    for i in df.index:\n",
    "        if df.ix[i].count() < 3:\n",
    "            df = df.drop(i, axis=0)\n",
    "    \n",
    "    features_list = list(df.columns)\n",
    "    features_list.remove('poi')\n",
    "    features = df[features_list]\n",
    "    labels = df['poi']\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features_labels_new1(df):\n",
    "    '''This is where the features are pared and features list is created for \n",
    "    the grader. It is also where I will do add any new variables.'''\n",
    "    \n",
    "    #add columns\n",
    "#     df['deferred_ratio'] = df['deferred_income']/(df['total_payments'] + 1)\n",
    "#     df['pct_from_poi'] = df['from_poi_to_this_person']/(df['from_messages'] + 1)\n",
    "#     df['pct_to_poi'] = df['from_this_person_to_poi']/(df['from_messages'] + 1)\n",
    "#     df['to_from'] = df['pct_from_poi']*df['pct_from_poi']\n",
    "    \n",
    "    #drop columns\n",
    "    df = df.drop('restricted_stock_deferred', axis=1)\n",
    "    df = df.drop('director_fees', axis=1)\n",
    "    df = df.drop('restricted_stock', axis=1)\n",
    "    \n",
    "    #drop rows\n",
    "    for i in df.index:\n",
    "        if df.ix[i].count() < 3:\n",
    "            df = df.drop(i, axis=0)\n",
    "    \n",
    "    features_list = list(df.columns)\n",
    "    features_list.remove('poi')\n",
    "    features_list.insert(0,'poi')\n",
    "#     features = df[features_list]\n",
    "#     labels = df['poi']\n",
    "    return df, features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features_labels_remove_non_salary(df):\n",
    "    '''This is where the features and labels are extracted to use as arguments\n",
    "    for sklearn\\'s cross_validation function. It is also where I will do add\n",
    "    any new variables.'''\n",
    "    \n",
    "    #add columns\n",
    "#     df['deferred_ratio'] = df['deferred_income']/(df['total_payments'] + 1)\n",
    "#     df['pct_from_poi'] = df['from_poi_to_this_person']/(df['from_messages'] + 1)\n",
    "#     df['pct_to_poi'] = df['from_this_person_to_poi']/(df['from_messages'] + 1)\n",
    "#     df['to_from'] = df['pct_from_poi']*df['pct_from_poi']\n",
    "    \n",
    "    #drop columns\n",
    "    df = df.drop('restricted_stock_deferred', axis=1)\n",
    "    df = df.drop('director_fees', axis=1)\n",
    "    df = df.drop('restricted_stock', axis=1)\n",
    "    \n",
    "    #drop rows\n",
    "    for i in df.index:\n",
    "        if df.ix[i].count() < 3:\n",
    "            df = df.drop(i, axis=0)\n",
    "    \n",
    "    df = df[df.loc[:,'salary']>0]\n",
    "    \n",
    "    features_list = list(df.columns)\n",
    "    features_list.remove('poi')\n",
    "    features = df[features_list]\n",
    "    labels = df['poi']\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features_labels_remove_non_salary1(df):\n",
    "    '''This is where the features and labels are extracted for the \n",
    "    Udacity grader. It is also where I will do add any new variables.'''\n",
    "    \n",
    "    #add columns\n",
    "#     df['deferred_ratio'] = df['deferred_income']/(df['total_payments'] + 1)\n",
    "#     df['pct_from_poi'] = df['from_poi_to_this_person']/(df['from_messages'] + 1)\n",
    "#     df['pct_to_poi'] = df['from_this_person_to_poi']/(df['from_messages'] + 1)\n",
    "#     df['to_from'] = df['pct_from_poi']*df['pct_from_poi']\n",
    "    \n",
    "    #drop columns\n",
    "    df = df.drop('restricted_stock_deferred', axis=1)\n",
    "    df = df.drop('director_fees', axis=1)\n",
    "    df = df.drop('restricted_stock', axis=1)\n",
    "    \n",
    "    #drop rows\n",
    "    for i in df.index:\n",
    "        if df.ix[i].count() < 3:\n",
    "            df = df.drop(i, axis=0)\n",
    "    \n",
    "    df = df[df.loc[:,'salary']>0]\n",
    "    \n",
    "    features_list = list(df.columns)\n",
    "    features_list.remove('poi')\n",
    "    features_list.insert(0,'poi')\n",
    "\n",
    "#     features = df[features_list]\n",
    "#     labels = df['poi']\n",
    "#     features = df[features_list]\n",
    "#     labels = df['poi']\n",
    "    return df, features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features_labels_remove_non_salary_log(df):\n",
    "    '''This is where the features and labels are extracted to use as arguments\n",
    "    for sklearn\\'s cross_validation function. It is also where I will do add\n",
    "    any new variables.'''\n",
    "    \n",
    "    #add columns\n",
    "#     df['deferred_ratio'] = df['deferred_income']/(df['total_payments'] + 1)\n",
    "#     df['pct_from_poi'] = df['from_poi_to_this_person']/(df['from_messages'] + 1)\n",
    "#     df['pct_to_poi'] = df['from_this_person_to_poi']/(df['from_messages'] + 1)\n",
    "#     df['to_from'] = df['pct_from_poi']*df['pct_from_poi']\n",
    "    \n",
    "    #drop columns\n",
    "    df = df.drop('restricted_stock_deferred', axis=1)\n",
    "    df = df.drop('director_fees', axis=1)\n",
    "    df = df.drop('restricted_stock', axis=1)\n",
    "    \n",
    "    #drop rows\n",
    "    for i in df.index:\n",
    "        if df.ix[i].count() < 3:\n",
    "            df = df.drop(i, axis=0)\n",
    "    \n",
    "    df = df[df.loc[:,'salary']>0]\n",
    "    features_list = list(df.columns)\n",
    "    features_list.remove('poi')\n",
    "    for i in features_list:\n",
    "            df[i] = df[i] + 1\n",
    "            df[i] = np.log(df[i])\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    features = df[features_list]\n",
    "    labels = df['poi']\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_features_labels_new_27(df):\n",
    "    '''This is where the features and labels are extracted to use as arguments\n",
    "    for sklearn\\'s cross_validation function. It is also where I will do add\n",
    "    any new variables.'''\n",
    "    \n",
    "\n",
    "    \n",
    "    #add columns\n",
    "#     df['deferred_ratio'] = df['deferred_income']/(df['total_payments'] + 1)\n",
    "    df['pct_from_poi'] = df['from_poi_to_this_person']/(df['from_messages'] + 1)\n",
    "    df['pct_to_poi'] = df['from_this_person_to_poi']/(df['from_messages'] + 1)\n",
    "    df['to_from'] = df['pct_from_poi']*df['pct_from_poi']\n",
    "    \n",
    "    #drop columns\n",
    "    df = df.drop('restricted_stock_deferred', axis=1)\n",
    "    df = df.drop('director_fees', axis=1)\n",
    "    df = df.drop('restricted_stock', axis=1)\n",
    "    df = df.drop('deferral_payments', axis=1)\n",
    "    df = df.drop('deferred_income', axis=1)\n",
    "    \n",
    "   \n",
    "    #drop rows\n",
    "    for i in df.index:\n",
    "        if df.ix[i].count() < 3:\n",
    "            df = df.drop(i, axis=0)\n",
    "    \n",
    "    features_list = list(df.columns)\n",
    "    features_list.remove('poi')\n",
    "    features = df[features_list]\n",
    "    labels = df['poi']\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features_labels_new_27_1(df):\n",
    "    '''This is where the features and labels are extracted to use as arguments\n",
    "    for sklearn\\'s cross_validation function. It is also where I will do add\n",
    "    any new variables.'''\n",
    "    \n",
    "\n",
    "    \n",
    "    #add columns\n",
    "#     df['deferred_ratio'] = df['deferred_income']/(df['total_payments'] + 1)\n",
    "    df['pct_from_poi'] = df['from_poi_to_this_person']/(df['from_messages'] + 1)\n",
    "    df['pct_to_poi'] = df['from_this_person_to_poi']/(df['from_messages'] + 1)\n",
    "    df['to_from'] = df['pct_from_poi']*df['pct_from_poi']\n",
    "    \n",
    "    #drop columns\n",
    "    df = df.drop('restricted_stock_deferred', axis=1)\n",
    "    df = df.drop('director_fees', axis=1)\n",
    "    df = df.drop('restricted_stock', axis=1)\n",
    "    df = df.drop('deferral_payments', axis=1)\n",
    "    df = df.drop('deferred_income', axis=1)\n",
    "    \n",
    "   \n",
    "    #drop rows\n",
    "    for i in df.index:\n",
    "        if df.ix[i].count() < 3:\n",
    "            df = df.drop(i, axis=0)\n",
    "    \n",
    "    \n",
    "    features_list = list(df.columns)\n",
    "    features_list.remove('poi')\n",
    "    features_list.insert(0,'poi')\n",
    "#     features = df[features_list]\n",
    "#     labels = df['poi']\n",
    "    return df, features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features_labels_new1(df):\n",
    "    '''This is where the features are pared and features list is created for \n",
    "    the grader. It is also where I will do add any new variables.'''\n",
    "    \n",
    "    #add columns\n",
    "#     df['deferred_ratio'] = df['deferred_income']/(df['total_payments'] + 1)\n",
    "    df['pct_from_poi'] = df['from_poi_to_this_person']/(df['from_messages'] + 1)\n",
    "    df['pct_to_poi'] = df['from_this_person_to_poi']/(df['from_messages'] + 1)\n",
    "    df['to_from'] = df['pct_from_poi']*df['pct_from_poi']\n",
    "    \n",
    "    #drop columns\n",
    "    df = df.drop('restricted_stock_deferred', axis=1)\n",
    "    df = df.drop('director_fees', axis=1)\n",
    "    df = df.drop('restricted_stock', axis=1)\n",
    "    df = df.drop('deferral_payments', axis=1)\n",
    "    df = df.drop('deferred_income', axis=1)\n",
    "    \n",
    "    #drop rows\n",
    "    for i in df.index:\n",
    "        if df.ix[i].count() < 3:\n",
    "            df = df.drop(i, axis=0)\n",
    "    \n",
    "    features_list = list(df.columns)\n",
    "    features_list.remove('poi')\n",
    "    features_list.insert(0,'poi')\n",
    "#     features = df[features_list]\n",
    "#     labels = df['poi']\n",
    "    return df, features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features_labels_remove_non_salary_log1(df):\n",
    "    '''This is where the features and labels are extracted for the \n",
    "    Udacity grader. It is also where I will do add any new variables.'''\n",
    "    \n",
    "    #add columns\n",
    "#     df['deferred_ratio'] = df['deferred_income']/(df['total_payments'] + 1)\n",
    "#     df['pct_from_poi'] = df['from_poi_to_this_person']/(df['from_messages'] + 1)\n",
    "#     df['pct_to_poi'] = df['from_this_person_to_poi']/(df['from_messages'] + 1)\n",
    "#     df['to_from'] = df['pct_from_poi']*df['pct_from_poi']\n",
    "    \n",
    "    #drop columns\n",
    "    df = df.drop('restricted_stock_deferred', axis=1)\n",
    "    df = df.drop('director_fees', axis=1)\n",
    "    df = df.drop('restricted_stock', axis=1)\n",
    "    \n",
    "    #drop rows\n",
    "    for i in df.index:\n",
    "        if df.ix[i].count() < 3:\n",
    "            df = df.drop(i, axis=0)\n",
    "    \n",
    "    df = df[df.loc[:,'salary']>0]\n",
    "    \n",
    "\n",
    "    features_list = list(df.columns)\n",
    "    features_list.remove('poi')\n",
    "    for i in features_list:\n",
    "            df[i] = df[i] + 1\n",
    "            df[i] = np.log(df[i])\n",
    "   \n",
    "    \n",
    "\n",
    "    features_list.insert(0,'poi')\n",
    "\n",
    "\n",
    "    return df, features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features_labels_drop(df):\n",
    "    '''This is where the features and labels are extracted to use as arguments\n",
    "    for sklearn\\'s cross_validation function. It is also where I will do add\n",
    "    any new variables.\n",
    "    dropping director_fees, restricted_stock_preferred'''\n",
    "    df = df.drop('restricted_stock_deferred', axis=1)\n",
    "    df = df.drop('director_fees', axis=1)\n",
    "    df = df.drop('restricted_stock', axis=1)\n",
    "    df = df.drop('total_payments', axis=1)\n",
    "    df = df.drop('total_stock_value', axis=1)\n",
    "    \n",
    "    features_list = list(df.columns)\n",
    "    features_list.remove('poi')\n",
    "    features = df[features_list]\n",
    "    labels = df['poi']\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features_labels(df):\n",
    "    '''This is where the features and labels are extracted to use as arguments\n",
    "    for sklearn\\'s cross_validation function. This function has no new features \n",
    "    added into the data set.'''\n",
    "    features_list = list(df.columns)\n",
    "    features_list.remove('poi')\n",
    "    features = df[features_list]\n",
    "    labels = df['poi']\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#features, labels = get_features_labels_new(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_train_test_split(features, labels):\n",
    "    '''This gets the train test split for the sklearn runs of the model'''\n",
    "    from sklearn import cross_validation\n",
    "    features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "    return features_train, features_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# features_train, features_test, labels_train, labels_test = get_train_test_split(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Functions\n",
    "The functions I will call to analyze the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code from vivek_29420285151271 to replace f1 as scoring criterion\n",
    "\n",
    "def precision_recall(labels,predictions):\n",
    "    ind_true_pos = [i for i in range(0,len(labels)) if (predictions[i]==1) & (labels[i]==1)]\n",
    "    ind_false_pos = [i for i in range(0,len(labels)) if ((predictions[i]==1) & (labels[i]==0))]\n",
    "    ind_false_neg = [i for i in range(0,len(labels)) if ((predictions[i]==0) & (labels[i]==1))]\n",
    "    ind_true_neg = [i for i in range(0,len(labels)) if ((predictions[i]==0) & (labels[i]==0))]\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    \n",
    "    \n",
    "    ind_labels = [i for i in range(0,len(labels)) if labels[i]==1]\n",
    "    \n",
    "    if len(ind_labels) !=0:\n",
    "        if float( len(ind_true_pos) + len(ind_false_pos))!=0:\n",
    "            precision = float(len(ind_true_pos))/float( len(ind_true_pos) + len(ind_false_pos))\n",
    "        if float( len(ind_true_pos) + len(ind_false_neg))!=0:\n",
    "            recall = float(len(ind_true_pos))/float( len(ind_true_pos) + len(ind_false_neg))\n",
    "        return precision, recall\n",
    "    else:\n",
    "        return -1,-1\n",
    "\n",
    "def custom_scorer(labels, predictions):\n",
    "    precision,recall = precision_recall(labels,predictions)\n",
    "    min_score = min(precision, recall)\n",
    "    return min_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_outcomes(grid_object):\n",
    "    '''Gets the print out of all the outcomes from the grid_search. It prints out the \n",
    "    best parameters found by the model and the outcomes of the test of the model on \n",
    "    the test set.'''\n",
    "    print \"Best parameters from the grid search:\", pprint(gridCV_object.best_params_)\n",
    "    clf_gridCV = gridCV_object.best_estimator_\n",
    "    print \"\\nBest Estimator Accuracy:\", clf_gridCV.score(features_test, labels_test)\n",
    "    clf_gridCV_pred = clf_gridCV.predict(features_test)\n",
    "    print \"\\nRecall Score:\", recall_score(labels_test, clf_gridCV_pred)\n",
    "    print \"\\nPrecision Score:\", precision_score(labels_test, clf_gridCV_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_outcomes_whole_data_set(grid_object):\n",
    "    '''Gets the print out of all the outcomes from the grid_search. It prints out the \n",
    "    best parameters found by the model and the outcomes of the test of the model on \n",
    "    the test set, but using the whole data set rather than the train-test split.'''\n",
    "    print \"Best parameters from the grid search:\", pprint(gridCV_object.best_params_)\n",
    "    clf_gridCV = gridCV_object.best_estimator_\n",
    "    print \"\\nBest Estimator Accuracy:\", clf_gridCV.score(features_test, labels_test)\n",
    "    clf_gridCV_pred = clf_gridCV.predict(features)\n",
    "    print \"\\nRecall Score:\", recall_score(labels, clf_gridCV_pred)\n",
    "    print \"\\nPrecision Score:\", precision_score(labels, clf_gridCV_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Analysis, feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am going to import the data set separately with all the variables, including serveral that I have already decided to get rid of, to do the exploratory data analysis without preconceptions. \n",
    "\n",
    "I call it dfE for Exploratory. I create it from the dictionary, transpose it, drop the email addresses, and change the data type to float. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = \"final_project_dataset.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(data, \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "dfE = pd.DataFrame(data_dict)\n",
    "dfE = dfE.transpose()\n",
    "dfE = dfE.drop('email_address', axis=1)\n",
    "dfE = dfE.drop('TOTAL')\n",
    "dfE = dfE.drop(\"THE TRAVEL AGENCY IN THE PARK\")\n",
    "dfE = dfE.drop(\"loan_advances\", axis=1)\n",
    "dfE = dfE.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of features about each person\n",
    "len(dfE.ix[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of poi's \n",
    "len(dfE.ix[dfE['poi']==1])\n",
    "# why doesn't the second dfE need an .ix? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bonus                         63\n",
       "deferral_payments            106\n",
       "deferred_income               96\n",
       "director_fees                128\n",
       "exercised_stock_options       43\n",
       "expenses                      50\n",
       "from_messages                 58\n",
       "from_poi_to_this_person       58\n",
       "from_this_person_to_poi       58\n",
       "long_term_incentive           79\n",
       "other                         53\n",
       "poi                            0\n",
       "restricted_stock              35\n",
       "restricted_stock_deferred    127\n",
       "salary                        50\n",
       "shared_receipt_with_poi       58\n",
       "to_messages                   58\n",
       "total_payments                21\n",
       "total_stock_value             19\n",
       "dtype: int64"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#total value of stock belonging to James Prentice\n",
    "#dfE.ix[:,0]\n",
    "# how many people have no salary (count ignores null values)\n",
    "# dfE['salary'].count()\n",
    "# percentage of people with NaN for total payments\n",
    "# ntNaN = dfE['total_payments'].count()\n",
    "# n = float(len(dfE))\n",
    "# NaN = (n-ntNaN)\n",
    "# (NaN/n)*100.0\n",
    "# percentage of poi's with NaN for total_payments\n",
    "#dfE.groupby('poi')['total_payments'].count()\n",
    "dfE.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is just so little data that I am hesitant to cut out anything. Still, when over 100 cases are missing for a particular variable it seems like it is almost surely doing more harm than good, so I am going to take out the variables with more than 90 cases missing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deferral_payments\n",
      "deferred_income\n",
      "director_fees\n",
      "restricted_stock_deferred\n"
     ]
    }
   ],
   "source": [
    "for i in dfE.columns: \n",
    "    if dfE[i].isnull().sum() > 90:\n",
    "        print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bonus                      63\n",
       "exercised_stock_options    43\n",
       "expenses                   50\n",
       "from_messages              58\n",
       "from_poi_to_this_person    58\n",
       "from_this_person_to_poi    58\n",
       "long_term_incentive        79\n",
       "other                      53\n",
       "poi                         0\n",
       "restricted_stock           35\n",
       "salary                     50\n",
       "shared_receipt_with_poi    58\n",
       "to_messages                58\n",
       "total_payments             21\n",
       "total_stock_value          19\n",
       "dtype: int64"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfE.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Big correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfE.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am really hesitant to throw out any variables with such a small data set, but the variables for which we can't even compute a correlation with the dependent variable seem like they can't be helping anything and must be increasing the dimensionality of the data set. So I will drop restricted_stock_deferred and director_fees. Also, restricted_stock does not seem to be helping much, either, since it has a 0.000107 correlation with the dependent variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get a list of how many missing variables each column has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfE.count(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfE.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that some variables occur only once or a few times. This makes them rather unreliable. They may have a correlation with the dependent variable but it would only be an accident. Or they might correlate in this data set but not in other data. So, especially with the models that are subject to the cures of dimensionality like SVC. So, I will also drop loan_advances (4 instances), total_payments and total_stock_value (both with only 1 instance). UPDATE: I misread the counts for the last two variables. It was a + e02. They stay in.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get a list of how many missing variables there are in each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in dfE.index:\n",
    "    if dfE.ix[i].count() < 4:\n",
    "        dfE = dfE.drop(i, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfE_salary = dfE[dfE.loc[:,'salary']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(dfE_salary[dfE.loc[:,'poi']==1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so by getting rid of the people with non-salary information we only loose 2 poi's. Worth it? Since there are only 2 removing them only increases or decreases precision. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfE.groupby('poi')['restricted_stock_deferred'].agg(np.median).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.groupby('poi')['director_fees'].agg(np.median).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So one thing I am going to do with this information is to remove the variables that are not associated in any way with the poi's. That might at least deal with the 1.0's problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many people are in the data set? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(dfE.ix[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so I dropped the people in the data set for whom I had three or fewer pieces of information and it decreased the performance of every model. So, they are staying in. The people dropped by the standard of having three or fewer pieces of information were not poi's so I didn't think it would make that much difference, but it did. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "\n",
    "Random forests are prone to over fitting and so are also something of a problem when dealing with a small data set. Also, very high dimensional data can be a problem. [Jake VanderPlas](https://www.youtube.com/watch?v=L7R4HUQ-eQ0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.689655172414\n",
      "\n",
      "Recall Score: 0.142857142857\n",
      "\n",
      "Precision Score: 0.25\n"
     ]
    }
   ],
   "source": [
    "df = import_data(data)\n",
    "\n",
    "#df.fillna(inplace=True, value=0)\n",
    "features, labels = get_features_labels_remove_non_salary(df)\n",
    "features_train, features_test, labels_train, labels_test = get_train_test_split(features, labels)\n",
    "\n",
    "#deal with missing data\n",
    "\n",
    "imp = Imputer(strategy='mean')\n",
    "features_train = imp.fit_transform(features_train)\n",
    "features_test = imp.fit_transform(features_test)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(max_depth=6 , max_features=10 , n_estimators=1000)\n",
    "clf.fit(features_train, labels_train)\n",
    "pred = clf.predict(features_test)\n",
    "\n",
    "#caculate accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "print accuracy_score(pred, labels_test)\n",
    "\n",
    "print \"\\nRecall Score:\", recall_score(labels_test, pred)\n",
    "print \"\\nPrecision Score:\", precision_score(labels_test, pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBC with email variables added in and the five 90+ NaN columns droped\n",
      "Best parameters from the grid search:{'clf__learning_rate': 0.7,\n",
      " 'clf__max_depth': 6,\n",
      " 'clf__max_features': 0.1,\n",
      " 'pca__n_components': 4}\n",
      " None\n",
      "\n",
      "Best Estimator Accuracy: 0.8\n",
      "\n",
      "Recall Score: 0.5\n",
      "\n",
      "Precision Score: 0.285714285714\n"
     ]
    }
   ],
   "source": [
    "### Gradiant Boosting Classifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Get custom scorer\n",
    "score = make_scorer(custom_scorer, greater_is_better=True)\n",
    "\n",
    "# get the df\n",
    "df = import_data(data)\n",
    "\n",
    "# impute 0 to missing data values\n",
    "#df.fillna(inplace=True, value=0) \n",
    "\n",
    "# Get data, here with the features unrealated to poi dropped AND Tanya's features added.\n",
    "features, labels = get_features_labels_new_27(df)\n",
    "\n",
    "# Get the test-train split\n",
    "features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(features, labels)\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "features_train = imp.fit_transform(features_train)\n",
    "features_test = imp.fit_transform(features_test)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "features_train = scaler.fit_transform(features_train)\n",
    "features_test = scaler.fit_transform(features_test)\n",
    "\n",
    "# Build pipeline\n",
    "Pipeline = Pipeline([\n",
    "#        ('imp', Imputer(missing_values='NaN')),\n",
    "        ('pca', PCA()),\n",
    "        ('clf', GradientBoostingClassifier(n_estimators=100))\n",
    "    ])\n",
    "\n",
    "# Build Grid\n",
    "x = [x for x in range(2,7)]\n",
    "f = [f for f in range(3,len(features.columns))]\n",
    "param_grid = { #'imp__strategy': ['most_frequent', 'median', 'mean'],\n",
    "               'pca__n_components': x,\n",
    "               'clf__learning_rate': [0.5, 0.7, 0.8, 0.9],\n",
    "               'clf__max_features': [0.05, 0.1, 0.5, .7, 1],\n",
    "               'clf__max_depth': [3, 4, 5, 6, 7, 8]}\n",
    "\n",
    "# set model parameters to grid search object\n",
    "gridCV_object = GridSearchCV(estimator = Pipeline, \n",
    "                             param_grid = param_grid,\n",
    "                             scoring = 'f1',\n",
    "                             cv = StratifiedShuffleSplit(labels_train, test_size=0.1,  n_iter=10))\n",
    "\n",
    "# train the model\n",
    "gridCV_object.fit(features_train, labels_train)\n",
    "\n",
    "print \"GBC with email variables added in and the five 90+ NaN columns droped\"\n",
    "# apply model to test data, print results\n",
    "get_outcomes(gridCV_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to start keeping records on the results of the gradient boosting classifier as I make changes to the data set. \n",
    "\n",
    "Here the GradientBoostingClassier had n_estimators=100 and used 0's for the missing data. The data set was `get_features_labels_new_27()` and had all the columns with more than 90 NaN's (about 4) removed. \n",
    "\n",
    "```\n",
    "\n",
    "Best parameters from the grid search:{'clf__learning_rate': 0.05, 'clf__max_depth': 4, 'clf__max_features': 0.7}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.888888888889\n",
    "\n",
    "Recall Score: 0.0\n",
    "\n",
    "Precision Score: 0.0\n",
    "```\n",
    "Not very promising. \n",
    "\n",
    "### Adding Email Data\n",
    "\n",
    "GBC with email variables added in and the five 90+ NaN columns droped. Also used the grid search for imputation method. \n",
    "\n",
    "```\n",
    "Best parameters from the grid search:{'clf__learning_rate': 0.05,\n",
    " 'clf__max_depth': 4,\n",
    " 'clf__max_features': 0.5,\n",
    " 'imp__strategy': 'mean'}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.8\n",
    "\n",
    "Recall Score: 0.142857142857\n",
    "\n",
    "Precision Score: 0.5\n",
    "```\n",
    "Just can't figure out how to get the recall up! \n",
    "\n",
    "### MinMaxScaling before Standardization\n",
    "GBC with email variables added in and the five 90+ NaN columns droped\n",
    "\n",
    "Did MinMax Scaling and imputing mean values for missing data before doing grid search. Only did 10 iternations. Going to try 1000\n",
    "```\n",
    "Best parameters from the grid search:{'clf__learning_rate': 0.1, 'clf__max_depth': 3, 'clf__max_features': 0.5}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.828571428571\n",
    "\n",
    "Recall Score: 0.333333333333\n",
    "\n",
    "Precision Score: 0.5\n",
    "```\n",
    "\n",
    "### With 100 iterations\n",
    "Somehow it gets worse on precision while getting better on Recall. There might be something wrong with the score function: \n",
    "\n",
    "```\n",
    "GBC with email variables added in and the five 90+ NaN columns droped\n",
    "Best parameters from the grid search:{'clf__learning_rate': 0.5, 'clf__max_depth': 4, 'clf__max_features': 0.5}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.685714285714\n",
    "\n",
    "Recall Score: 0.5\n",
    "\n",
    "Precision Score: 0.0909090909091\n",
    "```\n",
    "Maybe there is something wrong with the score function I have borrowed so I will try the f1:\n",
    "### With f1 as scoring function\n",
    "\n",
    "\n",
    "GBC with email variables added in and the five 90+ NaN columns droped and MinMaxScaling done before hand. \n",
    "Also, 100 iterations.\n",
    "```\n",
    "Best parameters from the grid search:{'clf__learning_rate': 0.05, 'clf__max_depth': 3, 'clf__max_features': 1}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.828571428571\n",
    "\n",
    "Recall Score: 0.25\n",
    "\n",
    "Precision Score: 0.25\n",
    "```\n",
    "I am going to stick with the f1 scorer for a while now. Next I will try the PCA before giving it GBC.\n",
    "\n",
    "### With PCA in pipeline\n",
    "Got a huge increase in Recall but precision is bad again. \n",
    "\n",
    "```\n",
    "GBC with email variables added in and the five 90+ NaN columns droped\n",
    "Best parameters from the grid search:{'clf__learning_rate': 0.05,\n",
    " 'clf__max_depth': 4,\n",
    " 'clf__max_features': 0.7,\n",
    " 'pca__n_components': 3}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.542857142857\n",
    "\n",
    "Recall Score: 0.6\n",
    "\n",
    "Precision Score: 0.176470588235\n",
    "```\n",
    "Just to keep ourselves sane we will try it with the custom score function. \n",
    "\n",
    "\n",
    "\n",
    "### w/ Custom Scorer\n",
    "\n",
    "```\n",
    "GBC with email variables added in and the five 90+ NaN columns droped\n",
    "Best parameters from the grid search:{'clf__learning_rate': 0.1,\n",
    " 'clf__max_depth': 4,\n",
    " 'clf__max_features': 0.7,\n",
    " 'pca__n_components': 10}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.685714285714\n",
    "\n",
    "Recall Score: 0.2\n",
    "\n",
    "Precision Score: 0.125\n",
    "```\n",
    "Ok, so the custom scorer sucks. No more of that. I will try the f1 scorer with 100 iterations, though. \n",
    "\n",
    "### F1 with 100 iterations\n",
    "Gave up. Took too long\n",
    "Now adding parameters. \n",
    "\n",
    "```\n",
    "GBC with email variables added in and the five 90+ NaN columns droped\n",
    "Best parameters from the grid search:{'clf__learning_rate': 0.7,\n",
    " 'clf__max_depth': 6,\n",
    " 'clf__max_features': 0.1,\n",
    " 'pca__n_components': 4}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.8\n",
    "\n",
    "Recall Score: 0.5\n",
    "\n",
    "Precision Score: 0.285714285714\n",
    "```\n",
    "That was pretty good with only 10 iterations. The learning rate was the highest number in the list so there might still be room or improvement. \n",
    "\n",
    "### Adding higher learning rate choice\n",
    "Trimming the pca and learning rate numbers of alternatives that were rejected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.880952380952\n",
      "\n",
      "Recall Score: 0.25\n",
      "\n",
      "Precision Score: 0.333333333333\n"
     ]
    }
   ],
   "source": [
    "df = import_data(data)\n",
    "\n",
    "#df.fillna(inplace=True, value=0)\n",
    "features, labels = get_features_labels_new_27(df)\n",
    "features_train, features_test, labels_train, labels_test = get_train_test_split(features, labels)\n",
    "\n",
    "#deal with missing data\n",
    "from sklearn.svm import LinearSVC\n",
    "imp = Imputer(strategy='most_frequent')\n",
    "features_train = imp.fit_transform(features_train)\n",
    "features_test = imp.fit_transform(features_test)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "features_train = scaler.fit_transform(features_train)\n",
    "features_test = scaler.fit_transform(features_test)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3)\n",
    "features_train = pca.fit_transform(features_train)\n",
    "features_test = pca.fit_transform(features_test)\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "clf.fit(features_train, labels_train)\n",
    "pred = clf.predict(features_test)\n",
    "\n",
    "#caculate accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "print accuracy_score(pred, labels_test)\n",
    "\n",
    "print \"\\nRecall Score:\", recall_score(labels_test, pred)\n",
    "print \"\\nPrecision Score:\", precision_score(labels_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = import_data(data)\n",
    "\n",
    "# impute 0 to missing data values\n",
    "# df.fillna(inplace=True, value=0) \n",
    "features, labels = get_features_labels_remove_non_salary(df)\n",
    "features_train, features_test, labels_train, labels_test = get_train_test_split(features, labels)\n",
    "\n",
    "#deal with missing data\n",
    "from sklearn.svm import LinearSVC\n",
    "imp = Imputer(strategy='most_frequent')\n",
    "features_train = imp.fit_transform(features_train)\n",
    "features_test = imp.fit_transform(features_test)\n",
    "\n",
    "#LinearSVC without pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "clf = LinearSVC()\n",
    "clf.fit(features_train, labels_train)\n",
    "pred = clf.predict(features_test)\n",
    "\n",
    "#caculate accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "print accuracy_score(pred, labels_test)\n",
    "\n",
    "print \"\\nRecall Score:\", recall_score(labels_test, pred)\n",
    "print \"\\nPrecision Score:\", precision_score(labels_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA explained Variance: array([ 0.44725245,  0.17013664,  0.12259992,  0.07351105,  0.05803404,\n",
      "        0.04368933,  0.02719228,  0.01921261,  0.01375639])\n",
      "\n",
      "0.833333333333\n",
      "\n",
      "Recall Score: 0.5\n",
      "\n",
      "Precision Score: 0.285714285714\n"
     ]
    }
   ],
   "source": [
    "#import data\n",
    "df = import_data(data)\n",
    "# df.fillna(inplace=True, value=0)\n",
    "features, labels = get_features_labels_new_27(df)\n",
    "features_train, features_test, labels_train, labels_test = get_train_test_split(features, labels)\n",
    "# print \"after split: %r\\n\" %features_train[1:2]\n",
    "\n",
    "#deal with missing data\n",
    "from sklearn.preprocessing import Imputer\n",
    "imp = Imputer(missing_values='NaN', strategy=\"most_frequent\")\n",
    "features_train = imp.fit_transform(features_train)\n",
    "features_test = imp.fit_transform(features_test)\n",
    "# print \"after imputer: %r\\n\" %features_train[1:2]\n",
    "\n",
    "#Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "features_train = scaler.fit_transform(features_train)\n",
    "features_test = scaler.fit_transform(features_test)\n",
    "# print \"after scaling: %r\\n\" %features_train[1:2]\n",
    "\n",
    "#PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=9)\n",
    "features_train = pca.fit_transform(features_train)\n",
    "features_test = pca.fit_transform(features_test)\n",
    "# print \"after scaling: %r\\n\" %features_train[1:2]\n",
    "print \"PCA explained Variance: %r\\n\" %pca.explained_variance_ratio_\n",
    "\n",
    "#LinearSVC without pipeline\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel='rbf', C=10000, gamma=0.01)\n",
    "clf.fit(features_train, labels_train)\n",
    "pred = clf.predict(features_test)\n",
    "\n",
    "# caculate accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "print accuracy_score(pred, labels_test)\n",
    "\n",
    "print \"\\nRecall Score:\", recall_score(labels_test, pred)\n",
    "print \"\\nPrecision Score:\", precision_score(labels_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "SVC results: Well, the linear kernel just won't work. There is obviously no linear separating plane in this data. The only kernels that finish in a reasonable amount of time are the sigmoid and rbf kernels. The gamma and C parameters don't seem to have any effect on the accuracy of the model. \n",
    "\n",
    "I looked at the predictions and I think the problem is that the model predicts that no one is a poi. This gives about 88% accuracy because 88% of the people in the test set are not poi's. In fact, when I test this by running the code: \n",
    "```\n",
    "1-(5/float(len(labels_test)))\n",
    "```\n",
    "I get the exact same number (0.886363636364) as I was getting from the accuracy_score. The recall and accuracy just stay at 0.0 for both. It just doesn't care!\n",
    "\n",
    "UPDATE: got it to perform great on the SVC here but it did lousey on the grader. With pca=9, C=10000000, gamma = 0.01 with the new columns outIt gave me like \n",
    "```\n",
    "0.904761904762\n",
    "\n",
    "Recall Score: 0.8\n",
    "\n",
    "Precision Score: 0.571428571429\n",
    "```\n",
    "But was an utter fail on the grader. Both precision and recall under 0.3. It is starting to feel like this is all bullshit! Why even bother if you get completely different results on one measure compared to the other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "print range(1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters from the grid search:{'clf__learning_rate': 0.6,\n",
      " 'clf__max_depth': 9,\n",
      " 'pca__n_components': 3,\n",
      " 'selection__k': 16}\n",
      " None\n",
      "\n",
      "Best Estimator Accuracy: 1.0\n",
      "\n",
      "Recall Score: 1.0\n",
      "\n",
      "Precision Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "# SVC rbf\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# Get custom scorer\n",
    "score = make_scorer(custom_scorer, greater_is_better=True)\n",
    "\n",
    "# get the df\n",
    "df = import_data(data)\n",
    "\n",
    "# Get data, here with the features unrealated to poi dropped AND Tanya's features added.\n",
    "features, labels = get_features_labels_new_27(df)\n",
    "\n",
    "# Get the test-train split\n",
    "#features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(features, labels)\n",
    "\n",
    "# Build pipeline\n",
    "Pipeline = Pipeline([\n",
    "        ('imp', Imputer(missing_values='NaN', strategy='median')),\n",
    "        ('std', MinMaxScaler()),\n",
    "        ('selection', SelectKBest()),\n",
    "        ('pca', PCA()),\n",
    "        ('clf', GradientBoostingClassifier(random_state=0, loss=\"exponential\", n_estimators=100))\n",
    "    ])\n",
    "\n",
    "# Build Grid\n",
    "k = [k for k in range(15,17)]\n",
    "x = [x for x in range(2,5)]\n",
    "# c = [1000000,100000,10000,100,10,0.01]\n",
    "# g = [0.1,0.01,0.001,0.0001]\n",
    "d = [d for d in range(8,13)]\n",
    "#e = [50, 100]\n",
    "r = [0.3, 0.5, 0.6, 0.7]\n",
    "#l = [\"deviance\", \"exponential\"]\n",
    "param_grid = {'selection__k': k,\n",
    "              'pca__n_components': x,\n",
    "              #'imp__strategy': ['most_frequent', 'mean', 'median'],\n",
    "              #'clf__n_estimators': e,\n",
    "              'clf__learning_rate': r,\n",
    "              'clf__max_depth': d#,\n",
    "              #'clf__loss': l\n",
    "             }\n",
    "\n",
    "# set model parameters to grid search object\n",
    "gridCV_object = GridSearchCV(estimator = Pipeline, \n",
    "                             param_grid = param_grid,\n",
    "                             scoring = score,\n",
    "                             cv = StratifiedShuffleSplit(labels, test_size=0.1,  n_iter=10))\n",
    "\n",
    "# train the model\n",
    "gridCV_object.fit(features, labels)\n",
    "\n",
    "# apply model to test data, print results\n",
    "get_outcomes(gridCV_object)\n",
    "\n",
    "#print gridCV_object.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Same model submitted to the grader: \n",
    "\n",
    "Start from begining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import data\n",
    "#import data\n",
    "df = import_data(data)\n",
    "df, features_list = get_features_labels_new_27_1(df)\n",
    "# print \"after split: %r\\n\" %features_train[1:2]\n",
    "\n",
    "\n",
    "\n",
    "#change back to dictionary\n",
    "df1 = df.transpose()\n",
    "df1 = df1.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "And these are the results going through the test classifier:\n",
      "\n",
      "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=15, score_func=<function f_classif at 0x10e2f40c8>)), ('pca', PCA(copy=True, n_components=3, whiten=False)), ('clf', ...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False))])\n",
      "\tAccuracy: 0.82329\tPrecision: 0.37092\tRecall: 0.34050\tF1: 0.35506\tF2: 0.34618\n",
      "\tTotal predictions: 14000\tTrue positives:  681\tFalse positives: 1155\tFalse negatives: 1319\tTrue negatives: 10845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tester import test_classifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# clf = RandomForestClassifier(max_depth=7 , max_features=10 , n_estimators=1000)\n",
    "\n",
    "clf = Pipeline([\n",
    "        ('imp', Imputer(missing_values='NaN', strategy='median')),\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('selection', SelectKBest(k=15)),\n",
    "        ('pca', PCA(n_components=3)),\n",
    "        ('clf', GradientBoostingClassifier(n_estimators=100,loss=\"exponential\", max_depth=9, learning_rate=0.7))\n",
    "    ])\n",
    "\n",
    "print \"\\n\\nAnd these are the results going through the test classifier:\\n\"\n",
    "test_classifier(clf, df1, features_list, folds = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grader Results\n",
    "\n",
    "### SVC\n",
    "Here are some results from the grader: \n",
    "```\n",
    "\n",
    "And these are the results going through the test classifier:\n",
    "\n",
    "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='most_frequent',\n",
    "    verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('pca', PCA(copy=True, n_components=9, whiten=False)), ('clf', SVC(C=10000, cache_size=200, class_weight=None, coef0=0.0,\n",
    "  decision_function_shape=None, degree=3, gamma=0.1, kernel='rbf',\n",
    "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "  tol=0.001, verbose=False))])\n",
    "\tAccuracy: 0.81729\tPrecision: 0.31645\tRecall: 0.24050\tF1: 0.27330\tF2: 0.25263\n",
    "\tTotal predictions: 14000\tTrue positives:  481\tFalse positives: 1039\tFalse negatives: 1519\tTrue negatives: 10961\n",
    "```\n",
    "The thing is that the precision was there but the recall was not. Those parameters came from running the SVC with f1 scoring with a training-test split. It worked well in with the StratifiedShuffleSplit tester with 100 iterations but did not break the 0.3 thresholds for both with the Udacity Grader. So now I am going to try new parameters found by running the StratifiedShuffleSplit test with the full data set.  \n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "And these are the results going through the test classifier:\n",
    "\n",
    "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='most_frequent',\n",
    "    verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('pca', PCA(copy=True, n_components=6, whiten=False)), ('clf', SVC(C=1000000, cache_size=200, class_weight=None, coef0=0.0,\n",
    "  decision_function_shape=None, degree=3, gamma=0.1, kernel='rbf',\n",
    "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "  tol=0.001, verbose=False))])\n",
    "\tAccuracy: 0.76743\tPrecision: 0.20572\tRecall: 0.21950\tF1: 0.21239\tF2: 0.21660\n",
    "\tTotal predictions: 14000\tTrue positives:  439\tFalse positives: 1695\tFalse negatives: 1561\tTrue negatives: 10305\n",
    "```\n",
    "So these results are even worse. The lesson is that using the whole data set gives even more misleading parameters for the grader. \n",
    "\n",
    "\n",
    "### Select K Best\n",
    "So here is what is driving me crazy. I get this result from using select K Best with the SVC: \n",
    "```\n",
    "Best parameters from the grid search:{'selection__k': 13}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.971428571429\n",
    "\n",
    "Recall Score: 0.833333333333\n",
    "\n",
    "Precision Score: 1.0\n",
    "```\n",
    "Great, heh? But when I give it to the grader: \n",
    "```\n",
    "And these are the results going through the test classifier:\n",
    "\n",
    "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='most_frequent',\n",
    "    verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=13, score_func=<function f_classif at 0x10e2f40c8>)), ('pca', PCA(copy=True, n_components=6, whiten=False)...,\n",
    "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "  tol=0.001, verbose=False))])\n",
    "\tAccuracy: 0.76779\tPrecision: 0.20172\tRecall: 0.21150\tF1: 0.20649\tF2: 0.20947\n",
    "\tTotal predictions: 14000\tTrue positives:  423\tFalse positives: 1674\tFalse negatives: 1577\tTrue negatives: 10326\n",
    "```\n",
    "\n",
    "How am I going to make this better? \n",
    "\n",
    "### Winner? \n",
    "\n",
    "\n",
    "And these are the results going through the test classifier:\n",
    "```\n",
    "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='most_frequent',\n",
    "    verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=16, score_func=<function f_classif at 0x10e2f40c8>)), ('pca', PCA(copy=True, n_components=3, whiten=False)...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False))])\n",
    "\tAccuracy: 0.82979\tPrecision: 0.37872\tRecall: 0.29900\tF1: 0.33417\tF2: 0.31214\n",
    "\tTotal predictions: 14000\tTrue positives:  598\tFalse positives:  981\tFalse negatives: 1402\tTrue negatives: 11019\n",
    "```\n",
    "Close enough for government work I would say. \n",
    "\n",
    "One more test\n",
    "\n",
    "## Victory!\n",
    "\n",
    "And these are the results going through the test classifier:\n",
    "```\n",
    "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=16, score_func=<function f_classif at 0x10e2f40c8>)), ('pca', PCA(copy=True, n_components=3, whiten=False)), ('clf', Gr...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False))])\n",
    "\tAccuracy: 0.81564\tPrecision: 0.33924\tRecall: 0.30650\tF1: 0.32204\tF2: 0.31253\n",
    "\tTotal predictions: 14000\tTrue positives:  613\tFalse positives: 1194\tFalse negatives: 1387\tTrue negatives: 10806\n",
    "```    \n",
    "### Again, but with the custom scoring function\n",
    "\n",
    "\n",
    "\n",
    "And these are the results going through the test classifier:\n",
    "\n",
    "```\n",
    "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=16, score_func=<function f_classif at 0x10e2f40c8>)), ('pca', PCA(copy=True, n_components=3, whiten=False)), ('clf', ...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False))])\n",
    "\tAccuracy: 0.82664\tPrecision: 0.38757\tRecall: 0.36800\tF1: 0.37753\tF2: 0.37175\n",
    "\tTotal predictions: 14000\tTrue positives:  736\tFalse positives: 1163\tFalse negatives: 1264\tTrue negatives: 10837\n",
    "```\n",
    "So now I finally have something that works well. The thing is that the function gives me 1.0 on all the parameters so the answers at this point from the tests are just arbitrary? Maybe there are lots of parameters that give me 1.0 and it is just stopping at the first one that gives me that? \n",
    "\n",
    "### more\n",
    "\n",
    "And these are the results going through the test classifier:\n",
    "```\n",
    "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=16, score_func=<function f_classif at 0x10e2f40c8>)), ('pca', PCA(copy=True, n_components=3, whiten=False)), ('clf', ...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False))])\n",
    "\tAccuracy: 0.82586\tPrecision: 0.38510\tRecall: 0.36700\tF1: 0.37583\tF2: 0.37048\n",
    "\tTotal predictions: 14000\tTrue positives:  734\tFalse positives: 1172\tFalse negatives: 1266\tTrue negatives: 10828\n",
    "```   \n",
    "## best?\n",
    "\n",
    "And these are the results going through the test classifier:\n",
    "\n",
    "```\n",
    "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=16, score_func=<function f_classif at 0x10e2f40c8>)), ('pca', PCA(copy=True, n_components=3, whiten=False)), ('clf', ...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False))])\n",
    "\tAccuracy: 0.82450\tPrecision: 0.38203\tRecall: 0.37000\tF1: 0.37592\tF2: 0.37235\n",
    "\tTotal predictions: 14000\tTrue positives:  740\tFalse positives: 1197\tFalse negatives: 1260\tTrue negatives: 10803\n",
    "```\n",
    "### Diminishing returns? \n",
    "Tried moving the learning rate up to 0.8 from 0.7. \n",
    "\n",
    "```\n",
    "\n",
    "And these are the results going through the test classifier:\n",
    "\n",
    "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=16, score_func=<function f_classif at 0x10e2f40c8>)), ('pca', PCA(copy=True, n_components=3, whiten=False)), ('clf', ...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False))])\n",
    "\tAccuracy: 0.82464\tPrecision: 0.38108\tRecall: 0.36450\tF1: 0.37260\tF2: 0.36770\n",
    "\tTotal predictions: 14000\tTrue positives:  729\tFalse positives: 1184\tFalse negatives: 1271\tTrue negatives: 10816\n",
    "```\n",
    "### Not much better. Learning rate 0.6\n",
    "```\n",
    "\n",
    "And these are the results going through the test classifier:\n",
    "\n",
    "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=16, score_func=<function f_classif at 0x10e2f40c8>)), ('pca', PCA(copy=True, n_components=3, whiten=False)), ('clf', ...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False))])\n",
    "\tAccuracy: 0.82543\tPrecision: 0.38279\tRecall: 0.36250\tF1: 0.37237\tF2: 0.36638\n",
    "\tTotal predictions: 14000\tTrue positives:  725\tFalse positives: 1169\tFalse negatives: 1275\tTrue negatives: 10831\n",
    "```\n",
    "\n",
    "## Exponential Wins!\n",
    "The best score yet. Maybe the thing was getting 1's on everything so it was just getting me the first thing that showed up? \n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "And these are the results going through the test classifier:\n",
    "\n",
    "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=16, score_func=<function f_classif at 0x10e2f40c8>)), ('pca', PCA(copy=True, n_components=3, whiten=False)), ('clf', ...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False))])\n",
    "\tAccuracy: 0.82864\tPrecision: 0.39394\tRecall: 0.37050\tF1: 0.38186\tF2: 0.37496\n",
    "\tTotal predictions: 14000\tTrue positives:  741\tFalse positives: 1140\tFalse negatives: 1259\tTrue negatives: 10860\n",
    "```\n",
    "\n",
    "### Going Down, reduced select K best to 15\n",
    "\n",
    "```\n",
    "\n",
    "And these are the results going through the test classifier:\n",
    "\n",
    "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=15, score_func=<function f_classif at 0x10e2f40c8>)), ('pca', PCA(copy=True, n_components=3, whiten=False)), ('clf', ...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False))])\n",
    "\tAccuracy: 0.82250\tPrecision: 0.36668\tRecall: 0.33350\tF1: 0.34931\tF2: 0.33965\n",
    "\tTotal predictions: 14000\tTrue positives:  667\tFalse positives: 1152\tFalse negatives: 1333\tTrue negatives: 10848\n",
    "```\n",
    "\n",
    "### Max Depth up to 10 doesn't help\n",
    "\n",
    "And these are the results going through the test classifier:\n",
    "```\n",
    "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=16, score_func=<function f_classif at 0x10e2f40c8>)), ('pca', PCA(copy=True, n_components=3, whiten=False)), ('clf', ...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False))])\n",
    "\tAccuracy: 0.82643\tPrecision: 0.38660\tRecall: 0.36650\tF1: 0.37628\tF2: 0.37035\n",
    "\tTotal predictions: 14000\tTrue positives:  733\tFalse positives: 1163\tFalse negatives: 1267\tTrue negatives: 10837\n",
    "```\n",
    "\n",
    "## Highest Precision from highest learning rate, 0.8\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "And these are the results going through the test classifier:\n",
    "\n",
    "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=16, score_func=<function f_classif at 0x10e2f40c8>)), ('pca', PCA(copy=True, n_components=3, whiten=False)), ('clf', ...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False))])\n",
    "\tAccuracy: 0.82800\tPrecision: 0.39137\tRecall: 0.36750\tF1: 0.37906\tF2: 0.37204\n",
    "\tTotal predictions: 14000\tTrue positives:  735\tFalse positives: 1143\tFalse negatives: 1265\tTrue negatives: 10857\n",
    "```\n",
    "\n",
    "### Learning Rate at 0.9 no better\n",
    "```\n",
    "\n",
    "And these are the results going through the test classifier:\n",
    "\n",
    "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=16, score_func=<function f_classif at 0x10e2f40c8>)), ('pca', PCA(copy=True, n_components=3, whiten=False)), ('clf', ...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False))])\n",
    "\tAccuracy: 0.82629\tPrecision: 0.38387\tRecall: 0.35700\tF1: 0.36995\tF2: 0.36207\n",
    "\tTotal predictions: 14000\tTrue positives:  714\tFalse positives: 1146\tFalse negatives: 1286\tTrue negatives: 10854\n",
    "```\n",
    "### Trying Whiten=True\n",
    "```\n",
    "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=16, score_func=<function f_classif at 0x10e2f40c8>)), ('pca', PCA(copy=True, n_components=3, whiten=True)), ('clf', G...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False))])\n",
    "\tAccuracy: 0.82479\tPrecision: 0.38035\tRecall: 0.36000\tF1: 0.36989\tF2: 0.36389\n",
    "\tTotal predictions: 14000\tTrue positives:  720\tFalse positives: 1173\tFalse negatives: 1280\tTrue negatives: 10827\n",
    "```\n",
    "### Tried Deviance \n",
    "```\n",
    "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=16, score_func=<function f_classif at 0x10e2f40c8>)), ('pca', PCA(copy=True, n_components=3, whiten=True)), ('clf', G...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False))])\n",
    "\tAccuracy: 0.82421\tPrecision: 0.37989\tRecall: 0.36450\tF1: 0.37203\tF2: 0.36748\n",
    "\tTotal predictions: 14000\tTrue positives:  729\tFalse positives: 1190\tFalse negatives: 1271\tTrue negatives: 10810\n",
    "```\n",
    "\n",
    "Seems like going to back to exponential is the right idea.\n",
    "\n",
    "###   GradientBoostingClassifier(n_estimators=100,loss=\"exponential\", max_depth=8, learning_rate=0.8)\n",
    "```\n",
    "\n",
    "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=16, score_func=<function f_classif at 0x10e2f40c8>)), ('pca', PCA(copy=True, n_components=3, whiten=True)), ('clf', G...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False))])\n",
    "\tAccuracy: 0.82657\tPrecision: 0.38593\tRecall: 0.36200\tF1: 0.37358\tF2: 0.36655\n",
    "\tTotal predictions: 14000\tTrue positives:  724\tFalse positives: 1152\tFalse negatives: 1276\tTrue negatives: 10848\n",
    "```\n",
    "### 'n_estimators=100,loss=\"exponential\", max_depth=9, learning_rate=0.7\n",
    "whiten was false\n",
    "```\n",
    "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=16, score_func=<function f_classif at 0x10e2f40c8>)), ('pca', PCA(copy=True, n_components=3, whiten=False)), ('clf', ...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False))])\n",
    "\tAccuracy: 0.82514\tPrecision: 0.38272\tRecall: 0.36550\tF1: 0.37391\tF2: 0.36882\n",
    "\tTotal predictions: 14000\tTrue positives:  731\tFalse positives: 1179\tFalse negatives: 1269\tTrue negatives: 10821\n",
    "```\n",
    "### kbest = 16, PCA(n_components=4) n_estimators=100, loss=\"exponential\", max_depth=9, learning_rate=0.7\n",
    "\n",
    "\n",
    "```\n",
    "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=16, score_func=<function f_classif at 0x10e2f40c8>)), ('pca', PCA(copy=True, n_components=4, whiten=False)), ('clf', ...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False))])\n",
    "\tAccuracy: 0.81100\tPrecision: 0.32910\tRecall: 0.31100\tF1: 0.31979\tF2: 0.31446\n",
    "\tTotal predictions: 14000\tTrue positives:  622\tFalse positives: 1268\tFalse negatives: 1378\tTrue negatives: 10732\n",
    "```\n",
    "\n",
    "### , SelectKBest(k=15)),PCA(n_components=3) n_estimators=100,loss=\"exponential\", max_depth=9, learning_rate=0.7))\n",
    "\n",
    "```\n",
    "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=15, score_func=<function f_classif at 0x10e2f40c8>)), ('pca', PCA(copy=True, n_components=3, whiten=False)), ('clf', ...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False))])\n",
    "\tAccuracy: 0.82329\tPrecision: 0.37092\tRecall: 0.34050\tF1: 0.35506\tF2: 0.34618\n",
    "\tTotal predictions: 14000\tTrue positives:  681\tFalse positives: 1155\tFalse negatives: 1319\tTrue negatives: 10845\n",
    "```\n",
    "I think I have enough to go on now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "First attempt with the new scorer. Not sure whether this is the old or the new data set. \n",
    "```\n",
    "\n",
    "Best parameters from the grid search:{'clf__max_depth': 8, 'imp__strategy': 'median', 'pca__n_components': 9}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.772727272727\n",
    "\n",
    "\n",
    "Recall Score: 0.2\n",
    "\n",
    "\n",
    "Precision Score: 0.142857142857\n",
    "```\n",
    "This time I am doing it with the original data set for sure. \n",
    "```\n",
    "\n",
    "Best parameters from the grid search:{'clf__max_depth': 5,\n",
    " 'imp__strategy': 'most_frequent',\n",
    " 'pca__n_components': 10}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.886363636364\n",
    "\n",
    "Recall Score: 0.2\n",
    "\n",
    "Precision Score: 0.5\n",
    "```\n",
    "With the original data in the data set we seem to do a little better than with the new, created variables. Precision seems to be a lot better with the original, untreated data. \n",
    " \n",
    "I am running it one more time with \\_new data just to make sure that was the problem. \n",
    "```\n",
    "Best parameters from the grid search:{'clf__max_depth': 9, 'imp__strategy': 'median', 'pca__n_components': 4}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.818181818182\n",
    "\n",
    "Recall Score: 0.2\n",
    "\n",
    "Precision Score: 0.2\n",
    "```\n",
    "Ok, that is not exactly the result we got before but it still shows the data with additional data to perform less well. \n",
    "\n",
    "Also, have to run Vivek's suggestion of limiting the depth of the trees to 2 or 3 or setting a minimum sample split because there is so little data. So here is one more time with the max depth set lower. That should not increase the preformance of the model (since our scoring features is selecting the model that performs the best) but it might provide some insight. Also, again following Vivek's suggestion, I will limit the principle component analysis to 10. \n",
    "```\n",
    "Best parameters from the grid search:{'clf__max_depth': 3, 'imp__strategy': 'most_frequent', 'pca__n_components': 3}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.75\n",
    "\n",
    "Recall Score: 0.2\n",
    "\n",
    "Precision Score: 0.125\n",
    "```\n",
    "Yeah, not such hot performance but at least is makes more sense. \n",
    "\n",
    "Now I am going to see if the model can work better with the two variables that have no association with the dependent variable, the 'drop' data set. \n",
    "```\n",
    "\n",
    "Best parameters from the grid search:{'clf__max_depth': 3, 'imp__strategy': 'most_frequent', 'pca__n_components': 6}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.977272727273\n",
    "\n",
    "Recall Score: 1.0\n",
    "\n",
    "Precision Score: 0.833333333333\n",
    "```\n",
    "Now, with that as a base line lets see if the Robust Scaler works better. \n",
    "```\n",
    "\n",
    "Best parameters from the grid search:{'clf__max_depth': 3, 'imp__strategy': 'most_frequent', 'pca__n_components': 7}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.909090909091\n",
    "\n",
    "Recall Score: 0.2\n",
    "\n",
    "Precision Score: 1.0\n",
    "```\n",
    "Ok, this is a lot worse. In fact, it is so much worse that I can't believe these results are correct. I must have changed something other than the just the scaler. \n",
    "\n",
    "These are the results with the two 0 poi variables dropped and the grid search run on the whole data set. \n",
    "```\n",
    "Best parameters from the grid search:{'clf__max_depth': 2, 'imp__strategy': 'most_frequent', 'pca__n_components': 8}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.909090909091\n",
    "\n",
    "Recall Score: 0.444444444444\n",
    "\n",
    "Precision Score: 0.888888888889\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results with 1000, 10 and 100 iterations\n",
    "\n",
    "### 1000 iterations\n",
    "So the first outcome from the model is pretty good. It is StratifiedShuffleSplit with 1000 iterations. The outcome was really good but it took forever. \n",
    "\n",
    "```\n",
    "Best parameters from the grid search:{'clf__max_depth': 9, 'pca__n_components': 10}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.886363636364\n",
    "\n",
    "\n",
    "Recall Score: 0.6\n",
    "\n",
    "\n",
    "Precision Score: 0.5\n",
    "```\n",
    "### 10 iterations\n",
    "This is great but I just can't use this for testing things out. It will take forever. So, I am going to try it with 10 iterations and see what happens. \n",
    "\n",
    "```\n",
    "\n",
    "Best parameters from the grid search:{'clf__max_depth': 9, 'pca__n_components': 12}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.863636363636\n",
    "\n",
    "\n",
    "Recall Score: 0.4\n",
    "\n",
    "\n",
    "Precision Score: 0.4\n",
    "```\n",
    "### 100\n",
    "Now we get the same depth but 12 instead of 10 principle components. The scores have gone down though. It is worth trying it at a higher number of iterations. I am going to try 100. Here is the outcome: \n",
    "```\n",
    "Best Estimator Accuracy: 0.818181818182\n",
    "\n",
    "\n",
    "Recall Score: 0.4\n",
    "\n",
    "\n",
    "Precision Score: 0.285714285714\n",
    "```\n",
    "Ok, so 100 iterations is worse than either 1000 or 10 iterations. That is kind of distressing. It would be nice if the behavior of the model was 'monotonic', that is, the more of one thing you do the more of something you are looking for you get. I was thinking that 100 iterations could serve as a good way to explore possibilities and narrow the search space while reserving 1000 iteration runs to make the final cut. Now I am not quite sure what to do. \n",
    "\n",
    "And here is another thing. I just ran the model again with 1000 iterations with the single addition of the clf's criterion being 'gini' or 'entropy'. It came back with gini as the better criterion. And the model had performance that was just as good as before on the 1000 iterations. In fact, the three scores of interest--accuracy, precision and recall--were exactly the same. But it found a max depth of 7 instead of 9. \n",
    "```\n",
    "Best parameters from the grid search:{'clf__criterion': 'gini', 'clf__max_depth': 7, 'pca__n_components': 10}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.886363636364\n",
    "\n",
    "\n",
    "Recall Score: 0.6\n",
    "\n",
    "\n",
    "Precision Score: 0.5\n",
    "```\n",
    "The reason that is so puzzling to me is that gini is the default scoring criterion for the model. So it was using gini before when it found the best preformance was at max depth of 9 instead of 7. How does that happen?\n",
    "\n",
    "### Adding variables\n",
    "\n",
    "In this iteration I try adding some new variables and the choice between the median and most_frequent imputation for the missing values. I also limited the search space for 'max_depth' and the number of components to 6-8 and 8-12 respectively. That may have been a mistake because I had found that the most effective max depth was 9. I don't know why I did that. \n",
    "```\n",
    "\n",
    "Best parameters from the grid search:{'clf__max_depth': 7, 'imp__strategy': 'most_frequent', 'pca__n_components': 8}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.886363636364\n",
    "\n",
    "\n",
    "Recall Score: 0.2\n",
    "\n",
    "\n",
    "Precision Score: 0.5\n",
    "```\n",
    "The results are strange in that adding more variables decreased the preformance, but I can't be sure that it wasn't because of my odd choice of cutting off the max_depth at 8 instead of 9. \n",
    "\n",
    "Also, I don't know why the max_depth is a useful parameter since the default is None. How could limiting the parameter improve preformance? \n",
    "\n",
    "## New Variables with modified parameter search space\n",
    "\n",
    "Just to satisfy my curiosity I am going to increase the max_depth parameter to 10 and raise the maximum number of principle components to 16 to account for the four newly added variables.\n",
    "\n",
    "#### Civil Libertarian Improvement\n",
    "\n",
    "So this turns out to have been a good idea. I have had a slight loss in recall but big gains in accuracy and precision. Given that I am a civil libertarian I personally feel this is an improvement. \n",
    "\n",
    "```\n",
    "\n",
    "Best parameters from the grid search:{'clf__max_depth': 8, 'imp__strategy': 'most_frequent', 'pca__n_components': 8}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.909090909091\n",
    "\n",
    "\n",
    "Recall Score: 0.4\n",
    "\n",
    "\n",
    "Precision Score: 0.666666666667\n",
    "```\n",
    "Now I have one problem with these results in that the number of principle components chosen was at the bottom of the range I had specified. So now I have to face the possibility that there was better preforming model with a smaller dimensional principle component space that was over looked by the paramters to which I limited the grid search. So, as a double check, I am going to let the space searced go down. I am also going to keep the 'most_frequent' strategy and not search that space anymore. \n",
    "\n",
    "### Fewer dimensions: Insanity\n",
    "\n",
    "Ok, this is the kind of insane behavior that is driving me insane. \n",
    "```\n",
    "\n",
    "Best parameters from the grid search:{'clf__max_depth': 8, 'pca__n_components': 7}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.863636363636\n",
    "\n",
    "\n",
    "Recall Score: 0.2\n",
    "\n",
    "\n",
    "Precision Score: 0.333333333333\n",
    "```\n",
    "Now, all I did was specify 'most_frequent' as the method of imputation and offered it the choice of finding few dimensions in the principle component analysis. Everything else was the same. So how could the model get worse? It had the choice of keeping the model that had preformed better in the last specificaiton of the model, so how could it get worse? \n",
    "## Showdown\n",
    "Ok, I am just going to test everything. I am going to have a model of 200 different possible parameter configurations, but I am going to know once and for all what's what. Since I have never had entropy come up as the better scoring criterion I am going to leave that out but I am going to try everything else I have tried with the broades possible ranges. \n",
    "\n",
    "I am doing this essentially because I have to know once and for all whether more variables improve things or not. \n",
    "```\n",
    "\n",
    "Best parameters from the grid search:{'clf__max_depth': 7, 'imp__strategy': 'most_frequent', 'pca__n_components': 8}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.886363636364\n",
    "\n",
    "\n",
    "Recall Score: 0.2\n",
    "\n",
    "\n",
    "Precision Score: 0.5\n",
    "```\n",
    "So by expanding the grid search the model actually does worse. That is really distressing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Test\n",
    "So now I have been running into cases where I expand the search grid but end up with a less effective model. This is puzzling. It seems that if I have the models parameters set wider but still including the original model it should do at least as well as the original model. So, I am going to test the proposition and try to recreate the problem. \n",
    "\n",
    "First I will run the model that preformed the best above. \n",
    "\n",
    "Then I will run a model with a slightly expanded grid search. \n",
    "\n",
    "Then I will run a model with a much expanded grid search. \n",
    "\n",
    "### Larger Data Set-Original Settings\n",
    "So I tried it with the original grid search settings, with the choice between 'median' and most_frequent', principle components pared down to the range from 2 to 10 and the max depth set to the range from 6 to 9. The results were pretty good. \n",
    "```\n",
    "\n",
    "Best parameters from the grid search:{'clf__max_depth': 8, 'imp__strategy': 'most_frequent', 'pca__n_components': 8}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.909090909091\n",
    "\n",
    "\n",
    "Recall Score: 0.4\n",
    "\n",
    "\n",
    "Precision Score: 0.666666666667\n",
    "```\n",
    "That is reassuring because it is the exact outcome from the model that I labeled the 'civil libertarian improvement' above. \n",
    "\n",
    "What was odd was when I gave it the exact settings that had preformed best in the broadest grid search with the smaller data set: depth 9/components 10, I got the error about something being 0. Then I changed the grid search to have lists for the components and depth parameters of 8,9 and 9,10 respectively so that model would run without objecting that it needed lists in the grid search parameters. The outcome is acceptable but the preformance has gone down since adding new variables. That is really weird. \n",
    "```\n",
    "\n",
    "Best parameters from the grid search:{'clf__max_depth': 8, 'imp__strategy': 'most_frequent', 'pca__n_components': 9}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.886363636364\n",
    "\n",
    "\n",
    "Recall Score: 0.4\n",
    "\n",
    "\n",
    "Precision Score: 0.5\n",
    "```\n",
    "So maybe adding new variables that are simply restatements of the information in the old variables does not really help. Maybe it makes things worse by increasing dimensionality without adding genuinely new information, or at least not new enough information to justify the increase in the dimensionality of the search space. (?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
