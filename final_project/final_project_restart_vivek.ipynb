{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final project restart: Vivek\n",
    "Import the modules, assign variables and define functions.\n",
    "\n",
    "This is the version that I am using since I got some advice from Vivek on the forum. He is really great and added a great customized function for getting the scoring function to work better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "                                                \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn import cross_validation\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from tester import test_classifier\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import\n",
    "Defining functions that I will use for data import and prep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = \"final_project_dataset.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def import_data(data):\n",
    "    '''This are the things I will do to import the data everytime, \n",
    "    regardless of what variables I make.'''\n",
    "    with open(data, \"r\") as data_file:\n",
    "        data_dict = pickle.load(data_file)\n",
    "    df = pd.DataFrame(data_dict)\n",
    "    df = df.transpose()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_df_features_labels_features_list(df):\n",
    "    '''This is where the features and labels are extracted to use as arguments\n",
    "    for sklearn\\'s StratifiedShuffleSplit function AND for the model that I submit\n",
    "    to the grader. That is why it returns four things. It is also where I add some \n",
    "    new variables.'''\n",
    "    \n",
    "    df = df.drop('email_address', axis=1)\n",
    "    \n",
    "    df = df.astype(float)\n",
    "    \n",
    "    #add columns\n",
    "    df['pct_from_poi'] = df['from_poi_to_this_person']/(df['from_messages'] + 1)\n",
    "    df['pct_to_poi'] = df['from_this_person_to_poi']/(df['from_messages'] + 1)\n",
    "    df['to_from'] = df['pct_from_poi']*df['pct_from_poi']\n",
    "    \n",
    "    #drop columns\n",
    "    df = df.drop(\"loan_advances\", axis=1)\n",
    "    df = df.drop('restricted_stock_deferred', axis=1)\n",
    "    df = df.drop('director_fees', axis=1)\n",
    "    df = df.drop('restricted_stock', axis=1)\n",
    "    df = df.drop('deferral_payments', axis=1)\n",
    "    df = df.drop('deferred_income', axis=1)\n",
    "   \n",
    "    # drop rows based on meaning\n",
    "\n",
    "    df = df.drop('TOTAL')\n",
    "    df = df.drop(\"THE TRAVEL AGENCY IN THE PARK\")\n",
    "    df = df.drop(\"LOCKHART EUGENE E\")\n",
    "\n",
    "\n",
    "\n",
    "    #drop rows\n",
    "    for i in df.index:\n",
    "        if df.ix[i].count() < 3:\n",
    "            df = df.drop(i, axis=0)\n",
    "    \n",
    "    features_list = list(df.columns)\n",
    "    features_list.remove('poi')\n",
    "    # get features of udacity_grader\n",
    "    features = df[features_list]\n",
    "    labels = df['poi']\n",
    "    # put poi back in for udacity grader\n",
    "    features_list.insert(0,'poi')\n",
    "\n",
    "    \n",
    "    return df, features, labels, features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_train_test_split(features, labels):\n",
    "    '''This gets the train test split for the sklearn runs of the model'''\n",
    "    from sklearn import cross_validation\n",
    "    features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "    return features_train, features_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Functions\n",
    "The functions I will call to analyze the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code from vivek_29420285151271 to replace f1 as scoring criterion\n",
    "\n",
    "def precision_recall(labels,predictions):\n",
    "    ind_true_pos = [i for i in range(0,len(labels)) if (predictions[i]==1) & (labels[i]==1)]\n",
    "    ind_false_pos = [i for i in range(0,len(labels)) if ((predictions[i]==1) & (labels[i]==0))]\n",
    "    ind_false_neg = [i for i in range(0,len(labels)) if ((predictions[i]==0) & (labels[i]==1))]\n",
    "    ind_true_neg = [i for i in range(0,len(labels)) if ((predictions[i]==0) & (labels[i]==0))]\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    \n",
    "    \n",
    "    ind_labels = [i for i in range(0,len(labels)) if labels[i]==1]\n",
    "    \n",
    "    if len(ind_labels) !=0:\n",
    "        if float( len(ind_true_pos) + len(ind_false_pos))!=0:\n",
    "            precision = float(len(ind_true_pos))/float( len(ind_true_pos) + len(ind_false_pos))\n",
    "        if float( len(ind_true_pos) + len(ind_false_neg))!=0:\n",
    "            recall = float(len(ind_true_pos))/float( len(ind_true_pos) + len(ind_false_neg))\n",
    "        return precision, recall\n",
    "    else:\n",
    "        return -1,-1\n",
    "\n",
    "def custom_scorer(labels, predictions):\n",
    "    precision,recall = precision_recall(labels,predictions)\n",
    "    min_score = min(precision, recall)\n",
    "    return min_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Analysis, feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because I cut some data set based on the EDA some of the things I did in EDA don't work anymore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of features about each person\n",
    "len(df.ix[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of poi's \n",
    "len(df.ix[df['poi']==1])\n",
    "# why doesn't the second dfE need an .ix? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bonus                      58\n",
       "exercised_stock_options    38\n",
       "expenses                   45\n",
       "from_messages              53\n",
       "from_poi_to_this_person    53\n",
       "from_this_person_to_poi    53\n",
       "long_term_incentive        74\n",
       "other                      48\n",
       "poi                         0\n",
       "salary                     45\n",
       "shared_receipt_with_poi    53\n",
       "to_messages                53\n",
       "total_payments             18\n",
       "total_stock_value          15\n",
       "pct_from_poi               53\n",
       "pct_to_poi                 53\n",
       "to_from                    53\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#total value of stock belonging to James Prentice\n",
    "#dfE.ix[:,0]\n",
    "# how many people have no salary (count ignores null values)\n",
    "# dfE['salary'].count()\n",
    "# percentage of people with NaN for total payments\n",
    "# ntNaN = dfE['total_payments'].count()\n",
    "# n = float(len(dfE))\n",
    "# NaN = (n-ntNaN)\n",
    "# (NaN/n)*100.0\n",
    "# percentage of poi's with NaN for total_payments\n",
    "#dfE.groupby('poi')['total_payments'].count()\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is just so little data that I am hesitant to cut out anything. Still, when over 100 cases are missing for a particular variable it seems like it is almost surely doing more harm than good, so I am going to take out the variables with more than 90 cases missing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in df.columns: \n",
    "    if df[i].isnull().sum() > 90:\n",
    "        print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bonus                      58\n",
       "exercised_stock_options    38\n",
       "expenses                   45\n",
       "from_messages              53\n",
       "from_poi_to_this_person    53\n",
       "from_this_person_to_poi    53\n",
       "long_term_incentive        74\n",
       "other                      48\n",
       "poi                         0\n",
       "salary                     45\n",
       "shared_receipt_with_poi    53\n",
       "to_messages                53\n",
       "total_payments             18\n",
       "total_stock_value          15\n",
       "pct_from_poi               53\n",
       "pct_to_poi                 53\n",
       "to_from                    53\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Big correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bonus</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>expenses</th>\n",
       "      <th>from_messages</th>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>other</th>\n",
       "      <th>poi</th>\n",
       "      <th>salary</th>\n",
       "      <th>shared_receipt_with_poi</th>\n",
       "      <th>to_messages</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>total_stock_value</th>\n",
       "      <th>pct_from_poi</th>\n",
       "      <th>pct_to_poi</th>\n",
       "      <th>to_from</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bonus</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.506998</td>\n",
       "      <td>0.033799</td>\n",
       "      <td>0.052725</td>\n",
       "      <td>0.562187</td>\n",
       "      <td>0.354634</td>\n",
       "      <td>0.625259</td>\n",
       "      <td>0.376371</td>\n",
       "      <td>0.302384</td>\n",
       "      <td>0.523190</td>\n",
       "      <td>0.549102</td>\n",
       "      <td>0.372997</td>\n",
       "      <td>0.569054</td>\n",
       "      <td>0.509441</td>\n",
       "      <td>0.017227</td>\n",
       "      <td>0.077179</td>\n",
       "      <td>0.004426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <td>0.506998</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.019412</td>\n",
       "      <td>-0.066129</td>\n",
       "      <td>0.160253</td>\n",
       "      <td>-0.030101</td>\n",
       "      <td>0.500016</td>\n",
       "      <td>0.536248</td>\n",
       "      <td>0.503551</td>\n",
       "      <td>0.607324</td>\n",
       "      <td>0.154333</td>\n",
       "      <td>0.079568</td>\n",
       "      <td>0.591690</td>\n",
       "      <td>0.963560</td>\n",
       "      <td>0.229132</td>\n",
       "      <td>0.138142</td>\n",
       "      <td>0.155346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>expenses</th>\n",
       "      <td>0.033799</td>\n",
       "      <td>0.019412</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.094144</td>\n",
       "      <td>-0.059244</td>\n",
       "      <td>0.014964</td>\n",
       "      <td>-0.100766</td>\n",
       "      <td>0.062874</td>\n",
       "      <td>0.060292</td>\n",
       "      <td>0.145364</td>\n",
       "      <td>0.223495</td>\n",
       "      <td>0.155070</td>\n",
       "      <td>0.109798</td>\n",
       "      <td>0.034707</td>\n",
       "      <td>0.071078</td>\n",
       "      <td>-0.132656</td>\n",
       "      <td>0.056303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from_messages</th>\n",
       "      <td>0.052725</td>\n",
       "      <td>-0.066129</td>\n",
       "      <td>0.094144</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.186708</td>\n",
       "      <td>0.588687</td>\n",
       "      <td>-0.071958</td>\n",
       "      <td>-0.101686</td>\n",
       "      <td>-0.074308</td>\n",
       "      <td>-0.003541</td>\n",
       "      <td>0.230855</td>\n",
       "      <td>0.475450</td>\n",
       "      <td>-0.033089</td>\n",
       "      <td>-0.036310</td>\n",
       "      <td>-0.166251</td>\n",
       "      <td>-0.183084</td>\n",
       "      <td>-0.090712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "      <td>0.562187</td>\n",
       "      <td>0.160253</td>\n",
       "      <td>-0.059244</td>\n",
       "      <td>0.186708</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.445063</td>\n",
       "      <td>0.212538</td>\n",
       "      <td>0.111249</td>\n",
       "      <td>0.167722</td>\n",
       "      <td>0.179055</td>\n",
       "      <td>0.659264</td>\n",
       "      <td>0.525667</td>\n",
       "      <td>0.154431</td>\n",
       "      <td>0.146366</td>\n",
       "      <td>0.392999</td>\n",
       "      <td>0.069042</td>\n",
       "      <td>0.364398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <td>0.354634</td>\n",
       "      <td>-0.030101</td>\n",
       "      <td>0.014964</td>\n",
       "      <td>0.588687</td>\n",
       "      <td>0.445063</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.083591</td>\n",
       "      <td>-0.110335</td>\n",
       "      <td>0.112940</td>\n",
       "      <td>0.021288</td>\n",
       "      <td>0.475748</td>\n",
       "      <td>0.568506</td>\n",
       "      <td>0.011556</td>\n",
       "      <td>0.001289</td>\n",
       "      <td>-0.162862</td>\n",
       "      <td>-0.040409</td>\n",
       "      <td>-0.094549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>long_term_incentive</th>\n",
       "      <td>0.625259</td>\n",
       "      <td>0.500016</td>\n",
       "      <td>-0.100766</td>\n",
       "      <td>-0.071958</td>\n",
       "      <td>0.212538</td>\n",
       "      <td>0.083591</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.529398</td>\n",
       "      <td>0.254723</td>\n",
       "      <td>0.484114</td>\n",
       "      <td>0.178944</td>\n",
       "      <td>0.134277</td>\n",
       "      <td>0.518498</td>\n",
       "      <td>0.495485</td>\n",
       "      <td>0.168470</td>\n",
       "      <td>0.030600</td>\n",
       "      <td>0.158882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other</th>\n",
       "      <td>0.376371</td>\n",
       "      <td>0.536248</td>\n",
       "      <td>0.062874</td>\n",
       "      <td>-0.101686</td>\n",
       "      <td>0.111249</td>\n",
       "      <td>-0.110335</td>\n",
       "      <td>0.529398</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.120270</td>\n",
       "      <td>0.606903</td>\n",
       "      <td>0.122591</td>\n",
       "      <td>0.040580</td>\n",
       "      <td>0.825589</td>\n",
       "      <td>0.627171</td>\n",
       "      <td>0.385961</td>\n",
       "      <td>0.143893</td>\n",
       "      <td>0.420111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poi</th>\n",
       "      <td>0.302384</td>\n",
       "      <td>0.503551</td>\n",
       "      <td>0.060292</td>\n",
       "      <td>-0.074308</td>\n",
       "      <td>0.167722</td>\n",
       "      <td>0.112940</td>\n",
       "      <td>0.254723</td>\n",
       "      <td>0.120270</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.264976</td>\n",
       "      <td>0.228313</td>\n",
       "      <td>0.058954</td>\n",
       "      <td>0.228705</td>\n",
       "      <td>0.365450</td>\n",
       "      <td>0.180266</td>\n",
       "      <td>0.347354</td>\n",
       "      <td>0.043723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salary</th>\n",
       "      <td>0.523190</td>\n",
       "      <td>0.607324</td>\n",
       "      <td>0.145364</td>\n",
       "      <td>-0.003541</td>\n",
       "      <td>0.179055</td>\n",
       "      <td>0.021288</td>\n",
       "      <td>0.484114</td>\n",
       "      <td>0.606903</td>\n",
       "      <td>0.264976</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.284995</td>\n",
       "      <td>0.187047</td>\n",
       "      <td>0.579260</td>\n",
       "      <td>0.614736</td>\n",
       "      <td>0.284375</td>\n",
       "      <td>0.008912</td>\n",
       "      <td>0.345766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shared_receipt_with_poi</th>\n",
       "      <td>0.549102</td>\n",
       "      <td>0.154333</td>\n",
       "      <td>0.223495</td>\n",
       "      <td>0.230855</td>\n",
       "      <td>0.659264</td>\n",
       "      <td>0.475748</td>\n",
       "      <td>0.178944</td>\n",
       "      <td>0.122591</td>\n",
       "      <td>0.228313</td>\n",
       "      <td>0.284995</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.847990</td>\n",
       "      <td>0.191069</td>\n",
       "      <td>0.176314</td>\n",
       "      <td>0.089933</td>\n",
       "      <td>0.046901</td>\n",
       "      <td>0.104841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to_messages</th>\n",
       "      <td>0.372997</td>\n",
       "      <td>0.079568</td>\n",
       "      <td>0.155070</td>\n",
       "      <td>0.475450</td>\n",
       "      <td>0.525667</td>\n",
       "      <td>0.568506</td>\n",
       "      <td>0.134277</td>\n",
       "      <td>0.040580</td>\n",
       "      <td>0.058954</td>\n",
       "      <td>0.187047</td>\n",
       "      <td>0.847990</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.133834</td>\n",
       "      <td>0.120864</td>\n",
       "      <td>-0.069479</td>\n",
       "      <td>-0.119978</td>\n",
       "      <td>-0.014733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_payments</th>\n",
       "      <td>0.569054</td>\n",
       "      <td>0.591690</td>\n",
       "      <td>0.109798</td>\n",
       "      <td>-0.033089</td>\n",
       "      <td>0.154431</td>\n",
       "      <td>0.011556</td>\n",
       "      <td>0.518498</td>\n",
       "      <td>0.825589</td>\n",
       "      <td>0.228705</td>\n",
       "      <td>0.579260</td>\n",
       "      <td>0.191069</td>\n",
       "      <td>0.133834</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.668166</td>\n",
       "      <td>0.186380</td>\n",
       "      <td>0.132469</td>\n",
       "      <td>0.133989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_stock_value</th>\n",
       "      <td>0.509441</td>\n",
       "      <td>0.963560</td>\n",
       "      <td>0.034707</td>\n",
       "      <td>-0.036310</td>\n",
       "      <td>0.146366</td>\n",
       "      <td>0.001289</td>\n",
       "      <td>0.495485</td>\n",
       "      <td>0.627171</td>\n",
       "      <td>0.365450</td>\n",
       "      <td>0.614736</td>\n",
       "      <td>0.176314</td>\n",
       "      <td>0.120864</td>\n",
       "      <td>0.668166</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.199847</td>\n",
       "      <td>0.145413</td>\n",
       "      <td>0.153154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pct_from_poi</th>\n",
       "      <td>0.017227</td>\n",
       "      <td>0.229132</td>\n",
       "      <td>0.071078</td>\n",
       "      <td>-0.166251</td>\n",
       "      <td>0.392999</td>\n",
       "      <td>-0.162862</td>\n",
       "      <td>0.168470</td>\n",
       "      <td>0.385961</td>\n",
       "      <td>0.180266</td>\n",
       "      <td>0.284375</td>\n",
       "      <td>0.089933</td>\n",
       "      <td>-0.069479</td>\n",
       "      <td>0.186380</td>\n",
       "      <td>0.199847</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.282706</td>\n",
       "      <td>0.934291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pct_to_poi</th>\n",
       "      <td>0.077179</td>\n",
       "      <td>0.138142</td>\n",
       "      <td>-0.132656</td>\n",
       "      <td>-0.183084</td>\n",
       "      <td>0.069042</td>\n",
       "      <td>-0.040409</td>\n",
       "      <td>0.030600</td>\n",
       "      <td>0.143893</td>\n",
       "      <td>0.347354</td>\n",
       "      <td>0.008912</td>\n",
       "      <td>0.046901</td>\n",
       "      <td>-0.119978</td>\n",
       "      <td>0.132469</td>\n",
       "      <td>0.145413</td>\n",
       "      <td>0.282706</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.181724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to_from</th>\n",
       "      <td>0.004426</td>\n",
       "      <td>0.155346</td>\n",
       "      <td>0.056303</td>\n",
       "      <td>-0.090712</td>\n",
       "      <td>0.364398</td>\n",
       "      <td>-0.094549</td>\n",
       "      <td>0.158882</td>\n",
       "      <td>0.420111</td>\n",
       "      <td>0.043723</td>\n",
       "      <td>0.345766</td>\n",
       "      <td>0.104841</td>\n",
       "      <td>-0.014733</td>\n",
       "      <td>0.133989</td>\n",
       "      <td>0.153154</td>\n",
       "      <td>0.934291</td>\n",
       "      <td>0.181724</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            bonus  exercised_stock_options  expenses  \\\n",
       "bonus                    1.000000                 0.506998  0.033799   \n",
       "exercised_stock_options  0.506998                 1.000000  0.019412   \n",
       "expenses                 0.033799                 0.019412  1.000000   \n",
       "from_messages            0.052725                -0.066129  0.094144   \n",
       "from_poi_to_this_person  0.562187                 0.160253 -0.059244   \n",
       "from_this_person_to_poi  0.354634                -0.030101  0.014964   \n",
       "long_term_incentive      0.625259                 0.500016 -0.100766   \n",
       "other                    0.376371                 0.536248  0.062874   \n",
       "poi                      0.302384                 0.503551  0.060292   \n",
       "salary                   0.523190                 0.607324  0.145364   \n",
       "shared_receipt_with_poi  0.549102                 0.154333  0.223495   \n",
       "to_messages              0.372997                 0.079568  0.155070   \n",
       "total_payments           0.569054                 0.591690  0.109798   \n",
       "total_stock_value        0.509441                 0.963560  0.034707   \n",
       "pct_from_poi             0.017227                 0.229132  0.071078   \n",
       "pct_to_poi               0.077179                 0.138142 -0.132656   \n",
       "to_from                  0.004426                 0.155346  0.056303   \n",
       "\n",
       "                         from_messages  from_poi_to_this_person  \\\n",
       "bonus                         0.052725                 0.562187   \n",
       "exercised_stock_options      -0.066129                 0.160253   \n",
       "expenses                      0.094144                -0.059244   \n",
       "from_messages                 1.000000                 0.186708   \n",
       "from_poi_to_this_person       0.186708                 1.000000   \n",
       "from_this_person_to_poi       0.588687                 0.445063   \n",
       "long_term_incentive          -0.071958                 0.212538   \n",
       "other                        -0.101686                 0.111249   \n",
       "poi                          -0.074308                 0.167722   \n",
       "salary                       -0.003541                 0.179055   \n",
       "shared_receipt_with_poi       0.230855                 0.659264   \n",
       "to_messages                   0.475450                 0.525667   \n",
       "total_payments               -0.033089                 0.154431   \n",
       "total_stock_value            -0.036310                 0.146366   \n",
       "pct_from_poi                 -0.166251                 0.392999   \n",
       "pct_to_poi                   -0.183084                 0.069042   \n",
       "to_from                      -0.090712                 0.364398   \n",
       "\n",
       "                         from_this_person_to_poi  long_term_incentive  \\\n",
       "bonus                                   0.354634             0.625259   \n",
       "exercised_stock_options                -0.030101             0.500016   \n",
       "expenses                                0.014964            -0.100766   \n",
       "from_messages                           0.588687            -0.071958   \n",
       "from_poi_to_this_person                 0.445063             0.212538   \n",
       "from_this_person_to_poi                 1.000000             0.083591   \n",
       "long_term_incentive                     0.083591             1.000000   \n",
       "other                                  -0.110335             0.529398   \n",
       "poi                                     0.112940             0.254723   \n",
       "salary                                  0.021288             0.484114   \n",
       "shared_receipt_with_poi                 0.475748             0.178944   \n",
       "to_messages                             0.568506             0.134277   \n",
       "total_payments                          0.011556             0.518498   \n",
       "total_stock_value                       0.001289             0.495485   \n",
       "pct_from_poi                           -0.162862             0.168470   \n",
       "pct_to_poi                             -0.040409             0.030600   \n",
       "to_from                                -0.094549             0.158882   \n",
       "\n",
       "                            other       poi    salary  \\\n",
       "bonus                    0.376371  0.302384  0.523190   \n",
       "exercised_stock_options  0.536248  0.503551  0.607324   \n",
       "expenses                 0.062874  0.060292  0.145364   \n",
       "from_messages           -0.101686 -0.074308 -0.003541   \n",
       "from_poi_to_this_person  0.111249  0.167722  0.179055   \n",
       "from_this_person_to_poi -0.110335  0.112940  0.021288   \n",
       "long_term_incentive      0.529398  0.254723  0.484114   \n",
       "other                    1.000000  0.120270  0.606903   \n",
       "poi                      0.120270  1.000000  0.264976   \n",
       "salary                   0.606903  0.264976  1.000000   \n",
       "shared_receipt_with_poi  0.122591  0.228313  0.284995   \n",
       "to_messages              0.040580  0.058954  0.187047   \n",
       "total_payments           0.825589  0.228705  0.579260   \n",
       "total_stock_value        0.627171  0.365450  0.614736   \n",
       "pct_from_poi             0.385961  0.180266  0.284375   \n",
       "pct_to_poi               0.143893  0.347354  0.008912   \n",
       "to_from                  0.420111  0.043723  0.345766   \n",
       "\n",
       "                         shared_receipt_with_poi  to_messages  total_payments  \\\n",
       "bonus                                   0.549102     0.372997        0.569054   \n",
       "exercised_stock_options                 0.154333     0.079568        0.591690   \n",
       "expenses                                0.223495     0.155070        0.109798   \n",
       "from_messages                           0.230855     0.475450       -0.033089   \n",
       "from_poi_to_this_person                 0.659264     0.525667        0.154431   \n",
       "from_this_person_to_poi                 0.475748     0.568506        0.011556   \n",
       "long_term_incentive                     0.178944     0.134277        0.518498   \n",
       "other                                   0.122591     0.040580        0.825589   \n",
       "poi                                     0.228313     0.058954        0.228705   \n",
       "salary                                  0.284995     0.187047        0.579260   \n",
       "shared_receipt_with_poi                 1.000000     0.847990        0.191069   \n",
       "to_messages                             0.847990     1.000000        0.133834   \n",
       "total_payments                          0.191069     0.133834        1.000000   \n",
       "total_stock_value                       0.176314     0.120864        0.668166   \n",
       "pct_from_poi                            0.089933    -0.069479        0.186380   \n",
       "pct_to_poi                              0.046901    -0.119978        0.132469   \n",
       "to_from                                 0.104841    -0.014733        0.133989   \n",
       "\n",
       "                         total_stock_value  pct_from_poi  pct_to_poi   to_from  \n",
       "bonus                             0.509441      0.017227    0.077179  0.004426  \n",
       "exercised_stock_options           0.963560      0.229132    0.138142  0.155346  \n",
       "expenses                          0.034707      0.071078   -0.132656  0.056303  \n",
       "from_messages                    -0.036310     -0.166251   -0.183084 -0.090712  \n",
       "from_poi_to_this_person           0.146366      0.392999    0.069042  0.364398  \n",
       "from_this_person_to_poi           0.001289     -0.162862   -0.040409 -0.094549  \n",
       "long_term_incentive               0.495485      0.168470    0.030600  0.158882  \n",
       "other                             0.627171      0.385961    0.143893  0.420111  \n",
       "poi                               0.365450      0.180266    0.347354  0.043723  \n",
       "salary                            0.614736      0.284375    0.008912  0.345766  \n",
       "shared_receipt_with_poi           0.176314      0.089933    0.046901  0.104841  \n",
       "to_messages                       0.120864     -0.069479   -0.119978 -0.014733  \n",
       "total_payments                    0.668166      0.186380    0.132469  0.133989  \n",
       "total_stock_value                 1.000000      0.199847    0.145413  0.153154  \n",
       "pct_from_poi                      0.199847      1.000000    0.282706  0.934291  \n",
       "pct_to_poi                        0.145413      0.282706    1.000000  0.181724  \n",
       "to_from                           0.153154      0.934291    0.181724  1.000000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am really hesitant to throw out any variables with such a small data set, but the variables for which we can't even compute a correlation with the dependent variable seem like they can't be helping anything and must be increasing the dimensionality of the data set. So I will drop restricted_stock_deferred and director_fees. Also, restricted_stock does not seem to be helping much, either, since it has a 0.000107 correlation with the dependent variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get a list of how many missing variables each column has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bonus                       81\n",
       "exercised_stock_options    101\n",
       "expenses                    94\n",
       "from_messages               86\n",
       "from_poi_to_this_person     86\n",
       "from_this_person_to_poi     86\n",
       "long_term_incentive         65\n",
       "other                       91\n",
       "poi                        139\n",
       "salary                      94\n",
       "shared_receipt_with_poi     86\n",
       "to_messages                 86\n",
       "total_payments             121\n",
       "total_stock_value          124\n",
       "pct_from_poi                86\n",
       "pct_to_poi                  86\n",
       "to_from                     86\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bonus</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>expenses</th>\n",
       "      <th>from_messages</th>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>other</th>\n",
       "      <th>poi</th>\n",
       "      <th>salary</th>\n",
       "      <th>shared_receipt_with_poi</th>\n",
       "      <th>to_messages</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>total_stock_value</th>\n",
       "      <th>pct_from_poi</th>\n",
       "      <th>pct_to_poi</th>\n",
       "      <th>to_from</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>81.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>1.210000e+02</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1201773.074074</td>\n",
       "      <td>2959559.257426</td>\n",
       "      <td>54192.010638</td>\n",
       "      <td>608.790698</td>\n",
       "      <td>64.895349</td>\n",
       "      <td>41.232558</td>\n",
       "      <td>746491.200000</td>\n",
       "      <td>466410.516484</td>\n",
       "      <td>0.129496</td>\n",
       "      <td>284087.542553</td>\n",
       "      <td>1176.465116</td>\n",
       "      <td>2073.860465</td>\n",
       "      <td>2.684455e+06</td>\n",
       "      <td>3377577.500000</td>\n",
       "      <td>1.062867</td>\n",
       "      <td>0.178236</td>\n",
       "      <td>4.803411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1441679.438330</td>\n",
       "      <td>5499449.598994</td>\n",
       "      <td>46108.377454</td>\n",
       "      <td>1841.033949</td>\n",
       "      <td>86.979244</td>\n",
       "      <td>100.073111</td>\n",
       "      <td>862917.421568</td>\n",
       "      <td>1397375.607531</td>\n",
       "      <td>0.336963</td>\n",
       "      <td>177131.115377</td>\n",
       "      <td>1178.317641</td>\n",
       "      <td>2582.700981</td>\n",
       "      <td>9.597861e+06</td>\n",
       "      <td>6553134.432835</td>\n",
       "      <td>1.927938</td>\n",
       "      <td>0.202476</td>\n",
       "      <td>16.932893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>70000.000000</td>\n",
       "      <td>3285.000000</td>\n",
       "      <td>148.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>69223.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>477.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>1.480000e+02</td>\n",
       "      <td>-44093.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>425000.000000</td>\n",
       "      <td>506765.000000</td>\n",
       "      <td>22479.000000</td>\n",
       "      <td>22.750000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>275000.000000</td>\n",
       "      <td>1203.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>211802.000000</td>\n",
       "      <td>249.750000</td>\n",
       "      <td>541.250000</td>\n",
       "      <td>4.775570e+05</td>\n",
       "      <td>495258.750000</td>\n",
       "      <td>0.045432</td>\n",
       "      <td>0.012417</td>\n",
       "      <td>0.002124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>750000.000000</td>\n",
       "      <td>1297049.000000</td>\n",
       "      <td>46547.500000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>422158.000000</td>\n",
       "      <td>51587.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>258741.000000</td>\n",
       "      <td>740.500000</td>\n",
       "      <td>1211.000000</td>\n",
       "      <td>1.112087e+06</td>\n",
       "      <td>1102872.500000</td>\n",
       "      <td>0.373864</td>\n",
       "      <td>0.100531</td>\n",
       "      <td>0.139787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1200000.000000</td>\n",
       "      <td>2542813.000000</td>\n",
       "      <td>78408.500000</td>\n",
       "      <td>145.500000</td>\n",
       "      <td>72.250000</td>\n",
       "      <td>24.750000</td>\n",
       "      <td>831809.000000</td>\n",
       "      <td>331983.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>308606.500000</td>\n",
       "      <td>1888.250000</td>\n",
       "      <td>2634.750000</td>\n",
       "      <td>2.093263e+06</td>\n",
       "      <td>2721124.250000</td>\n",
       "      <td>1.131719</td>\n",
       "      <td>0.263720</td>\n",
       "      <td>1.283187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8000000.000000</td>\n",
       "      <td>34348384.000000</td>\n",
       "      <td>228763.000000</td>\n",
       "      <td>14368.000000</td>\n",
       "      <td>528.000000</td>\n",
       "      <td>609.000000</td>\n",
       "      <td>5145434.000000</td>\n",
       "      <td>10359729.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1111258.000000</td>\n",
       "      <td>5521.000000</td>\n",
       "      <td>15149.000000</td>\n",
       "      <td>1.035598e+08</td>\n",
       "      <td>49110078.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>121.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                bonus  exercised_stock_options       expenses  from_messages  \\\n",
       "count       81.000000               101.000000      94.000000      86.000000   \n",
       "mean   1201773.074074           2959559.257426   54192.010638     608.790698   \n",
       "std    1441679.438330           5499449.598994   46108.377454    1841.033949   \n",
       "min      70000.000000              3285.000000     148.000000      12.000000   \n",
       "25%     425000.000000            506765.000000   22479.000000      22.750000   \n",
       "50%     750000.000000           1297049.000000   46547.500000      41.000000   \n",
       "75%    1200000.000000           2542813.000000   78408.500000     145.500000   \n",
       "max    8000000.000000          34348384.000000  228763.000000   14368.000000   \n",
       "\n",
       "       from_poi_to_this_person  from_this_person_to_poi  long_term_incentive  \\\n",
       "count                86.000000                86.000000            65.000000   \n",
       "mean                 64.895349                41.232558        746491.200000   \n",
       "std                  86.979244               100.073111        862917.421568   \n",
       "min                   0.000000                 0.000000         69223.000000   \n",
       "25%                  10.000000                 1.000000        275000.000000   \n",
       "50%                  35.000000                 8.000000        422158.000000   \n",
       "75%                  72.250000                24.750000        831809.000000   \n",
       "max                 528.000000               609.000000       5145434.000000   \n",
       "\n",
       "                 other         poi          salary  shared_receipt_with_poi  \\\n",
       "count        91.000000  139.000000       94.000000                86.000000   \n",
       "mean     466410.516484    0.129496   284087.542553              1176.465116   \n",
       "std     1397375.607531    0.336963   177131.115377              1178.317641   \n",
       "min           2.000000    0.000000      477.000000                 2.000000   \n",
       "25%        1203.000000    0.000000   211802.000000               249.750000   \n",
       "50%       51587.000000    0.000000   258741.000000               740.500000   \n",
       "75%      331983.000000    0.000000   308606.500000              1888.250000   \n",
       "max    10359729.000000    1.000000  1111258.000000              5521.000000   \n",
       "\n",
       "        to_messages  total_payments  total_stock_value  pct_from_poi  \\\n",
       "count     86.000000    1.210000e+02         124.000000     86.000000   \n",
       "mean    2073.860465    2.684455e+06     3377577.500000      1.062867   \n",
       "std     2582.700981    9.597861e+06     6553134.432835      1.927938   \n",
       "min       57.000000    1.480000e+02      -44093.000000      0.000000   \n",
       "25%      541.250000    4.775570e+05      495258.750000      0.045432   \n",
       "50%     1211.000000    1.112087e+06     1102872.500000      0.373864   \n",
       "75%     2634.750000    2.093263e+06     2721124.250000      1.131719   \n",
       "max    15149.000000    1.035598e+08    49110078.000000     11.000000   \n",
       "\n",
       "       pct_to_poi     to_from  \n",
       "count   86.000000   86.000000  \n",
       "mean     0.178236    4.803411  \n",
       "std      0.202476   16.932893  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.012417    0.002124  \n",
       "50%      0.100531    0.139787  \n",
       "75%      0.263720    1.283187  \n",
       "max      0.944444  121.000000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that some variables occur only once or a few times. This makes them rather unreliable. They may have a correlation with the dependent variable but it would only be an accident. Or they might correlate in this data set but not in other data. So, especially with the models that are subject to the cures of dimensionality like SVC. So, I will also drop loan_advances (4 instances), total_payments and total_stock_value (both with only 1 instance). UPDATE: I misread the counts for the last two variables. It was a + e02. They stay in.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get a list of how many missing variables there are in each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in df.index:\n",
    "    if df.ix[i].count() < 4:\n",
    "        df = df.drop(i, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_salary = df[df.loc[:,'salary']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelreinhard/anaconda/lib/python2.7/site-packages/pandas/core/frame.py:1997: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \"DataFrame index.\", UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_salary[df.loc[:,'poi']==1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so by getting rid of the people with non-salary information we only loose 2 poi's. Worth it? Since there are only 2 removing them only increases or decreases precision. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Column not found: restricted_stock_deferred'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-881cad3d66cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'poi'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'restricted_stock_deferred'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'barh'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/michaelreinhard/anaconda/lib/python2.7/site-packages/pandas/core/groupby.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3489\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3490\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Column not found: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3491\u001b[0m             \u001b[0;31m# kind of a kludge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3492\u001b[0m             return SeriesGroupBy(self.obj[key], selection=key,\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Column not found: restricted_stock_deferred'"
     ]
    }
   ],
   "source": [
    "df.groupby('poi')['restricted_stock_deferred'].agg(np.median).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Column not found: director_fees'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-fbc84e218780>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'poi'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'director_fees'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'barh'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/michaelreinhard/anaconda/lib/python2.7/site-packages/pandas/core/groupby.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3489\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3490\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Column not found: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3491\u001b[0m             \u001b[0;31m# kind of a kludge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3492\u001b[0m             return SeriesGroupBy(self.obj[key], selection=key,\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Column not found: director_fees'"
     ]
    }
   ],
   "source": [
    "df.groupby('poi')['director_fees'].agg(np.median).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So one thing I am going to do with this information is to remove the variables that are not associated in any way with the poi's. That might at least deal with the 1.0's problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many people are in the data set? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.ix[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so I dropped the people in the data set for whom I had three or fewer pieces of information and it decreased the performance of every model. So, they are staying in. The people dropped by the standard of having three or fewer pieces of information were not poi's so I didn't think it would make that much difference, but it did. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tested with Stratified Shuffle Split\n",
    "I have done this two ways: \n",
    "1) using the whole data set (all features and labels) and letting StratifiedShuffleSplit create the training/test split. The problem with this is that I get a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__n_estimators': 100, 'imp__strategy': 'median', 'pca__n_components': 3, 'clf__max_depth': 6, 'clf__learning_rate': 0.1, 'clf__loss': 'exponential', 'selection__k': 16}\n",
      "make_scorer(custom_scorer)\n"
     ]
    }
   ],
   "source": [
    "# GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# Get custom scorer\n",
    "score = make_scorer(custom_scorer, greater_is_better=True)\n",
    "\n",
    "# get the df\n",
    "df = import_data(data)\n",
    "\n",
    "#df.fillna(inplace=True, value=0)\n",
    "\n",
    "# Get data, here with the features unrealated to poi dropped AND Tanya's features added.\n",
    "df, features, labels, features_list = get_df_features_labels_features_list(df)\n",
    "\n",
    "# Get the test-train split\n",
    "features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(features, labels)\n",
    "\n",
    "# Build pipeline\n",
    "Pipeline = Pipeline([\n",
    "        ('imp', Imputer(missing_values='NaN')),\n",
    "        ('std', MinMaxScaler()),\n",
    "        ('selection', SelectKBest()),\n",
    "        ('pca', PCA()),\n",
    "        ('clf', GradientBoostingClassifier(random_state=0))\n",
    "    ])\n",
    "\n",
    "# Build Grid\n",
    "# pre-processing\n",
    "k = [k for k in range(16,17)]\n",
    "c = [x for x in range(3,4)]\n",
    "\n",
    "# estimator parameters\n",
    "e = [100]\n",
    "r = [0.1]\n",
    "d = [d for d in range(6, 7)]\n",
    "l = [\"exponential\"]\n",
    "\n",
    "param_grid = {'selection__k': k,\n",
    "              'pca__n_components': c,\n",
    "              'imp__strategy': ['median'],\n",
    "              'clf__n_estimators': e,\n",
    "              'clf__learning_rate': r,\n",
    "              'clf__max_depth': d,\n",
    "              'clf__loss': l\n",
    "             }\n",
    "\n",
    "# set model parameters to grid search object\n",
    "gridCV_object = GridSearchCV(estimator = Pipeline, \n",
    "                             param_grid = param_grid,\n",
    "                             scoring = score,\n",
    "                             cv = StratifiedShuffleSplit(labels_train, test_size=0.1,  n_iter=10))\n",
    "\n",
    "# train the model\n",
    "gridCV_object.fit(features_train, labels_train)\n",
    "\n",
    "print gridCV_object.best_params_\n",
    "print gridCV_object.scorer_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_CV = gridCV_object.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.84      0.90      0.87        29\n",
      "        1.0       0.25      0.17      0.20         6\n",
      "\n",
      "avg / total       0.74      0.77      0.75        35\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print classification_report(labels_test, pred_CV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Doing the tests with a data set split into training and testing splits we get scores on recall and precision that are way too high. When I run it with then entire data set using StratiffiedShuffleSplit() and then test it on the entir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Model submitted to the grader:\n",
    "Note that I am using the same function to get the data for both the model submitted to the grader--df and features_list--and the data for the model that I have been testing with StrattifiedShuffleSplit()--features and labels.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import data\n",
    "df = import_data(data)\n",
    "df, features, labels, features_list = get_df_features_labels_features_list(df)\n",
    "\n",
    "#change back to dictionary\n",
    "df1 = df.transpose()\n",
    "df1 = df1.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "And these are the results going through the test classifier:\n",
      "\n",
      "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=16, score_func=<function f_classif at 0x10e2f60c8>)), ('pca', PCA(copy=True, n_components=3, whiten=False)), ('clf', ...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False))])\n",
      "\tAccuracy: 0.83593\tPrecision: 0.41643\tRecall: 0.37000\tF1: 0.39185\tF2: 0.37844\n",
      "\tTotal predictions: 14000\tTrue positives:  740\tFalse positives: 1037\tFalse negatives: 1260\tTrue negatives: 10963\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tester import test_classifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# clf = RandomForestClassifier(max_depth=7 , max_features=10 , n_estimators=1000)\n",
    "\n",
    "\n",
    "#{'imp__strategy': 'median', 'pca__n_components': 9, 'clf__max_depth': 5, 'clf__learning_rate': 0.4, 'clf__loss': 'exponential', 'selection__k': 16}\n",
    "\n",
    "clf = Pipeline([\n",
    "        ('imp', Imputer(missing_values='NaN', strategy='median')),\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('selection', SelectKBest(k=16)),\n",
    "        ('pca', PCA(n_components=3)),\n",
    "        ('clf', GradientBoostingClassifier(n_estimators=100,loss=\"exponential\", max_depth=6, learning_rate=0.1))\n",
    "    ])\n",
    "\n",
    "print \"\\n\\nAnd these are the results going through the test classifier:\\n\"\n",
    "test_classifier(clf, df1, features_list, folds = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grader Results\n",
    "\n",
    "### SVC\n",
    "Here are some results from the grader: \n",
    "```\n",
    "\n",
    "And these are the results going through the test classifier:\n",
    "\n",
    "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='most_frequent',\n",
    "    verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('pca', PCA(copy=True, n_components=9, whiten=False)), ('clf', SVC(C=10000, cache_size=200, class_weight=None, coef0=0.0,\n",
    "  decision_function_shape=None, degree=3, gamma=0.1, kernel='rbf',\n",
    "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "  tol=0.001, verbose=False))])\n",
    "\tAccuracy: 0.81729\tPrecision: 0.31645\tRecall: 0.24050\tF1: 0.27330\tF2: 0.25263\n",
    "\tTotal predictions: 14000\tTrue positives:  481\tFalse positives: 1039\tFalse negatives: 1519\tTrue negatives: 10961\n",
    "```\n",
    "The thing is that the precision was there but the recall was not. Those parameters came from running the SVC with f1 scoring with a training-test split. It worked well in with the StratifiedShuffleSplit tester with 100 iterations but did not break the 0.3 thresholds for both with the Udacity Grader. So now I am going to try new parameters found by running the StratifiedShuffleSplit test with the full data set.  \n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "And these are the results going through the test classifier:\n",
    "\n",
    "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='most_frequent',\n",
    "    verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('pca', PCA(copy=True, n_components=6, whiten=False)), ('clf', SVC(C=1000000, cache_size=200, class_weight=None, coef0=0.0,\n",
    "  decision_function_shape=None, degree=3, gamma=0.1, kernel='rbf',\n",
    "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "  tol=0.001, verbose=False))])\n",
    "\tAccuracy: 0.76743\tPrecision: 0.20572\tRecall: 0.21950\tF1: 0.21239\tF2: 0.21660\n",
    "\tTotal predictions: 14000\tTrue positives:  439\tFalse positives: 1695\tFalse negatives: 1561\tTrue negatives: 10305\n",
    "```\n",
    "So these results are even worse. The lesson is that using the whole data set gives even more misleading parameters for the grader. \n",
    "\n",
    "\n",
    "### Select K Best\n",
    "So here is what is driving me crazy. I get this result from using select K Best with the SVC: \n",
    "```\n",
    "Best parameters from the grid search:{'selection__k': 13}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.971428571429\n",
    "\n",
    "Recall Score: 0.833333333333\n",
    "\n",
    "Precision Score: 1.0\n",
    "```\n",
    "Great, heh? But when I give it to the grader: \n",
    "```\n",
    "And these are the results going through the test classifier:\n",
    "\n",
    "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='most_frequent',\n",
    "    verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=13, score_func=<function f_classif at 0x10e2f40c8>)), ('pca', PCA(copy=True, n_components=6, whiten=False)...,\n",
    "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "  tol=0.001, verbose=False))])\n",
    "\tAccuracy: 0.76779\tPrecision: 0.20172\tRecall: 0.21150\tF1: 0.20649\tF2: 0.20947\n",
    "\tTotal predictions: 14000\tTrue positives:  423\tFalse positives: 1674\tFalse negatives: 1577\tTrue negatives: 10326\n",
    "```\n",
    "\n",
    "How am I going to make this better? \n",
    "\n",
    "### Winner? \n",
    "\n",
    "\n",
    "And these are the results going through the test classifier:\n",
    "```\n",
    "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='most_frequent',\n",
    "    verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=16, score_func=<function f_classif at 0x10e2f40c8>)), ('pca', PCA(copy=True, n_components=3, whiten=False)...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False))])\n",
    "\tAccuracy: 0.82979\tPrecision: 0.37872\tRecall: 0.29900\tF1: 0.33417\tF2: 0.31214\n",
    "\tTotal predictions: 14000\tTrue positives:  598\tFalse positives:  981\tFalse negatives: 1402\tTrue negatives: 11019\n",
    "```\n",
    "Close enough for government work I would say. \n",
    "\n",
    "One more test\n",
    "\n",
    "## Victory!\n",
    "\n",
    "And these are the results going through the test classifier:\n",
    "```\n",
    "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=16, score_func=<function f_classif at 0x10e2f40c8>)), ('pca', PCA(copy=True, n_components=3, whiten=False)), ('clf', Gr...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False))])\n",
    "\tAccuracy: 0.81564\tPrecision: 0.33924\tRecall: 0.30650\tF1: 0.32204\tF2: 0.31253\n",
    "\tTotal predictions: 14000\tTrue positives:  613\tFalse positives: 1194\tFalse negatives: 1387\tTrue negatives: 10806\n",
    "```    \n",
    "### Again, but with the custom scoring function\n",
    "\n",
    "\n",
    "\n",
    "And these are the results going through the test classifier:\n",
    "\n",
    "```\n",
    "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=16, score_func=<function f_classif at 0x10e2f40c8>)), ('pca', PCA(copy=True, n_components=3, whiten=False)), ('clf', ...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False))])\n",
    "\tAccuracy: 0.82664\tPrecision: 0.38757\tRecall: 0.36800\tF1: 0.37753\tF2: 0.37175\n",
    "\tTotal predictions: 14000\tTrue positives:  736\tFalse positives: 1163\tFalse negatives: 1264\tTrue negatives: 10837\n",
    "```\n",
    "So now I finally have something that works well. The thing is that the function gives me 1.0 on all the parameters so the answers at this point from the tests are just arbitrary? Maybe there are lots of parameters that give me 1.0 and it is just stopping at the first one that gives me that? \n",
    "\n",
    "### more\n",
    "\n",
    "And these are the results going through the test classifier:\n",
    "```\n",
    "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=16, score_func=<function f_classif at 0x10e2f40c8>)), ('pca', PCA(copy=True, n_components=3, whiten=False)), ('clf', ...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False))])\n",
    "\tAccuracy: 0.82586\tPrecision: 0.38510\tRecall: 0.36700\tF1: 0.37583\tF2: 0.37048\n",
    "\tTotal predictions: 14000\tTrue positives:  734\tFalse positives: 1172\tFalse negatives: 1266\tTrue negatives: 10828\n",
    "```   \n",
    "## best?\n",
    "\n",
    "And these are the results going through the test classifier:\n",
    "\n",
    "```\n",
    "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=16, score_func=<function f_classif at 0x10e2f40c8>)), ('pca', PCA(copy=True, n_components=3, whiten=False)), ('clf', ...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False))])\n",
    "\tAccuracy: 0.82450\tPrecision: 0.38203\tRecall: 0.37000\tF1: 0.37592\tF2: 0.37235\n",
    "\tTotal predictions: 14000\tTrue positives:  740\tFalse positives: 1197\tFalse negatives: 1260\tTrue negatives: 10803\n",
    "```\n",
    "### Diminishing returns? \n",
    "Tried moving the learning rate up to 0.8 from 0.7. \n",
    "\n",
    "```\n",
    "\n",
    "And these are the results going through the test classifier:\n",
    "\n",
    "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=16, score_func=<function f_classif at 0x10e2f40c8>)), ('pca', PCA(copy=True, n_components=3, whiten=False)), ('clf', ...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False))])\n",
    "\tAccuracy: 0.82464\tPrecision: 0.38108\tRecall: 0.36450\tF1: 0.37260\tF2: 0.36770\n",
    "\tTotal predictions: 14000\tTrue positives:  729\tFalse positives: 1184\tFalse negatives: 1271\tTrue negatives: 10816\n",
    "```\n",
    "### Not much better. Learning rate 0.6\n",
    "```\n",
    "\n",
    "And these are the results going through the test classifier:\n",
    "\n",
    "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=16, score_func=<function f_classif at 0x10e2f40c8>)), ('pca', PCA(copy=True, n_components=3, whiten=False)), ('clf', ...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False))])\n",
    "\tAccuracy: 0.82543\tPrecision: 0.38279\tRecall: 0.36250\tF1: 0.37237\tF2: 0.36638\n",
    "\tTotal predictions: 14000\tTrue positives:  725\tFalse positives: 1169\tFalse negatives: 1275\tTrue negatives: 10831\n",
    "```\n",
    "\n",
    "## Exponential Wins!\n",
    "The best score yet. Maybe the thing was getting 1's on everything so it was just getting me the first thing that showed up? \n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "And these are the results going through the test classifier:\n",
    "\n",
    "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=16, score_func=<function f_classif at 0x10e2f40c8>)), ('pca', PCA(copy=True, n_components=3, whiten=False)), ('clf', ...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False))])\n",
    "\tAccuracy: 0.82864\tPrecision: 0.39394\tRecall: 0.37050\tF1: 0.38186\tF2: 0.37496\n",
    "\tTotal predictions: 14000\tTrue positives:  741\tFalse positives: 1140\tFalse negatives: 1259\tTrue negatives: 10860\n",
    "```\n",
    "\n",
    "### Going Down, reduced select K best to 15\n",
    "\n",
    "```\n",
    "\n",
    "And these are the results going through the test classifier:\n",
    "\n",
    "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=15, score_func=<function f_classif at 0x10e2f40c8>)), ('pca', PCA(copy=True, n_components=3, whiten=False)), ('clf', ...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False))])\n",
    "\tAccuracy: 0.82250\tPrecision: 0.36668\tRecall: 0.33350\tF1: 0.34931\tF2: 0.33965\n",
    "\tTotal predictions: 14000\tTrue positives:  667\tFalse positives: 1152\tFalse negatives: 1333\tTrue negatives: 10848\n",
    "```\n",
    "\n",
    "### Max Depth up to 10 doesn't help\n",
    "\n",
    "And these are the results going through the test classifier:\n",
    "```\n",
    "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=16, score_func=<function f_classif at 0x10e2f40c8>)), ('pca', PCA(copy=True, n_components=3, whiten=False)), ('clf', ...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False))])\n",
    "\tAccuracy: 0.82643\tPrecision: 0.38660\tRecall: 0.36650\tF1: 0.37628\tF2: 0.37035\n",
    "\tTotal predictions: 14000\tTrue positives:  733\tFalse positives: 1163\tFalse negatives: 1267\tTrue negatives: 10837\n",
    "```\n",
    "\n",
    "## Highest Precision from highest learning rate, 0.8\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "And these are the results going through the test classifier:\n",
    "\n",
    "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=16, score_func=<function f_classif at 0x10e2f40c8>)), ('pca', PCA(copy=True, n_components=3, whiten=False)), ('clf', ...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False))])\n",
    "\tAccuracy: 0.82800\tPrecision: 0.39137\tRecall: 0.36750\tF1: 0.37906\tF2: 0.37204\n",
    "\tTotal predictions: 14000\tTrue positives:  735\tFalse positives: 1143\tFalse negatives: 1265\tTrue negatives: 10857\n",
    "```\n",
    "\n",
    "### Learning Rate at 0.9 no better\n",
    "```\n",
    "\n",
    "And these are the results going through the test classifier:\n",
    "\n",
    "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=16, score_func=<function f_classif at 0x10e2f40c8>)), ('pca', PCA(copy=True, n_components=3, whiten=False)), ('clf', ...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False))])\n",
    "\tAccuracy: 0.82629\tPrecision: 0.38387\tRecall: 0.35700\tF1: 0.36995\tF2: 0.36207\n",
    "\tTotal predictions: 14000\tTrue positives:  714\tFalse positives: 1146\tFalse negatives: 1286\tTrue negatives: 10854\n",
    "```\n",
    "### Trying Whiten=True\n",
    "```\n",
    "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=16, score_func=<function f_classif at 0x10e2f40c8>)), ('pca', PCA(copy=True, n_components=3, whiten=True)), ('clf', G...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False))])\n",
    "\tAccuracy: 0.82479\tPrecision: 0.38035\tRecall: 0.36000\tF1: 0.36989\tF2: 0.36389\n",
    "\tTotal predictions: 14000\tTrue positives:  720\tFalse positives: 1173\tFalse negatives: 1280\tTrue negatives: 10827\n",
    "```\n",
    "### Tried Deviance \n",
    "```\n",
    "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=16, score_func=<function f_classif at 0x10e2f40c8>)), ('pca', PCA(copy=True, n_components=3, whiten=True)), ('clf', G...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False))])\n",
    "\tAccuracy: 0.82421\tPrecision: 0.37989\tRecall: 0.36450\tF1: 0.37203\tF2: 0.36748\n",
    "\tTotal predictions: 14000\tTrue positives:  729\tFalse positives: 1190\tFalse negatives: 1271\tTrue negatives: 10810\n",
    "```\n",
    "\n",
    "Seems like going to back to exponential is the right idea.\n",
    "\n",
    "###   GradientBoostingClassifier(n_estimators=100,loss=\"exponential\", max_depth=8, learning_rate=0.8)\n",
    "```\n",
    "\n",
    "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=16, score_func=<function f_classif at 0x10e2f40c8>)), ('pca', PCA(copy=True, n_components=3, whiten=True)), ('clf', G...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False))])\n",
    "\tAccuracy: 0.82657\tPrecision: 0.38593\tRecall: 0.36200\tF1: 0.37358\tF2: 0.36655\n",
    "\tTotal predictions: 14000\tTrue positives:  724\tFalse positives: 1152\tFalse negatives: 1276\tTrue negatives: 10848\n",
    "```\n",
    "### 'n_estimators=100,loss=\"exponential\", max_depth=9, learning_rate=0.7\n",
    "whiten was false\n",
    "```\n",
    "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=16, score_func=<function f_classif at 0x10e2f40c8>)), ('pca', PCA(copy=True, n_components=3, whiten=False)), ('clf', ...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False))])\n",
    "\tAccuracy: 0.82514\tPrecision: 0.38272\tRecall: 0.36550\tF1: 0.37391\tF2: 0.36882\n",
    "\tTotal predictions: 14000\tTrue positives:  731\tFalse positives: 1179\tFalse negatives: 1269\tTrue negatives: 10821\n",
    "```\n",
    "### kbest = 16, PCA(n_components=4) n_estimators=100, loss=\"exponential\", max_depth=9, learning_rate=0.7\n",
    "\n",
    "\n",
    "```\n",
    "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=16, score_func=<function f_classif at 0x10e2f40c8>)), ('pca', PCA(copy=True, n_components=4, whiten=False)), ('clf', ...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False))])\n",
    "\tAccuracy: 0.81100\tPrecision: 0.32910\tRecall: 0.31100\tF1: 0.31979\tF2: 0.31446\n",
    "\tTotal predictions: 14000\tTrue positives:  622\tFalse positives: 1268\tFalse negatives: 1378\tTrue negatives: 10732\n",
    "```\n",
    "\n",
    "### , SelectKBest(k=15)),PCA(n_components=3) n_estimators=100,loss=\"exponential\", max_depth=9, learning_rate=0.7))\n",
    "\n",
    "```\n",
    "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=15, score_func=<function f_classif at 0x10e2f40c8>)), ('pca', PCA(copy=True, n_components=3, whiten=False)), ('clf', ...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False))])\n",
    "\tAccuracy: 0.82329\tPrecision: 0.37092\tRecall: 0.34050\tF1: 0.35506\tF2: 0.34618\n",
    "\tTotal predictions: 14000\tTrue positives:  681\tFalse positives: 1155\tFalse negatives: 1319\tTrue negatives: 10845\n",
    "```\n",
    "I think I have enough to go on now. \n",
    "\n",
    "\n",
    "## April 30, going backwards\n",
    "Tried new data prep and and the new grid_search. Can't get the precision and recall scores from the grid search but when I tried the parameters in the grader it came out: \n",
    "```\n",
    "\n",
    "\n",
    "And these are the results going through the test classifier:\n",
    "\n",
    "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=16, score_func=<function f_classif at 0x10e2f40c8>)), ('pca', PCA(copy=True, n_components=9, whiten=False)), ('clf', ...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False))])\n",
    "\tAccuracy: 0.81421\tPrecision: 0.31065\tRecall: 0.24650\tF1: 0.27488\tF2: 0.25712\n",
    "\tTotal predictions: 14000\tTrue positives:  493\tFalse positives: 1094\tFalse negatives: 1507\tTrue negatives: 10906\n",
    "```\n",
    "So that is moving backwards. Eveything was tuned except the n_estimators was tuned (and I assume it was at the default of 100). \n",
    "\n",
    "### Reproducing Last High Scores of pre=0.39, recall=0.36\n",
    "Trying to reproduce results from last week now with the grader. \n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "And these are the results going through the test classifier:\n",
    "\n",
    "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=16, score_func=<function f_classif at 0x10e2f40c8>)), ('pca', PCA(copy=True, n_components=3, whiten=False)), ('clf', ...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False))])\n",
    "\tAccuracy: 0.83429\tPrecision: 0.39796\tRecall: 0.31200\tF1: 0.34978\tF2: 0.32609\n",
    "\tTotal predictions: 14000\tTrue positives:  624\tFalse positives:  944\tFalse negatives: 1376\tTrue negatives: 11056\n",
    "```\n",
    "Almost as good at before. Had to gues on the max_depth and the learning rate. Tried with 5 for max_depth and 0.2 for learning rate. Will try 0.1 and 6: \n",
    "\n",
    "## Best Yet. \n",
    "Had set max_depth at 6 and learning rate at 0.1. k=16 and pca=3. Now I will try to get it on the grid search. \n",
    "```\n",
    "And these are the results going through the test classifier:\n",
    "\n",
    "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=16, score_func=<function f_classif at 0x10e2f40c8>)), ('pca', PCA(copy=True, n_components=3, whiten=False)), ('clf', ...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False))])\n",
    "\tAccuracy: 0.83471\tPrecision: 0.41209\tRecall: 0.36800\tF1: 0.38880\tF2: 0.37605\n",
    "\tTotal predictions: 14000\tTrue positives:  736\tFalse positives: 1050\tFalse negatives: 1264\tTrue negatives: 10950\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "First attempt with the new scorer. Not sure whether this is the old or the new data set. \n",
    "```\n",
    "\n",
    "Best parameters from the grid search:{'clf__max_depth': 8, 'imp__strategy': 'median', 'pca__n_components': 9}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.772727272727\n",
    "\n",
    "\n",
    "Recall Score: 0.2\n",
    "\n",
    "\n",
    "Precision Score: 0.142857142857\n",
    "```\n",
    "This time I am doing it with the original data set for sure. \n",
    "```\n",
    "\n",
    "Best parameters from the grid search:{'clf__max_depth': 5,\n",
    " 'imp__strategy': 'most_frequent',\n",
    " 'pca__n_components': 10}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.886363636364\n",
    "\n",
    "Recall Score: 0.2\n",
    "\n",
    "Precision Score: 0.5\n",
    "```\n",
    "With the original data in the data set we seem to do a little better than with the new, created variables. Precision seems to be a lot better with the original, untreated data. \n",
    " \n",
    "I am running it one more time with \\_new data just to make sure that was the problem. \n",
    "```\n",
    "Best parameters from the grid search:{'clf__max_depth': 9, 'imp__strategy': 'median', 'pca__n_components': 4}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.818181818182\n",
    "\n",
    "Recall Score: 0.2\n",
    "\n",
    "Precision Score: 0.2\n",
    "```\n",
    "Ok, that is not exactly the result we got before but it still shows the data with additional data to perform less well. \n",
    "\n",
    "Also, have to run Vivek's suggestion of limiting the depth of the trees to 2 or 3 or setting a minimum sample split because there is so little data. So here is one more time with the max depth set lower. That should not increase the preformance of the model (since our scoring features is selecting the model that performs the best) but it might provide some insight. Also, again following Vivek's suggestion, I will limit the principle component analysis to 10. \n",
    "```\n",
    "Best parameters from the grid search:{'clf__max_depth': 3, 'imp__strategy': 'most_frequent', 'pca__n_components': 3}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.75\n",
    "\n",
    "Recall Score: 0.2\n",
    "\n",
    "Precision Score: 0.125\n",
    "```\n",
    "Yeah, not such hot performance but at least is makes more sense. \n",
    "\n",
    "Now I am going to see if the model can work better with the two variables that have no association with the dependent variable, the 'drop' data set. \n",
    "```\n",
    "\n",
    "Best parameters from the grid search:{'clf__max_depth': 3, 'imp__strategy': 'most_frequent', 'pca__n_components': 6}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.977272727273\n",
    "\n",
    "Recall Score: 1.0\n",
    "\n",
    "Precision Score: 0.833333333333\n",
    "```\n",
    "Now, with that as a base line lets see if the Robust Scaler works better. \n",
    "```\n",
    "\n",
    "Best parameters from the grid search:{'clf__max_depth': 3, 'imp__strategy': 'most_frequent', 'pca__n_components': 7}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.909090909091\n",
    "\n",
    "Recall Score: 0.2\n",
    "\n",
    "Precision Score: 1.0\n",
    "```\n",
    "Ok, this is a lot worse. In fact, it is so much worse that I can't believe these results are correct. I must have changed something other than the just the scaler. \n",
    "\n",
    "These are the results with the two 0 poi variables dropped and the grid search run on the whole data set. \n",
    "```\n",
    "Best parameters from the grid search:{'clf__max_depth': 2, 'imp__strategy': 'most_frequent', 'pca__n_components': 8}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.909090909091\n",
    "\n",
    "Recall Score: 0.444444444444\n",
    "\n",
    "Precision Score: 0.888888888889\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results with 1000, 10 and 100 iterations\n",
    "\n",
    "### 1000 iterations\n",
    "So the first outcome from the model is pretty good. It is StratifiedShuffleSplit with 1000 iterations. The outcome was really good but it took forever. \n",
    "\n",
    "```\n",
    "Best parameters from the grid search:{'clf__max_depth': 9, 'pca__n_components': 10}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.886363636364\n",
    "\n",
    "\n",
    "Recall Score: 0.6\n",
    "\n",
    "\n",
    "Precision Score: 0.5\n",
    "```\n",
    "### 10 iterations\n",
    "This is great but I just can't use this for testing things out. It will take forever. So, I am going to try it with 10 iterations and see what happens. \n",
    "\n",
    "```\n",
    "\n",
    "Best parameters from the grid search:{'clf__max_depth': 9, 'pca__n_components': 12}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.863636363636\n",
    "\n",
    "\n",
    "Recall Score: 0.4\n",
    "\n",
    "\n",
    "Precision Score: 0.4\n",
    "```\n",
    "### 100\n",
    "Now we get the same depth but 12 instead of 10 principle components. The scores have gone down though. It is worth trying it at a higher number of iterations. I am going to try 100. Here is the outcome: \n",
    "```\n",
    "Best Estimator Accuracy: 0.818181818182\n",
    "\n",
    "\n",
    "Recall Score: 0.4\n",
    "\n",
    "\n",
    "Precision Score: 0.285714285714\n",
    "```\n",
    "Ok, so 100 iterations is worse than either 1000 or 10 iterations. That is kind of distressing. It would be nice if the behavior of the model was 'monotonic', that is, the more of one thing you do the more of something you are looking for you get. I was thinking that 100 iterations could serve as a good way to explore possibilities and narrow the search space while reserving 1000 iteration runs to make the final cut. Now I am not quite sure what to do. \n",
    "\n",
    "And here is another thing. I just ran the model again with 1000 iterations with the single addition of the clf's criterion being 'gini' or 'entropy'. It came back with gini as the better criterion. And the model had performance that was just as good as before on the 1000 iterations. In fact, the three scores of interest--accuracy, precision and recall--were exactly the same. But it found a max depth of 7 instead of 9. \n",
    "```\n",
    "Best parameters from the grid search:{'clf__criterion': 'gini', 'clf__max_depth': 7, 'pca__n_components': 10}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.886363636364\n",
    "\n",
    "\n",
    "Recall Score: 0.6\n",
    "\n",
    "\n",
    "Precision Score: 0.5\n",
    "```\n",
    "The reason that is so puzzling to me is that gini is the default scoring criterion for the model. So it was using gini before when it found the best preformance was at max depth of 9 instead of 7. How does that happen?\n",
    "\n",
    "### Adding variables\n",
    "\n",
    "In this iteration I try adding some new variables and the choice between the median and most_frequent imputation for the missing values. I also limited the search space for 'max_depth' and the number of components to 6-8 and 8-12 respectively. That may have been a mistake because I had found that the most effective max depth was 9. I don't know why I did that. \n",
    "```\n",
    "\n",
    "Best parameters from the grid search:{'clf__max_depth': 7, 'imp__strategy': 'most_frequent', 'pca__n_components': 8}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.886363636364\n",
    "\n",
    "\n",
    "Recall Score: 0.2\n",
    "\n",
    "\n",
    "Precision Score: 0.5\n",
    "```\n",
    "The results are strange in that adding more variables decreased the preformance, but I can't be sure that it wasn't because of my odd choice of cutting off the max_depth at 8 instead of 9. \n",
    "\n",
    "Also, I don't know why the max_depth is a useful parameter since the default is None. How could limiting the parameter improve preformance? \n",
    "\n",
    "## New Variables with modified parameter search space\n",
    "\n",
    "Just to satisfy my curiosity I am going to increase the max_depth parameter to 10 and raise the maximum number of principle components to 16 to account for the four newly added variables.\n",
    "\n",
    "#### Civil Libertarian Improvement\n",
    "\n",
    "So this turns out to have been a good idea. I have had a slight loss in recall but big gains in accuracy and precision. Given that I am a civil libertarian I personally feel this is an improvement. \n",
    "\n",
    "```\n",
    "\n",
    "Best parameters from the grid search:{'clf__max_depth': 8, 'imp__strategy': 'most_frequent', 'pca__n_components': 8}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.909090909091\n",
    "\n",
    "\n",
    "Recall Score: 0.4\n",
    "\n",
    "\n",
    "Precision Score: 0.666666666667\n",
    "```\n",
    "Now I have one problem with these results in that the number of principle components chosen was at the bottom of the range I had specified. So now I have to face the possibility that there was better preforming model with a smaller dimensional principle component space that was over looked by the paramters to which I limited the grid search. So, as a double check, I am going to let the space searced go down. I am also going to keep the 'most_frequent' strategy and not search that space anymore. \n",
    "\n",
    "### Fewer dimensions: Insanity\n",
    "\n",
    "Ok, this is the kind of insane behavior that is driving me insane. \n",
    "```\n",
    "\n",
    "Best parameters from the grid search:{'clf__max_depth': 8, 'pca__n_components': 7}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.863636363636\n",
    "\n",
    "\n",
    "Recall Score: 0.2\n",
    "\n",
    "\n",
    "Precision Score: 0.333333333333\n",
    "```\n",
    "Now, all I did was specify 'most_frequent' as the method of imputation and offered it the choice of finding few dimensions in the principle component analysis. Everything else was the same. So how could the model get worse? It had the choice of keeping the model that had preformed better in the last specificaiton of the model, so how could it get worse? \n",
    "## Showdown\n",
    "Ok, I am just going to test everything. I am going to have a model of 200 different possible parameter configurations, but I am going to know once and for all what's what. Since I have never had entropy come up as the better scoring criterion I am going to leave that out but I am going to try everything else I have tried with the broades possible ranges. \n",
    "\n",
    "I am doing this essentially because I have to know once and for all whether more variables improve things or not. \n",
    "```\n",
    "\n",
    "Best parameters from the grid search:{'clf__max_depth': 7, 'imp__strategy': 'most_frequent', 'pca__n_components': 8}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.886363636364\n",
    "\n",
    "\n",
    "Recall Score: 0.2\n",
    "\n",
    "\n",
    "Precision Score: 0.5\n",
    "```\n",
    "So by expanding the grid search the model actually does worse. That is really distressing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Test\n",
    "So now I have been running into cases where I expand the search grid but end up with a less effective model. This is puzzling. It seems that if I have the models parameters set wider but still including the original model it should do at least as well as the original model. So, I am going to test the proposition and try to recreate the problem. \n",
    "\n",
    "First I will run the model that preformed the best above. \n",
    "\n",
    "Then I will run a model with a slightly expanded grid search. \n",
    "\n",
    "Then I will run a model with a much expanded grid search. \n",
    "\n",
    "### Larger Data Set-Original Settings\n",
    "So I tried it with the original grid search settings, with the choice between 'median' and most_frequent', principle components pared down to the range from 2 to 10 and the max depth set to the range from 6 to 9. The results were pretty good. \n",
    "```\n",
    "\n",
    "Best parameters from the grid search:{'clf__max_depth': 8, 'imp__strategy': 'most_frequent', 'pca__n_components': 8}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.909090909091\n",
    "\n",
    "\n",
    "Recall Score: 0.4\n",
    "\n",
    "\n",
    "Precision Score: 0.666666666667\n",
    "```\n",
    "That is reassuring because it is the exact outcome from the model that I labeled the 'civil libertarian improvement' above. \n",
    "\n",
    "What was odd was when I gave it the exact settings that had preformed best in the broadest grid search with the smaller data set: depth 9/components 10, I got the error about something being 0. Then I changed the grid search to have lists for the components and depth parameters of 8,9 and 9,10 respectively so that model would run without objecting that it needed lists in the grid search parameters. The outcome is acceptable but the preformance has gone down since adding new variables. That is really weird. \n",
    "```\n",
    "\n",
    "Best parameters from the grid search:{'clf__max_depth': 8, 'imp__strategy': 'most_frequent', 'pca__n_components': 9}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.886363636364\n",
    "\n",
    "\n",
    "Recall Score: 0.4\n",
    "\n",
    "\n",
    "Precision Score: 0.5\n",
    "```\n",
    "So maybe adding new variables that are simply restatements of the information in the old variables does not really help. Maybe it makes things worse by increasing dimensionality without adding genuinely new information, or at least not new enough information to justify the increase in the dimensionality of the search space. (?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to start keeping records on the results of the gradient boosting classifier as I make changes to the data set. \n",
    "\n",
    "Here the GradientBoostingClassier had n_estimators=100 and used 0's for the missing data. The data set was `get_features_labels_new_27()` and had all the columns with more than 90 NaN's (about 4) removed. \n",
    "\n",
    "```\n",
    "\n",
    "Best parameters from the grid search:{'clf__learning_rate': 0.05, 'clf__max_depth': 4, 'clf__max_features': 0.7}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.888888888889\n",
    "\n",
    "Recall Score: 0.0\n",
    "\n",
    "Precision Score: 0.0\n",
    "```\n",
    "Not very promising. \n",
    "\n",
    "### Adding Email Data\n",
    "\n",
    "GBC with email variables added in and the five 90+ NaN columns droped. Also used the grid search for imputation method. \n",
    "\n",
    "```\n",
    "Best parameters from the grid search:{'clf__learning_rate': 0.05,\n",
    " 'clf__max_depth': 4,\n",
    " 'clf__max_features': 0.5,\n",
    " 'imp__strategy': 'mean'}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.8\n",
    "\n",
    "Recall Score: 0.142857142857\n",
    "\n",
    "Precision Score: 0.5\n",
    "```\n",
    "Just can't figure out how to get the recall up! \n",
    "\n",
    "### MinMaxScaling before Standardization\n",
    "GBC with email variables added in and the five 90+ NaN columns droped\n",
    "\n",
    "Did MinMax Scaling and imputing mean values for missing data before doing grid search. Only did 10 iternations. Going to try 1000\n",
    "```\n",
    "Best parameters from the grid search:{'clf__learning_rate': 0.1, 'clf__max_depth': 3, 'clf__max_features': 0.5}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.828571428571\n",
    "\n",
    "Recall Score: 0.333333333333\n",
    "\n",
    "Precision Score: 0.5\n",
    "```\n",
    "\n",
    "### With 100 iterations\n",
    "Somehow it gets worse on precision while getting better on Recall. There might be something wrong with the score function: \n",
    "\n",
    "```\n",
    "GBC with email variables added in and the five 90+ NaN columns droped\n",
    "Best parameters from the grid search:{'clf__learning_rate': 0.5, 'clf__max_depth': 4, 'clf__max_features': 0.5}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.685714285714\n",
    "\n",
    "Recall Score: 0.5\n",
    "\n",
    "Precision Score: 0.0909090909091\n",
    "```\n",
    "Maybe there is something wrong with the score function I have borrowed so I will try the f1:\n",
    "### With f1 as scoring function\n",
    "\n",
    "\n",
    "GBC with email variables added in and the five 90+ NaN columns droped and MinMaxScaling done before hand. \n",
    "Also, 100 iterations.\n",
    "```\n",
    "Best parameters from the grid search:{'clf__learning_rate': 0.05, 'clf__max_depth': 3, 'clf__max_features': 1}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.828571428571\n",
    "\n",
    "Recall Score: 0.25\n",
    "\n",
    "Precision Score: 0.25\n",
    "```\n",
    "I am going to stick with the f1 scorer for a while now. Next I will try the PCA before giving it GBC.\n",
    "\n",
    "### With PCA in pipeline\n",
    "Got a huge increase in Recall but precision is bad again. \n",
    "\n",
    "```\n",
    "GBC with email variables added in and the five 90+ NaN columns droped\n",
    "Best parameters from the grid search:{'clf__learning_rate': 0.05,\n",
    " 'clf__max_depth': 4,\n",
    " 'clf__max_features': 0.7,\n",
    " 'pca__n_components': 3}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.542857142857\n",
    "\n",
    "Recall Score: 0.6\n",
    "\n",
    "Precision Score: 0.176470588235\n",
    "```\n",
    "Just to keep ourselves sane we will try it with the custom score function. \n",
    "\n",
    "\n",
    "\n",
    "### w/ Custom Scorer\n",
    "\n",
    "```\n",
    "GBC with email variables added in and the five 90+ NaN columns droped\n",
    "Best parameters from the grid search:{'clf__learning_rate': 0.1,\n",
    " 'clf__max_depth': 4,\n",
    " 'clf__max_features': 0.7,\n",
    " 'pca__n_components': 10}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.685714285714\n",
    "\n",
    "Recall Score: 0.2\n",
    "\n",
    "Precision Score: 0.125\n",
    "```\n",
    "Ok, so the custom scorer sucks. No more of that. I will try the f1 scorer with 100 iterations, though. \n",
    "\n",
    "### F1 with 100 iterations\n",
    "Gave up. Took too long\n",
    "Now adding parameters. \n",
    "\n",
    "```\n",
    "GBC with email variables added in and the five 90+ NaN columns droped\n",
    "Best parameters from the grid search:{'clf__learning_rate': 0.7,\n",
    " 'clf__max_depth': 6,\n",
    " 'clf__max_features': 0.1,\n",
    " 'pca__n_components': 4}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.8\n",
    "\n",
    "Recall Score: 0.5\n",
    "\n",
    "Precision Score: 0.285714285714\n",
    "```\n",
    "That was pretty good with only 10 iterations. The learning rate was the highest number in the list so there might still be room or improvement. \n",
    "\n",
    "### Adding higher learning rate choice\n",
    "Trimming the pca and learning rate numbers of alternatives that were rejected. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
