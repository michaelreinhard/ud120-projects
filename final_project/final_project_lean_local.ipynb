{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the final project with all of the experiments stripped out.\n",
    "\n",
    "### load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.preprocessing import Imputer\n",
    "                                                \n",
    "from pprint import pprint\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import data into Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bonus</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>email_address</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>expenses</th>\n",
       "      <th>from_messages</th>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <th>...</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>other</th>\n",
       "      <th>poi</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>salary</th>\n",
       "      <th>shared_receipt_with_poi</th>\n",
       "      <th>to_messages</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>total_stock_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ALLEN PHILLIP K</th>\n",
       "      <td>4175000</td>\n",
       "      <td>2869717</td>\n",
       "      <td>-3081055</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phillip.allen@enron.com</td>\n",
       "      <td>1729541</td>\n",
       "      <td>13868</td>\n",
       "      <td>2195</td>\n",
       "      <td>47</td>\n",
       "      <td>65</td>\n",
       "      <td>...</td>\n",
       "      <td>304805</td>\n",
       "      <td>152</td>\n",
       "      <td>False</td>\n",
       "      <td>126027</td>\n",
       "      <td>-126027</td>\n",
       "      <td>201955</td>\n",
       "      <td>1407</td>\n",
       "      <td>2902</td>\n",
       "      <td>4484442</td>\n",
       "      <td>1729541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BADUM JAMES P</th>\n",
       "      <td>NaN</td>\n",
       "      <td>178980</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>257817</td>\n",
       "      <td>3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>182466</td>\n",
       "      <td>257817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BANNANTINE JAMES M</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-5104</td>\n",
       "      <td>NaN</td>\n",
       "      <td>james.bannantine@enron.com</td>\n",
       "      <td>4046157</td>\n",
       "      <td>56301</td>\n",
       "      <td>29</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>864523</td>\n",
       "      <td>False</td>\n",
       "      <td>1757552</td>\n",
       "      <td>-560222</td>\n",
       "      <td>477</td>\n",
       "      <td>465</td>\n",
       "      <td>566</td>\n",
       "      <td>916197</td>\n",
       "      <td>5243487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BAXTER JOHN C</th>\n",
       "      <td>1200000</td>\n",
       "      <td>1295738</td>\n",
       "      <td>-1386055</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6680544</td>\n",
       "      <td>11200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1586055</td>\n",
       "      <td>2660303</td>\n",
       "      <td>False</td>\n",
       "      <td>3942714</td>\n",
       "      <td>NaN</td>\n",
       "      <td>267102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5634343</td>\n",
       "      <td>10623258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BAY FRANKLIN R</th>\n",
       "      <td>400000</td>\n",
       "      <td>260455</td>\n",
       "      <td>-201641</td>\n",
       "      <td>NaN</td>\n",
       "      <td>frank.bay@enron.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>129142</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69</td>\n",
       "      <td>False</td>\n",
       "      <td>145796</td>\n",
       "      <td>-82782</td>\n",
       "      <td>239671</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>827696</td>\n",
       "      <td>63014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      bonus deferral_payments deferred_income director_fees  \\\n",
       "ALLEN PHILLIP K     4175000           2869717        -3081055           NaN   \n",
       "BADUM JAMES P           NaN            178980             NaN           NaN   \n",
       "BANNANTINE JAMES M      NaN               NaN           -5104           NaN   \n",
       "BAXTER JOHN C       1200000           1295738        -1386055           NaN   \n",
       "BAY FRANKLIN R       400000            260455         -201641           NaN   \n",
       "\n",
       "                                 email_address exercised_stock_options  \\\n",
       "ALLEN PHILLIP K        phillip.allen@enron.com                 1729541   \n",
       "BADUM JAMES P                              NaN                  257817   \n",
       "BANNANTINE JAMES M  james.bannantine@enron.com                 4046157   \n",
       "BAXTER JOHN C                              NaN                 6680544   \n",
       "BAY FRANKLIN R             frank.bay@enron.com                     NaN   \n",
       "\n",
       "                   expenses from_messages from_poi_to_this_person  \\\n",
       "ALLEN PHILLIP K       13868          2195                      47   \n",
       "BADUM JAMES P          3486           NaN                     NaN   \n",
       "BANNANTINE JAMES M    56301            29                      39   \n",
       "BAXTER JOHN C         11200           NaN                     NaN   \n",
       "BAY FRANKLIN R       129142           NaN                     NaN   \n",
       "\n",
       "                   from_this_person_to_poi        ...         \\\n",
       "ALLEN PHILLIP K                         65        ...          \n",
       "BADUM JAMES P                          NaN        ...          \n",
       "BANNANTINE JAMES M                       0        ...          \n",
       "BAXTER JOHN C                          NaN        ...          \n",
       "BAY FRANKLIN R                         NaN        ...          \n",
       "\n",
       "                   long_term_incentive    other    poi restricted_stock  \\\n",
       "ALLEN PHILLIP K                 304805      152  False           126027   \n",
       "BADUM JAMES P                      NaN      NaN  False              NaN   \n",
       "BANNANTINE JAMES M                 NaN   864523  False          1757552   \n",
       "BAXTER JOHN C                  1586055  2660303  False          3942714   \n",
       "BAY FRANKLIN R                     NaN       69  False           145796   \n",
       "\n",
       "                   restricted_stock_deferred  salary shared_receipt_with_poi  \\\n",
       "ALLEN PHILLIP K                      -126027  201955                    1407   \n",
       "BADUM JAMES P                            NaN     NaN                     NaN   \n",
       "BANNANTINE JAMES M                   -560222     477                     465   \n",
       "BAXTER JOHN C                            NaN  267102                     NaN   \n",
       "BAY FRANKLIN R                        -82782  239671                     NaN   \n",
       "\n",
       "                   to_messages total_payments total_stock_value  \n",
       "ALLEN PHILLIP K           2902        4484442           1729541  \n",
       "BADUM JAMES P              NaN         182466            257817  \n",
       "BANNANTINE JAMES M         566         916197           5243487  \n",
       "BAXTER JOHN C              NaN        5634343          10623258  \n",
       "BAY FRANKLIN R             NaN         827696             63014  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "df = pd.DataFrame(data_dict)\n",
    "df = df.transpose()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Select Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop('email_address', axis=1)\n",
    "df = df.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Eliminate Outliers/Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.drop('TOTAL')\n",
    "df = df.drop(\"THE TRAVEL AGENCY IN THE PARK\")\n",
    "df = df.drop(\"loan_advances\", axis=1)\n",
    "#letting the imputer deal with missing data\n",
    "#df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Create New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['deferred_ratio'] = df['deferred_income']/(df['total_payments'] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sklearn prep: features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_list = list(df.columns)\n",
    "features_list.remove('poi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Try a variety of classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I am going to try the GridSearchCV tutorial with the data in the machine learning class. \n",
    "\n",
    "From page 1, I start with the data in a pandas data frame. \n",
    "\n",
    "We don't have any categorical variables besides the dependent variable so I will not have to use the sklearn.preprocessing.LabelEncoder() or the pandas.get_dummies() functions. \n",
    "\n",
    "The data is in a pandas data frame and has not been split into training or testing data sets. So what we have to do is create the features and labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = df['poi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = df[features_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    cross_validation.train_test_split(features, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of training: 100\n",
      "length of test: 44\n"
     ]
    }
   ],
   "source": [
    "print \"length of training: %r\\nlength of test: %r\" % (len(features_train), len(features_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bonus</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>expenses</th>\n",
       "      <th>from_messages</th>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>other</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>salary</th>\n",
       "      <th>shared_receipt_with_poi</th>\n",
       "      <th>to_messages</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>total_stock_value</th>\n",
       "      <th>deferred_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>25.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1264800.000000</td>\n",
       "      <td>662323.285714</td>\n",
       "      <td>-522176.909091</td>\n",
       "      <td>98500.666667</td>\n",
       "      <td>3126352.275862</td>\n",
       "      <td>49334.000000</td>\n",
       "      <td>638.433333</td>\n",
       "      <td>61.966667</td>\n",
       "      <td>60.933333</td>\n",
       "      <td>556129.000000</td>\n",
       "      <td>232793.448276</td>\n",
       "      <td>748024.314286</td>\n",
       "      <td>2490040.333333</td>\n",
       "      <td>247592.103448</td>\n",
       "      <td>1199.400000</td>\n",
       "      <td>2363.300000</td>\n",
       "      <td>2014551.631579</td>\n",
       "      <td>3061289.973684</td>\n",
       "      <td>-1.891790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1700498.382828</td>\n",
       "      <td>1031184.388916</td>\n",
       "      <td>885882.506154</td>\n",
       "      <td>52916.611166</td>\n",
       "      <td>5580156.736252</td>\n",
       "      <td>39144.547234</td>\n",
       "      <td>1417.034462</td>\n",
       "      <td>106.853262</td>\n",
       "      <td>143.676824</td>\n",
       "      <td>542692.247151</td>\n",
       "      <td>420959.519439</td>\n",
       "      <td>1087385.799032</td>\n",
       "      <td>6352191.596559</td>\n",
       "      <td>80992.065895</td>\n",
       "      <td>1225.187715</td>\n",
       "      <td>3482.927902</td>\n",
       "      <td>2935916.274954</td>\n",
       "      <td>5082618.262049</td>\n",
       "      <td>5.288708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>70000.000000</td>\n",
       "      <td>10259.000000</td>\n",
       "      <td>-3081055.000000</td>\n",
       "      <td>38346.000000</td>\n",
       "      <td>59539.000000</td>\n",
       "      <td>475.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>69223.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-2604490.000000</td>\n",
       "      <td>-140264.000000</td>\n",
       "      <td>76399.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>136.000000</td>\n",
       "      <td>475.000000</td>\n",
       "      <td>63014.000000</td>\n",
       "      <td>-17.818773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>400000.000000</td>\n",
       "      <td>51930.000000</td>\n",
       "      <td>-590337.500000</td>\n",
       "      <td>78819.000000</td>\n",
       "      <td>1030329.000000</td>\n",
       "      <td>16514.000000</td>\n",
       "      <td>27.500000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>1231.000000</td>\n",
       "      <td>322088.500000</td>\n",
       "      <td>-118159.250000</td>\n",
       "      <td>211844.000000</td>\n",
       "      <td>219.500000</td>\n",
       "      <td>535.500000</td>\n",
       "      <td>450568.750000</td>\n",
       "      <td>698411.500000</td>\n",
       "      <td>-0.597028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>750000.000000</td>\n",
       "      <td>201073.500000</td>\n",
       "      <td>-200000.000000</td>\n",
       "      <td>119292.000000</td>\n",
       "      <td>1635238.000000</td>\n",
       "      <td>38559.000000</td>\n",
       "      <td>40.500000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>369721.000000</td>\n",
       "      <td>2401.000000</td>\n",
       "      <td>441096.000000</td>\n",
       "      <td>-88669.000000</td>\n",
       "      <td>249201.000000</td>\n",
       "      <td>830.500000</td>\n",
       "      <td>1324.000000</td>\n",
       "      <td>1185341.000000</td>\n",
       "      <td>1786678.500000</td>\n",
       "      <td>-0.243617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1100000.000000</td>\n",
       "      <td>743196.750000</td>\n",
       "      <td>-87500.000000</td>\n",
       "      <td>128578.000000</td>\n",
       "      <td>2604490.000000</td>\n",
       "      <td>77978.000000</td>\n",
       "      <td>202.250000</td>\n",
       "      <td>46.250000</td>\n",
       "      <td>45.250000</td>\n",
       "      <td>547586.500000</td>\n",
       "      <td>400572.000000</td>\n",
       "      <td>885438.500000</td>\n",
       "      <td>-75009.750000</td>\n",
       "      <td>288542.000000</td>\n",
       "      <td>1554.750000</td>\n",
       "      <td>2572.750000</td>\n",
       "      <td>2073245.250000</td>\n",
       "      <td>3571813.250000</td>\n",
       "      <td>-0.093823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8000000.000000</td>\n",
       "      <td>3131860.000000</td>\n",
       "      <td>-4167.000000</td>\n",
       "      <td>137864.000000</td>\n",
       "      <td>30766064.000000</td>\n",
       "      <td>137767.000000</td>\n",
       "      <td>6759.000000</td>\n",
       "      <td>528.000000</td>\n",
       "      <td>609.000000</td>\n",
       "      <td>2035380.000000</td>\n",
       "      <td>1573324.000000</td>\n",
       "      <td>4131594.000000</td>\n",
       "      <td>15456290.000000</td>\n",
       "      <td>404338.000000</td>\n",
       "      <td>4527.000000</td>\n",
       "      <td>15149.000000</td>\n",
       "      <td>15456290.000000</td>\n",
       "      <td>30766064.000000</td>\n",
       "      <td>-0.002079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                bonus  deferral_payments  deferred_income  director_fees  \\\n",
       "count       25.000000          14.000000        11.000000       3.000000   \n",
       "mean   1264800.000000      662323.285714   -522176.909091   98500.666667   \n",
       "std    1700498.382828     1031184.388916    885882.506154   52916.611166   \n",
       "min      70000.000000       10259.000000  -3081055.000000   38346.000000   \n",
       "25%     400000.000000       51930.000000   -590337.500000   78819.000000   \n",
       "50%     750000.000000      201073.500000   -200000.000000  119292.000000   \n",
       "75%    1100000.000000      743196.750000    -87500.000000  128578.000000   \n",
       "max    8000000.000000     3131860.000000     -4167.000000  137864.000000   \n",
       "\n",
       "       exercised_stock_options       expenses  from_messages  \\\n",
       "count                29.000000      29.000000      30.000000   \n",
       "mean            3126352.275862   49334.000000     638.433333   \n",
       "std             5580156.736252   39144.547234    1417.034462   \n",
       "min               59539.000000     475.000000      12.000000   \n",
       "25%             1030329.000000   16514.000000      27.500000   \n",
       "50%             1635238.000000   38559.000000      40.500000   \n",
       "75%             2604490.000000   77978.000000     202.250000   \n",
       "max            30766064.000000  137767.000000    6759.000000   \n",
       "\n",
       "       from_poi_to_this_person  from_this_person_to_poi  long_term_incentive  \\\n",
       "count                30.000000                30.000000            19.000000   \n",
       "mean                 61.966667                60.933333        556129.000000   \n",
       "std                 106.853262               143.676824        542692.247151   \n",
       "min                   0.000000                 0.000000         69223.000000   \n",
       "25%                  10.000000                 0.250000        250000.000000   \n",
       "50%                  26.000000                 6.500000        369721.000000   \n",
       "75%                  46.250000                45.250000        547586.500000   \n",
       "max                 528.000000               609.000000       2035380.000000   \n",
       "\n",
       "                other  restricted_stock  restricted_stock_deferred  \\\n",
       "count       29.000000         35.000000                   6.000000   \n",
       "mean    232793.448276     748024.314286             2490040.333333   \n",
       "std     420959.519439    1087385.799032             6352191.596559   \n",
       "min          2.000000   -2604490.000000             -140264.000000   \n",
       "25%       1231.000000     322088.500000             -118159.250000   \n",
       "50%       2401.000000     441096.000000              -88669.000000   \n",
       "75%     400572.000000     885438.500000              -75009.750000   \n",
       "max    1573324.000000    4131594.000000            15456290.000000   \n",
       "\n",
       "              salary  shared_receipt_with_poi   to_messages   total_payments  \\\n",
       "count      29.000000                30.000000     30.000000        38.000000   \n",
       "mean   247592.103448              1199.400000   2363.300000   2014551.631579   \n",
       "std     80992.065895              1225.187715   3482.927902   2935916.274954   \n",
       "min     76399.000000                10.000000    136.000000       475.000000   \n",
       "25%    211844.000000               219.500000    535.500000    450568.750000   \n",
       "50%    249201.000000               830.500000   1324.000000   1185341.000000   \n",
       "75%    288542.000000              1554.750000   2572.750000   2073245.250000   \n",
       "max    404338.000000              4527.000000  15149.000000  15456290.000000   \n",
       "\n",
       "       total_stock_value  deferred_ratio  \n",
       "count          38.000000       11.000000  \n",
       "mean      3061289.973684       -1.891790  \n",
       "std       5082618.262049        5.288708  \n",
       "min         63014.000000      -17.818773  \n",
       "25%        698411.500000       -0.597028  \n",
       "50%       1786678.500000       -0.243617  \n",
       "75%       3571813.250000       -0.093823  \n",
       "max      30766064.000000       -0.002079  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model I will try this with is decision trees since that is what they go through in the tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imp = Imputer(missing_values='NaN', strategy='median')\n",
    "features_train_imp = imp.fit_transform(features_train)\n",
    "features_test_imp = imp.fit_transform(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features_train_imp[0][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features_test_imp[0][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state = 53)\n",
    "clf = clf.fit(features_train_imp, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features_test_imp[0][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall score:\n",
      "0.0\n",
      "\n",
      "\n",
      "Precision score:\n",
      "0.0\n",
      "\n",
      "\n",
      "Decision Tree Accuracy score:\n",
      "0.75\n",
      "\n",
      "Decision Tree Parameters:\n",
      "{'class_weight': None,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': None,\n",
      " 'max_features': None,\n",
      " 'max_leaf_nodes': None,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'presort': False,\n",
      " 'random_state': 53,\n",
      " 'splitter': 'best'}\n"
     ]
    }
   ],
   "source": [
    "labels_pred = clf.predict(features_test_imp)\n",
    "print \"Recall score:\"\n",
    "print recall_score(labels_test, labels_pred)\n",
    "print \"\\n\"\n",
    "\n",
    "print \"Precision score:\"\n",
    "print precision_score(labels_test, labels_pred)\n",
    "print \"\\n\"\n",
    "\n",
    "print \"Decision Tree Accuracy score:\"\n",
    "print clf.score(features_test_imp, labels_test)\n",
    "\n",
    "print \"\\nDecision Tree Parameters:\"\n",
    "pprint(clf.get_params())\n",
    "\n",
    "\n",
    "# precision_recall_fscore_support(labels_test, labels_pred)a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can't believe that we get 0 scores for both precision and recall, so I investigate further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(labels_test)):\n",
    "    if labels_test[i] == 1 and labels_pred[i]:\n",
    "        count += 1\n",
    "        print \"got one!\"\n",
    "print count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And I still can't believe we get nothing so I try one more test. I print out the test set labels and the predicted values for the labels side by side and visually inspect them. Finally, I am satisfied that the decision tree model gets us absolutely nothing. Its high accuracy score is merely an artifact of the very low number of positives making it possible for a model to achieve fairly high accuracy by simply predicting 0 for person of interest most of the time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True: 0.0 Pred: 1.0\n",
      "\n",
      "True: 0.0 Pred: 0.0\n",
      "\n",
      "True: 0.0 Pred: 0.0\n",
      "\n",
      "True: 0.0 Pred: 0.0\n",
      "\n",
      "True: 0.0 Pred: 0.0\n",
      "\n",
      "True: 0.0 Pred: 0.0\n",
      "\n",
      "True: 0.0 Pred: 0.0\n",
      "\n",
      "True: 1.0 Pred: 0.0\n",
      "\n",
      "True: 0.0 Pred: 0.0\n",
      "\n",
      "True: 0.0 Pred: 0.0\n",
      "\n",
      "True: 0.0 Pred: 0.0\n",
      "\n",
      "True: 0.0 Pred: 1.0\n",
      "\n",
      "True: 0.0 Pred: 0.0\n",
      "\n",
      "True: 0.0 Pred: 0.0\n",
      "\n",
      "True: 0.0 Pred: 0.0\n",
      "\n",
      "True: 0.0 Pred: 1.0\n",
      "\n",
      "True: 0.0 Pred: 0.0\n",
      "\n",
      "True: 0.0 Pred: 0.0\n",
      "\n",
      "True: 0.0 Pred: 0.0\n",
      "\n",
      "True: 0.0 Pred: 1.0\n",
      "\n",
      "True: 0.0 Pred: 0.0\n",
      "\n",
      "True: 1.0 Pred: 0.0\n",
      "\n",
      "True: 0.0 Pred: 0.0\n",
      "\n",
      "True: 1.0 Pred: 0.0\n",
      "\n",
      "True: 0.0 Pred: 0.0\n",
      "\n",
      "True: 0.0 Pred: 0.0\n",
      "\n",
      "True: 0.0 Pred: 0.0\n",
      "\n",
      "True: 0.0 Pred: 1.0\n",
      "\n",
      "True: 1.0 Pred: 0.0\n",
      "\n",
      "True: 0.0 Pred: 0.0\n",
      "\n",
      "True: 1.0 Pred: 0.0\n",
      "\n",
      "True: 0.0 Pred: 0.0\n",
      "\n",
      "True: 0.0 Pred: 0.0\n",
      "\n",
      "True: 0.0 Pred: 0.0\n",
      "\n",
      "True: 0.0 Pred: 0.0\n",
      "\n",
      "True: 0.0 Pred: 0.0\n",
      "\n",
      "True: 0.0 Pred: 0.0\n",
      "\n",
      "True: 0.0 Pred: 0.0\n",
      "\n",
      "True: 0.0 Pred: 0.0\n",
      "\n",
      "True: 0.0 Pred: 0.0\n",
      "\n",
      "True: 0.0 Pred: 0.0\n",
      "\n",
      "True: 0.0 Pred: 0.0\n",
      "\n",
      "True: 0.0 Pred: 1.0\n",
      "\n",
      "True: 0.0 Pred: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(labels_test)):\n",
    "    print \"True: %r Pred: %r\\n\" %(labels_test[i], labels_pred[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, unless I find some other way to do make a bunch of train-test splits when I run a model I may not be able to get anywhere just running looking for models with high recall on just the single small data set we have here. I am going to try the grid search with the scoring function set to recall but we are going to have to have to think that the recall score might not give us enough to work with. \n",
    "\n",
    "The cross validation parameter of the grid search may save the day. It may, though I am not sure, make some changes in the train-test split somehow, sampling from within the data set, to give us more leverage. I will have to try it out to see. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search Cross Validation\n",
    "So we make lists of the parameters of the model that we want to search through. Then there are parameters of the GridSearchCV model itself. Then we fit the model, test its predictions and print out the best estimator and its parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters from the parameter grid:\n",
      "{'max_depth': 4, 'min_samples_split': 25}\n",
      "\n",
      "Complete set of parameters for best estimator:\n",
      "{'class_weight': None,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': 4,\n",
      " 'max_features': None,\n",
      " 'max_leaf_nodes': None,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 25,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'presort': False,\n",
      " 'random_state': 53,\n",
      " 'splitter': 'best'}\n",
      "\n",
      "Accuracy score for default decision tree:\n",
      "0.75\n",
      "\n",
      "Accuracy score for the grid search estimator:\n",
      "0.772727272727\n",
      "\n",
      "F1 score for default decision tree: 0.0\n",
      "F1 score for grid search: 0.20000000000000001\n"
     ]
    }
   ],
   "source": [
    "from sklearn import grid_search\n",
    "#create a dictionary with the parameeters we want to search through\n",
    "param_grid = {'min_samples_split': [1, 5, 10, 15, 20, 25, 30],\n",
    "             'max_depth': [4, 5, 6, 7, 8]}\n",
    "\n",
    "#create the grid_search object\n",
    "gridCV_object = grid_search.GridSearchCV(estimator = DecisionTreeClassifier(random_state=53),\n",
    "                                        param_grid = param_grid,\n",
    "                                        cv = 5)\n",
    "\n",
    "#fit to the data\n",
    "gridCV_object.fit(features_train_imp, labels_train)\n",
    "\n",
    "#find the best parameters chosen\n",
    "print \"Best parameters from the parameter grid:\" \n",
    "pprint(gridCV_object.best_params_)\n",
    "\n",
    "#get the best estimator\n",
    "clf_2 = gridCV_object.best_estimator_\n",
    "\n",
    "#get the best parameters from the best estimator\n",
    "print \"\\nComplete set of parameters for best estimator:\"\n",
    "pprint(clf_2.get_params())\n",
    "\n",
    "#compare scores for the default and the grid search decision trees\n",
    "print \"\\nAccuracy score for default decision tree:\"\n",
    "print clf.score(features_test_imp, labels_test)\n",
    "\n",
    "print \"\\nAccuracy score for the grid search estimator:\"\n",
    "print clf_2.score(features_test_imp, labels_test)\n",
    "\n",
    "labels_clf_pred = clf.predict(features_test_imp)\n",
    "\n",
    "labels_clf2_pred = clf_2.predict(features_test_imp)\n",
    "\n",
    "print \"\\nF1 score for default decision tree: %r\\nF1 score for grid search: %r\" %(recall_score(labels_test, labels_clf_pred), recall_score(labels_test, labels_clf2_pred) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, there first time there was a big drop for the grid search estimator. Now, with the loan_advances variable dropped from the beginning, the accuracy score goes up with the grid search. Now the big problem is to figure out how to get the grid search optimize on the recall variable rather than the accuracy score.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n",
      "0.142857142857\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "labels_pred_2 = clf_2.predict(features_test_imp)\n",
    "print recall_score(labels_test, labels_pred_2) \n",
    "print precision_score(labels_test, labels_pred_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so the decision tree classifier is absolutely terrible but it is at least doing something after grid search as opposed to the tree classifier with just the default parameters which was just flat out zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweeking the Decision Tree: StratifiedShuffleSplit\n",
    "This is a model that, I believe, lets you get a bit more milliage out of the grid search by doing splits of the training data to get more robust tests of the data making up for the small size of the data set. \n",
    "\n",
    "I am going to try to find the way to do this with the recall score guiding the seleciton of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters chosen from the grid:\n",
      "{'max_depth': 4, 'min_samples_split': 15}\n",
      "Best estimator:\n",
      "The best parameters for the best estimator:\n",
      "{'class_weight': None,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': 4,\n",
      " 'max_features': None,\n",
      " 'max_leaf_nodes': None,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 15,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'presort': False,\n",
      " 'random_state': 53,\n",
      " 'splitter': 'best'}\n",
      "\n",
      " Recall score for default decision tree:\n",
      "0.0\n",
      "\n",
      "Recall score for best estimator:\n",
      "0.142857142857\n"
     ]
    }
   ],
   "source": [
    "#keep same param_grid for now\n",
    "#create StratefiedShuffleSplit object\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "cross_validator = StratifiedShuffleSplit(y = labels_train, random_state = 0)\n",
    "\n",
    "#create grid_search object\n",
    "gridCV_object = grid_search.GridSearchCV(estimator = DecisionTreeClassifier(random_state=53),\n",
    "                                        param_grid = param_grid,\n",
    "                                        scoring = 'recall',\n",
    "                                        cv = cross_validator)\n",
    "\n",
    "#fit to the data to the new grid search\n",
    "gridCV_object.fit(features_train_imp, labels_train)\n",
    "\n",
    "#what were the parameteres chosen from the grid?\n",
    "print \"Parameters chosen from the grid:\"\n",
    "pprint(gridCV_object.best_params_)\n",
    "\n",
    "#get best estimator\n",
    "print \"Best estimator:\"\n",
    "clf_recall = gridCV_object.best_estimator_\n",
    "\n",
    "#get the best parameters for the best estimator\n",
    "print \"The best parameters for the best estimator:\"\n",
    "pprint(clf_recall.get_params())\n",
    "\n",
    "#check the scores\n",
    "from sklearn.metrics import recall_score\n",
    "print \"\\n Recall score for default decision tree:\"\n",
    "predictions_recall = clf.predict(features_test_imp)\n",
    "print recall_score(predictions_recall, labels_test)\n",
    "\n",
    "#now for the grid search determined parameters:\n",
    "print \"\\nRecall score for best estimator:\"\n",
    "predictions_recall = clf_recall.predict(features_test_imp)\n",
    "print recall_score(predictions_recall, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall\n",
    "Now I am going to do the whole thing for the recall score, still using the decision tree but using a wider set of parameters and using the stratified shuffle split testing method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters chosen from the grid:\n",
      "{'max_depth': 4, 'min_samples_split': 15}\n",
      "Best estimator:\n",
      "The best parameters for the best estimator:\n",
      "{'class_weight': None,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': 4,\n",
      " 'max_features': None,\n",
      " 'max_leaf_nodes': None,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 15,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'presort': False,\n",
      " 'random_state': 53,\n",
      " 'splitter': 'best'}\n",
      "\n",
      "Recall score for default decision tree:\n",
      "0.0\n",
      "\n",
      "Recall score for best estimator:\n",
      "0.142857142857\n"
     ]
    }
   ],
   "source": [
    "#keep same param_grid for now\n",
    "#create StratefiedShuffleSplit object\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "cross_validator = StratifiedShuffleSplit(y = labels_train, random_state = 0)\n",
    "\n",
    "param_grid = {'min_samples_split': [1, 2, 3, 4, 5, 6, 10, 15, 20, 25, 30],\n",
    "             'max_depth': [4, 5, 6, 7, 8]}\n",
    "\n",
    "#create grid_search object\n",
    "gridCV_object = grid_search.GridSearchCV(estimator = DecisionTreeClassifier(random_state=53),\n",
    "                                        param_grid = param_grid,\n",
    "                                        scoring = 'recall',\n",
    "                                        cv = cross_validator)\n",
    "\n",
    "#fit to the data to the new grid search\n",
    "gridCV_object.fit(features_train_imp, labels_train)\n",
    "\n",
    "#what were the parameteres chosen from the grid?\n",
    "print \"Parameters chosen from the grid:\"\n",
    "pprint(gridCV_object.best_params_)\n",
    "\n",
    "#get best estimator\n",
    "print \"Best estimator:\"\n",
    "clf_recall = gridCV_object.best_estimator_\n",
    "\n",
    "#get the best parameters for the best estimator\n",
    "print \"The best parameters for the best estimator:\"\n",
    "pprint(clf_recall.get_params())\n",
    "\n",
    "#check the scores\n",
    "from sklearn.metrics import recall_score\n",
    "print \"\\nRecall score for default decision tree:\"\n",
    "predictions_1 = clf.predict(features_test_imp)\n",
    "print recall_score(predictions_1, labels_test)\n",
    "\n",
    "#now for the grid search determined parameters:\n",
    "print \"\\nRecall score for best estimator:\"\n",
    "predictions_recall = clf_recall.predict(features_test_imp)\n",
    "print recall_score(predictions_recall, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines\n",
    "The next step is to use all of this wonderful grid searching capability on the Gaussian Naive Bayes model. That will get me the best parameters.\n",
    "\n",
    "First, I look up what the possible parameters are for the GaussianNB. Of course, this will also require that I normalize the data and do the Principle Components analysis on the data before it goes to the model so I will also have to do some data preparation and build a pipeline. \n",
    "\n",
    "The Gaussian Naive Bayes model takes the parameters of--ok, well it doesn't really have any parameters to take. It just wants the normalized data and the principle components if you are doing that. So, we will skip the Gaussian for now and, following the tutorial, add in the pipeline for the decision tree classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Accuracy Score:\n",
      "0.840909090909\n",
      "\n",
      "Pipeline parameters:\n",
      "{'clf': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=53, splitter='best'),\n",
      " 'clf__class_weight': None,\n",
      " 'clf__criterion': 'gini',\n",
      " 'clf__max_depth': None,\n",
      " 'clf__max_features': None,\n",
      " 'clf__max_leaf_nodes': None,\n",
      " 'clf__min_samples_leaf': 1,\n",
      " 'clf__min_samples_split': 2,\n",
      " 'clf__min_weight_fraction_leaf': 0.0,\n",
      " 'clf__presort': False,\n",
      " 'clf__random_state': 53,\n",
      " 'clf__splitter': 'best',\n",
      " 'imp': Imputer(axis=0, copy=True, missing_values='NaN', strategy='most_frequent',\n",
      "    verbose=0),\n",
      " 'imp__axis': 0,\n",
      " 'imp__copy': True,\n",
      " 'imp__missing_values': 'NaN',\n",
      " 'imp__strategy': 'most_frequent',\n",
      " 'imp__verbose': 0,\n",
      " 'pca': PCA(copy=True, n_components=None, whiten=False),\n",
      " 'pca__copy': True,\n",
      " 'pca__n_components': None,\n",
      " 'pca__whiten': False,\n",
      " 'std': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
      " 'std__copy': True,\n",
      " 'std__with_mean': True,\n",
      " 'std__with_std': True,\n",
      " 'steps': [('imp',\n",
      "            Imputer(axis=0, copy=True, missing_values='NaN', strategy='most_frequent',\n",
      "    verbose=0)),\n",
      "           ('std', StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
      "           ('pca', PCA(copy=True, n_components=None, whiten=False)),\n",
      "           ('clf',\n",
      "            DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=53, splitter='best'))]}\n",
      "\n",
      "\n",
      "Recall Score: 0.4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# imp = Imputer(missing_values='NaN', strategy='0')\n",
    "# features_train = imp.fit_transform(features_train)\n",
    "# features_test = imp.fit_transform(features_test)\n",
    "\n",
    "Pipeline = Pipeline([\n",
    "        ('imp', Imputer(missing_values='NaN', strategy='most_frequent')),\n",
    "        ('std', StandardScaler()),\n",
    "        ('pca', PCA()),\n",
    "        ('clf', DecisionTreeClassifier(random_state = 53))\n",
    "    ])\n",
    "\n",
    "pipeline = Pipeline.fit(features_train, labels_train)\n",
    "\n",
    "print \"Pipeline Accuracy Score:\"\n",
    "print pipeline.score(features_test, labels_test)\n",
    "\n",
    "print \"\\nPipeline parameters:\"\n",
    "pprint(pipeline.get_params())\n",
    "\n",
    "clf_pipeline_pred = pipeline.predict(features_test)\n",
    "print \"\\n\\nRecall Score:\", recall_score(labels_test, clf_pipeline_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to run the grid search with the pipeline put in as the estimator. The things we want to change in the pipeline will be put in through the grid search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=8, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='most_frequent',\n",
       "    verbose=0)), ('std', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=None, whiten=False)), ('clf', DecisionTreeClassifier(class_weight=None, criterion='gini', ...plit=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=53, splitter='best'))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'pca__n_components': [2, 3, 4, 5, 6, 7], 'clf__max_depth': [4, 5, 6, 7, 8], 'clf__min_samples_split': [1, 2, 3, 4, 5, 6, 10, 15, 20, 25, 30], 'imp__strategy': ['median', 'most_frequent']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'imp__strategy': ['median', 'most_frequent'], \n",
    "              'clf__min_samples_split': [1, 2, 3, 4, 5, 6, 10, 15, 20, 25, 30],\n",
    "              'clf__max_depth': [4, 5, 6, 7, 8],\n",
    "              'pca__n_components': [2,3,4,5,6,7]}\n",
    "\n",
    "#create the GridSearchCV object using param_grid\n",
    "gridCV_object = grid_search.GridSearchCV(estimator = pipeline, \n",
    "                                        param_grid = param_grid,\n",
    "                                        cv = 8)\n",
    "\n",
    "gridCV_object.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters from the grid search:{'clf__max_depth': 4,\n",
      " 'clf__min_samples_split': 10,\n",
      " 'imp__strategy': 'median',\n",
      " 'pca__n_components': 7}\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "print \"Best parameters from the grid search:\", pprint(gridCV_object.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_gridCV = gridCV_object.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best estimator accuracy: 0.863636363636\n"
     ]
    }
   ],
   "source": [
    "print \"\\nBest estimator accuracy:\", clf_gridCV.score(features_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Recall Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "clf_gridCV_pred = clf_gridCV.predict(features_test)\n",
    "print \"\\n\\nRecall Score:\", recall_score(labels_test, clf_gridCV_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why didn't the recall score go up? What did I do wrong? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can switch out the tree classifier for the random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=8, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('std', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=None, whiten=False)), ('clf', RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "         ...stimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=53, verbose=0, warm_start=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'pca__n_components': [2, 3, 4, 5, 6, 7], 'clf__max_depth': [4, 5, 6, 7, 8], 'imp__strategy': ['median', 'most_frequent']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        ('imp', Imputer()),\n",
    "        ('std', StandardScaler()),\n",
    "        ('pca', PCA()),\n",
    "        ('clf', RandomForestClassifier(random_state=53, class_weight = 'balanced'))\n",
    "    ])\n",
    "\n",
    "param_grid = {'imp__strategy':['median', 'most_frequent'],\n",
    "              'clf__max_depth': [4,5,6,7,8],\n",
    "              'pca__n_components': [2,3,4,5,6,7]}\n",
    "\n",
    "gridCV_object = grid_search.GridSearchCV(estimator = pipeline, \n",
    "                                        param_grid = param_grid,\n",
    "                                        cv = 8)\n",
    "\n",
    "gridCV_object.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look at the preformance of this model. Now that we have fit the model we look at how well it does. First we print out the parameters of the model that was chosen by grid search as the best one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters from the grid search:{'clf__max_depth': 6, 'imp__strategy': 'median', 'pca__n_components': 2}\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "print \"Best parameters from the grid search:\", pprint(gridCV_object.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we have the model that is best. It goes to a depth of 6 in the decision tree, it uses the median to replace missing values and it uses two principle components. \n",
    "\n",
    "Now, we look at how well it does in recall, precision and the f1 score. To do that we have to make that best model into an object, use that object to make predictions, and then use those predictions to get the scores of the model on the recall, precision and f1 measures. \n",
    "\n",
    "Make the model an object with a name based on what it is, a classifier using random forest and grid search, so clf_rf_gridCV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_rf_gridCV = gridCV_object.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to use that model to get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_rf_pred = clf_rf_gridCV.predict(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the predictions we use them to get recall score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "Recall Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "print \"\\n\\nRecall Score:\", recall_score(labels_test, y_rf_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters chosen from the grid:\n",
      "{'max_depth': 4, 'min_samples_split': 15}\n",
      "Best estimator:\n",
      "The best parameters for the best estimator:\n",
      "{'class_weight': None,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': 4,\n",
      " 'max_features': None,\n",
      " 'max_leaf_nodes': None,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 15,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'presort': False,\n",
      " 'random_state': 53,\n",
      " 'splitter': 'best'}\n",
      "\n",
      "Recall score for default decision tree:\n",
      "0.0\n",
      "\n",
      "Recall score for best estimator:\n",
      "0.142857142857\n"
     ]
    }
   ],
   "source": [
    "#keep same param_grid for now\n",
    "#create StratefiedShuffleSplit object\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "cross_validator = StratifiedShuffleSplit(y = labels_train, random_state = 0)\n",
    "\n",
    "param_grid = {'min_samples_split': [1, 2, 3, 4, 5, 6, 10, 15, 20, 25, 30],\n",
    "             'max_depth': [4, 5, 6, 7, 8]}\n",
    "\n",
    "#create grid_search object\n",
    "gridCV_object = grid_search.GridSearchCV(estimator = DecisionTreeClassifier(random_state=53),\n",
    "                                        param_grid = param_grid,\n",
    "                                        scoring = 'recall',\n",
    "                                        cv = cross_validator)\n",
    "\n",
    "#fit to the data to the new grid search\n",
    "gridCV_object.fit(features_train_imp, labels_train)\n",
    "\n",
    "#what were the parameteres chosen from the grid?\n",
    "print \"Parameters chosen from the grid:\"\n",
    "pprint(gridCV_object.best_params_)\n",
    "\n",
    "#get best estimator\n",
    "print \"Best estimator:\"\n",
    "clf_recall = gridCV_object.best_estimator_\n",
    "\n",
    "#get the best parameters for the best estimator\n",
    "print \"The best parameters for the best estimator:\"\n",
    "pprint(clf_recall.get_params())\n",
    "\n",
    "#check the scores\n",
    "from sklearn.metrics import recall_score\n",
    "print \"\\nRecall score for default decision tree:\"\n",
    "predictions_1 = clf.predict(features_test_imp)\n",
    "print recall_score(predictions_1, labels_test)\n",
    "\n",
    "#now for the grid search determined parameters:\n",
    "print \"\\nRecall score for best estimator:\"\n",
    "predictions_recall = clf_recall.predict(features_test_imp)\n",
    "print recall_score(predictions_recall, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Tune precision and recall > 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardize by subtracting mean and dividing by std_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "# Create a minimum and maximum processor object\n",
    "standard_scaler = preprocessing.StandardScaler()\n",
    "\n",
    "# Create an object to transform the data to fit minmax processor\n",
    "features_train = standard_scaler.fit_transform(features_train)\n",
    "features_test = standard_scaler.fit_transform(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I am going to use the select k best method on the data and then see how it does with the models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "COACH QUESTION: Where do I do the k-best thing? Can it be added to a pipeline below? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB()\n",
      "\tAccuracy: 0.61633\tPrecision: 0.21514\tRecall: 0.70900\tF1: 0.33011\tF2: 0.48592\n",
      "\tTotal predictions: 15000\tTrue positives: 1418\tFalse positives: 5173\tFalse negatives:  582\tTrue negatives: 7827\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "test_classifier(clf, my_dataset, features_list, folds = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COACH QUESTION: Why doesn't the rbf kernel work? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got a divide by zero when trying out: SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Precision or recall may be undefined due to a lack of true positive predicitons.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(kernel='rbf',C=100) \n",
    "# tried gamma=0.001, C=100.\n",
    "# kernel = 'linear', kernel='rbf'\n",
    "test_classifier(clf, my_dataset, features_list, folds=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got a divide by zero when trying out: Pipeline(steps=[('reduce_dim', PCA(copy=True, n_components=7, whiten=False)), ('svm', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "Precision or recall may be undefined due to a lack of true positive predicitons.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import PCA\n",
    "estimators = [('reduce_dim', PCA(n_components=7)), ('svm', svm.SVC(kernel='rbf'))]\n",
    "clf = Pipeline(estimators)\n",
    "test_classifier(clf, my_dataset, features_list, folds = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Winner\n",
    "COACH QUESTION: So the Guassian Naive Bayes with the dimensions of the data set reduced to 8 through prinicple component analysis is the one that I got to break the 0.3 thresholds in Recall and Precision. I did it by just doing a manual search through the possible number of components. I have no idea which components were chosen and on what basis they worked, which 8 worked better than 9 or why 7 didn't make the 0.3 cut at all. I have no intuition for why any of this worked. And I don't know how to visualize it. Can I just submit this for the final project? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('reduce_dim', PCA(copy=True, n_components=8, whiten=False)), ('Gaussian', GaussianNB())])\n",
      "\tAccuracy: 0.85053\tPrecision: 0.42568\tRecall: 0.34650\tF1: 0.38203\tF2: 0.35989\n",
      "\tTotal predictions: 15000\tTrue positives:  693\tFalse positives:  935\tFalse negatives: 1307\tTrue negatives: 12065\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.decomposition import PCA\n",
    "estimators = [('reduce_dim', PCA(n_components=8)), ('Gaussian', GaussianNB())]\n",
    "clf = Pipeline(estimators)\n",
    "test_classifier(clf, my_dataset, features_list, folds = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
