{
 "metadata": {
  "name": "",
  "signature": "sha256:a5bd1bc0abab3dbb97ed55ca639476df8d1c376fb527f9dbfee685ee4efe9b00"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "0.009/0.108"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 35,
       "text": [
        "0.08333333333333333"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "0.099/0.108"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 36,
       "text": [
        "0.9166666666666667"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "0.1*0.8*0.5"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 37,
       "text": [
        "0.04000000000000001"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "0.3*0.2*0.5"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 38,
       "text": [
        "0.03"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "0.04/0.07"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 39,
       "text": [
        "0.5714285714285714"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "0.03/0.07"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 40,
       "text": [
        "0.4285714285714285"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "0.5*0.1*0.8"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 41,
       "text": [
        "0.04000000000000001"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "0.5*0.5*0.2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 42,
       "text": [
        "0.05"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a, b = 0.04/0.09, 0.05/0.09"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 44,
       "text": [
        "0.4444444444444445"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "b"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 45,
       "text": [
        "0.5555555555555556"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "from time import time\n",
      "sys.path.append(\"../tools/\")\n",
      "#from email_preprocess import preprocess"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%ls"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "svm_author_id.py\r\n"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%cd ud120-projects/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[Errno 2] No such file or directory: 'ud120-projects/'\n",
        "/Users/michaelreinhard/nano/machineLearning/ud120-projects/svm\n"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pwd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 53,
       "text": [
        "u'/Users/michaelreinhard/nano/machineLearning/ud120-projects/svm'"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%cd '../ud120-projects/'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[Errno 2] No such file or directory: '../ud120-projects/'\n",
        "/Users/michaelreinhard/nano/machineLearning/ud120-projects/svm\n"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%ls"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "svm_author_id.py\r\n"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%cd tools"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[Errno 2] No such file or directory: 'tools'\n",
        "/Users/michaelreinhard/nano/machineLearning/ud120-projects/svm\n"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%ls"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "svm_author_id.py\r\n"
       ]
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "from time import time\n",
      "sys.path.append(\"../tools/\")\n",
      "from email_preprocess import preprocess"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "features_train, features_test, labels_train, labels_test = preprocess()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "no. of Chris training emails: 7936\n",
        "no. of Sara training emails: 7884\n"
       ]
      }
     ],
     "prompt_number": 59
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now I just run the training algorithm for Naive Bayes just like they did in the lesson. \n",
      "\n",
      "The first step is to import the packages I need. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import naive_bayes"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 61
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "features_train[:10] # the sparseness of language"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 62,
       "text": [
        "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
        "         0.        ,  0.        ],\n",
        "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
        "         0.        ,  0.        ],\n",
        "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
        "         0.        ,  0.        ],\n",
        "       ..., \n",
        "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
        "         0.        ,  0.        ],\n",
        "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
        "         0.        ,  0.        ],\n",
        "       [ 0.        ,  0.        ,  0.        , ...,  0.20846995,\n",
        "         0.        ,  0.        ]])"
       ]
      }
     ],
     "prompt_number": 62
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = naive_bayes.GaussianNB()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf.fit(features_train, labels_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 64,
       "text": [
        "GaussianNB()"
       ]
      }
     ],
     "prompt_number": 64
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pred = clf.predict(features_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pred[:100]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 84,
       "text": [
        "array([0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,\n",
        "       0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,\n",
        "       1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,\n",
        "       0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,\n",
        "       1, 1, 1, 1, 0, 0, 0, 0])"
       ]
      }
     ],
     "prompt_number": 84
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sum(pred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 85,
       "text": [
        "903"
       ]
      }
     ],
     "prompt_number": 85
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sum(labels_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 86,
       "text": [
        "865"
       ]
      }
     ],
     "prompt_number": 86
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sum_pred_labels = pred + labels_test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 87
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sum_pred_labels"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 88,
       "text": [
        "array([0, 0, 2, ..., 2, 0, 0])"
       ]
      }
     ],
     "prompt_number": 88
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "accuracy_count = 0\n",
      "for i in sum_pred_labels:\n",
      "    if i == 0 | i == 2:\n",
      "        accuracy_count += 1\n",
      "print accuracy_count"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "861\n"
       ]
      }
     ],
     "prompt_number": 90
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "861/len(labels_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 92,
       "text": [
        "0.48976109215017066"
       ]
      }
     ],
     "prompt_number": 92
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I really don't know why that didn't work. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pred_by_test = pred*labels_test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 93
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sum(pred_by_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 94,
       "text": [
        "861"
       ]
      }
     ],
     "prompt_number": 94
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(labels_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 68,
       "text": [
        "1758"
       ]
      }
     ],
     "prompt_number": 68
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 69
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sum(pred_by_test)/len(labels_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 70,
       "text": [
        "0.48976109215017066"
       ]
      }
     ],
     "prompt_number": 70
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "correct = sum(pred_by_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 71
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "total = len(labels_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 72
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "correct/total"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 76,
       "text": [
        "0.48976109215017066"
       ]
      }
     ],
     "prompt_number": 76
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, what I tried to do in the code above was compute the success rate by myself by multiplying the predictions by the actual values and dividing by the number of cases. This wildly underestimated the accuracy of the model. When the model predicts 1 and the actual data is one it produces the correct answer, 1. When the model and the actual data disagree, with one producing a zero and the other producing a 1, the outcome is 0, which is also correct. But when they both produce zero, the outcome is 0, which is wrong. That is counting the outcome as wrong when it should be counted as right. That is why there is such a huge underestimate. \n",
      "\n",
      "What if I added the two together and then looped through, counting 2 or 0 as correct (the cases where both lists were 1 or both were 0) and cases where the value was 1 as wrong (the cases where one value was 1 and the other 0). I'll try that later."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sum(labels_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 81,
       "text": [
        "865"
       ]
      }
     ],
     "prompt_number": 81
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "861.0/865.0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 82,
       "text": [
        "0.9953757225433526"
       ]
      }
     ],
     "prompt_number": 82
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import accuracy_score\n",
      "accuracy = accuracy_score(pred, labels_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 95
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "accuracy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 96,
       "text": [
        "0.97383390216154719"
       ]
      }
     ],
     "prompt_number": 96
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Testing the amount of time the model takes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t0 = time()\n",
      "clf.fit(features_train, labels_train)\n",
      "print \"Training time is: \", round(time()-t0, 3), \"s\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training time is:  1.203 s\n"
       ]
      }
     ],
     "prompt_number": 97
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t0 = time()\n",
      "clf.predict(features_test)\n",
      "print \"Training time is: \", round(time()-t0, 3), \"s\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training time is:  0.248 s\n"
       ]
      }
     ],
     "prompt_number": 98
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Support Vector Machines**\n",
      "Lesson 2"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First navigate to, or confirm that we are in, the 'svm' folder."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%cd svm"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/Users/michaelreinhard/nano/machineLearning/ud120-projects/svm\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%ls"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "svm_author_id.py\r\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%run svm_author_id.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "no. of Chris training emails: 7936\n",
        "no. of Sara training emails: 7884\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I don't remember but I think we were supposed to run the svm_author_id.py file, but I was not able to do that at first, so I simply ended up concatenating the file to stdout and pasting the lines in from there. It turns out that the commands are either %run or execfile('file_name'). Below I do it all three ways just to have a record. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%run svm_author_id.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "no. of Chris training emails: 7936\n",
        "no. of Sara training emails: 7884\n"
       ]
      }
     ],
     "prompt_number": 116
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "execfile(\"svm_author_id.py\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "no. of Chris training emails: 7936\n",
        "no. of Sara training emails: 7884\n"
       ]
      }
     ],
     "prompt_number": 119
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%cat svm_author_id.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "#!/usr/bin/python\r\n",
        "\r\n",
        "\"\"\" \r\n",
        "    This is the code to accompany the Lesson 2 (SVM) mini-project.\r\n",
        "\r\n",
        "    Use a SVM to identify emails from the Enron corpus by their authors:    \r\n",
        "    Sara has label 0\r\n",
        "    Chris has label 1\r\n",
        "\"\"\"\r\n",
        "    \r\n",
        "import sys\r\n",
        "from time import time\r\n",
        "sys.path.append(\"../tools/\")\r\n",
        "from email_preprocess import preprocess\r\n",
        "\r\n",
        "\r\n",
        "### features_train and features_test are the features for the training\r\n",
        "### and testing datasets, respectively\r\n",
        "### labels_train and labels_test are the corresponding item labels\r\n",
        "features_train, features_test, labels_train, labels_test = preprocess()\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#########################################################\r\n",
        "### your code goes here ###\r\n",
        "\r\n",
        "#########################################################\r\n",
        "\r\n",
        "\r\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "from time import time\n",
      "sys.path.append(\"../tools/\")\n",
      "from email_preprocess import preprocess"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "features_train, features_test, labels_train, labels_test = preprocess()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "no. of Chris training emails: 7936\n",
        "no. of Sara training emails: 7884\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we create the 'learning machine' or 'the classifier'. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import svm\n",
      "lin_clf = svm.LinearSVC()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then we fit the model. Since we have been getting the error \"a float is required\" we will convert the values in labels train to floating point numbers. First I will demonstrate that the fit function does not work with the variables as come out of the preprocess() function. Then I will show that converting the values in labels_train does not solve the problem. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lin_clf.fit(features_train, labels_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
        "     intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',\n",
        "     random_state=None, tol=0.0001, verbose=0)"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pred = lin_clf.predict(features_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import accuracy_score\n",
      "accuracy = accuracy_score(pred, labels_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "accuracy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "0.98976109215017061"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This turns out to be wrong. It is not the accuracy we are looking for. Reviewing the intro to the project I see that we are to set the kernel for the svm to linear, which suggests that my original approach of using the word 'linear' as an argument to the svm classifier may have been closer to the correct approach, though that led to all the problems with requiring a float. Let me try it one more time using the approach suggested in the documentation, this time actually stipulating the name of the argument, 'kernel'. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "linear_svc = svm.SVC(kernel='linear')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now I fit the model to the training data set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "linear_svc.fit(features_train, labels_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,\n",
        "  kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
        "  shrinking=True, tol=0.001, verbose=False)"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we generate the predictions. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pred = linear_svc.predict(features_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 187
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The predictions took a few seconds but it was no inconvenience. \n",
      "\n",
      "Now we use the pre-packed function from sklearn to test the accuracy of the model. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import accuracy_score\n",
      "accuracy = accuracy_score(pred, labels_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 188
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "accuracy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 189,
       "text": [
        "0.98407281001137659"
       ]
      }
     ],
     "prompt_number": 189
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Which turns out to be correct. \n",
      "\n",
      "Now we look at how long it takes to run the model and compare it to the speed of the naive bayes model. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t0 = time()\n",
      "linear_svc.fit(features_train, labels_train)\n",
      "print \"Training time is: \", round(time()-t0, 3), \"s\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training time is:  223.446 s\n"
       ]
      }
     ],
     "prompt_number": 190
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t0 = time()\n",
      "linear_svc.predict(features_test)\n",
      "print \"Training time is: \", round(time()-t0, 3), \"s\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training time is:  22.975 s\n"
       ]
      }
     ],
     "prompt_number": 191
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we run the model with 1% of the data and see how much faster it runs and what the accuracy is. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "features_train = features_train[:len(features_train)/100] \n",
      "labels_train = labels_train[:len(labels_train)/100] \n",
      "\n",
      "linear_svc = svm.SVC(kernel='linear')\n",
      "linear_svc.fit(features_train, labels_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,\n",
        "  kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
        "  shrinking=True, tol=0.001, verbose=False)"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pred = linear_svc.predict(features_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import accuracy_score\n",
      "accuracy = accuracy_score(pred, labels_test)\n",
      "print accuracy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.884527872582\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we try the svm with the more complicated kernel 'rbf' while keeping the data set small and then report the accuracy. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "linear_svc = svm.SVC(kernel='rbf', C=10000.0)\n",
      "linear_svc.fit(features_train, labels_train)\n",
      "pred = linear_svc.predict(features_test)\n",
      "accuracy = accuracy_score(pred, labels_test)\n",
      "print accuracy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.892491467577\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we see if the rbf model still works well on the full data set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "features_train, features_test, labels_train, labels_test = preprocess()\n",
      "\n",
      "linear_svc = svm.SVC(kernel='rbf', C=10000.0)\n",
      "linear_svc.fit(features_train, labels_train)\n",
      "pred = linear_svc.predict(features_test)\n",
      "accuracy = accuracy_score(pred, labels_test)\n",
      "print accuracy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "no. of Chris training emails: 7936\n",
        "no. of Sara training emails: 7884\n",
        "0.990898748578"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we are asked to get predictions for the cases \n",
      "10\n",
      "26 \n",
      "50\n",
      "Using the reduced size training set. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# features_train = features_train[:len(features_train)/100] \n",
      "# labels_train = labels_train[:len(labels_train)/100] \n",
      "features_train, features_test, labels_train, labels_test = preprocess()\n",
      "\n",
      "\n",
      "linear_svc = svm.SVC(kernel='rbf', C=10000.0)\n",
      "linear_svc.fit(features_train, labels_train)\n",
      "pred = linear_svc.predict(features_test)\n",
      "# p_10 = pred[11]\n",
      "# p_26 = pred[27]\n",
      "# p_50 = pred[51]\n",
      "# print pred_10, pred_26, pred_50"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "no. of Chris training emails: 7936\n",
        "no. of Sara training emails: 7884\n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "p_10 = pred[10]\n",
      "p_26 = pred[26]\n",
      "p_50 = pred[50]\n",
      "print pred_10, pred_26, pred_50"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0] [1] [1]\n"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sum(pred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 44,
       "text": [
        "877"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Lesson 3: Decision Trees**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import tree\n",
      "clf = tree.DecisionTreeClassifier()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We are asked to calculate the entropy of a node. The formula for entropy is: $$\\Sigma {p_ilog_2(p_i)}$$ where p is the proportion of cases with a given label in a particular class. \n",
      "\n",
      "And we have 2 out of 4 of the labels in a given class, so $p_i = 0.5$. Now all we have to do is multiply 0.5 times the $log_2$ of 0.5."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import math\n",
      "\n",
      "0.5*math.log(0.5,2) + 0.5*math.log(0.5,2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "-1.0"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "But the entropy should be bound between 0 and 1? The correct answer is 1, but why? Turns out I had the forumla wrong. It is $-p_i$ for the first term, $$\\Sigma {-p_ilog_2(p_i)}$$ So the calculation is: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "-0.5*math.log(0.5,2) + -0.5*math.log(0.5,2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "1.0"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we have to do a specific problem for quiz 5: Information Gain Calculation. Here is the problem: $$-\\frac{2}{3}\\log_2(\\frac{2}{3})-\\frac{1}{3}\\log_2(\\frac{1}{3})$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "-(0.67)*math.log(0.67, 2)-(0.33)*math.log(0.33, 2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "0.9149263727797274"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we have to calculate the entropy in the child branches. Since one of the child categories has only one observation in it the entropy of that branch is 0 times the weight of that branch, 1/4. So that branch is 0. \n",
      "\n",
      "The other branch has 3/4 of the cases, so it is 3/4*0.9149263727797274. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "(3.0/4.0)*(-(0.67)*math.log(0.67, 2)-(0.33)*math.log(0.33, 2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "0.6861947795847956"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, what is the information gain? The information gain is the entropy we started with minus the entropy we ended up with. The parent has an entropy of 1.0, recall, because there was maximum confusion. So it is 1 minus the entropy of the child branches, 0.6861947795847956, or: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "1 - 0.6861947795847956"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "0.3138052204152044"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So if we split on the 'grade' (steep versus flat) we gain 0.314 in information. \n",
      "\n",
      "Now we want to calculate gain from splitting on the bumpiness. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "-0.5*math.log(0.5,2)-(0.5)*math.log(0.5,2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "1.0"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      ".5*1+0.5*1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "1.0"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So what is the entropy of the bumpy train, where we have one two cases and two labels represented? $p_i$ is 0.5 for both labels so, "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "-0.5*math.log(0.5,2)-0.5*math.log(0.5,2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "1.0"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we have to evaluate the information gain on splitting on speed limit. There are two values to speed limit and the two values line up perfectly with the two possible values for the labels. So, all the cases that have 1 on the speed limit (a speed limit exists) have the same value, slow, on speed, and all the cases that have 0 on the speed limit (there is no speed limit) have the same value, fast, on the label. So it seems like knowing whether there is a speed limit tells us a lot. In fact, in this data set, knowing the value of speed limit fully determines the value of interest, the label, for speed: slow or fast. \n",
      "\n",
      "So, intuitively, there should be a 100% information gain. \n",
      "\n",
      "(By the way, this seems a lot like the idea of degrees of freedom. Is there some formal relationship?)\n",
      "\n",
      "So, to test this intuition, I will go through the entropy calculations and subtract the end state entropy from the begining state entropy. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "-(1)*math.log(1,2)-(1)*math.log(1,2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "-0.0"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Going to the sklearn documentation we see that the spliting function that the algorithm defaults to is something called the 'gini' function. You can change the criteria with the argument criterion='gini'. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Bias-Variance Trade-off**\n",
      "\n",
      "Bias is the degree to which a model is insensitive to the data. Variance, the opposing quality, is the degree to which a model is sensitive to the data. A model with too much bias does not take into account local conditions or new data enough. It persists in making decisions that may be appropriate to the environment it is optimized to deal with, but which are inefficient under current conditions. The problem with the high variance model is that it over-reacts to the data and so, when it sees new data, has \"nothing to default to\". \n",
      "\n",
      "Sebastian's explanation of what goes wrong when a model has too much variance is not entirely clear to me, but it seems that over-reacting to the data could mean changing your decisions too much, going from one extreme to another, or finding yourself unable to make a decision at all. A high variance model can only deal with situations it has seen before, otherwise it has no default to go to. The high bias model is one that is stuck on its default no matter what data it encounters.   \n",
      "\n",
      "We want a model that can generalize but does not over react to the data. \n",
      "\n",
      "I think I will write something about decision making and Churchill. His discussion of decision making in the war cabinet and particularly his objections to having 'unharnessed' ministers with nothing to do but engage in that 'exalted brooding' characteristic of committees made up of those with no operational responsibilities, seems to be a good illustration of what the bias-variance trade-off is getting at. \n",
      "\n",
      "Big problem with decision trees is that they are prone to over fitting, which I suppose is an example of having too much variance and not enough bias. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*Mini-Project: Decision Trees*"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%cd /Users/michaelreinhard/nano/machineLearning/ud120-projects/decision_tree"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/Users/michaelreinhard/nano/machineLearning/ud120-projects/decision_tree\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*Update*\n",
      "\n",
      "After going back and looking at the course files I found the directions to the udacity repositor for this course and the directions for cloning a repository [here](https://git-scm.com/book/en/v2/Git-Basics-Getting-a-Git-Repository). Now the respository is cloned and in my machineLearning directory. I also used the git init command and the git add \\*.ipynb to get make my ipython notebooks part of a git respository, though I am still not entirely sure how this works. \n",
      "\n",
      "Anyway, the code above has been rewritten so that this notebook will always take me to the correct repository. I have also added this notebook to the git repository, michaelreinhard/machineLearning (or at least I think I have) and so the updeated version that skips all the looking around will be the latest version that we will use. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%ls"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "dt_author_id.py\r\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%run dt_author_id.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "no. of Chris training emails: 7936\n",
        "no. of Sara training emails: 7884\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That only took a few seconds to run, so now the data should be ready. We see we have almost 8,000 emails from both parties. \n",
      "\n",
      "Now we need to get the 'tree' module from the sklearn and create the classifier. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import tree\n",
      "\n",
      "clf = tree.DecisionTreeClassifier(min_samples_split=40)\n",
      "clf.fit(features_train, labels_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 23,
       "text": [
        "DecisionTreeClassifier(compute_importances=None, criterion='gini',\n",
        "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
        "            min_density=None, min_samples_leaf=1, min_samples_split=40,\n",
        "            random_state=None, splitter='best')"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pred = clf.predict(features_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import accuracy_score\n",
      "accuracy = accuracy_score(pred, labels_test)\n",
      "print accuracy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.978384527873\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The answer above is different from the answer I got yesterday the first time I ran the model even though both were accepted by the grader. The first time I got 0.977, so we were one off in the third decimal place. Apparently we are allowed to round down to two decimal places. Still, it seems odd that I would get two different answers. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we are asked to get the number of features in the data, or, in other words, the number of columns in the data set. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(features_train[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 35,
       "text": [
        "3785"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we come to the question I could not get to work the first time around. \n",
      "\n",
      "The challenge is to see what is the effect on the accuracy of the model of using less data. Up till now we have been using the frequencies of words from the top ten percent of words occuring in the data set. \n",
      "\n",
      "Now we go into the pre-processing code and change the value of the selector from the /tools/email_preprocess.py file. The function is \n",
      "```\n",
      "selector = SelectPercentile(f_classif, percentile=10)\n",
      "```\n",
      "Once we have done that we will run the dt_author_id.py script, which  prepares the data for the decision tree model and presumably calls the email_process.py script at some point. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%run dt_author_id.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "no. of Chris training emails: 7936\n",
        "no. of Sara training emails: 7884\n"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now I am going to run the features_train[0] to see how many words are in the data set. I am expecting to see some 3 hundreds of words rather than something over 3,000."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "features_train[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 37,
       "text": [
        "array([ 0.,  0.,  0., ...,  0.,  0.,  0.])"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Oh yeah, I want the number of columns. Use len(). "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(features_train[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 38,
       "text": [
        "3785"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, no change. But maybe there is something I am missing. I will run the code just to be sure. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = tree.DecisionTreeClassifier(min_samples_split=40)\n",
      "clf.fit(features_train, labels_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 39,
       "text": [
        "DecisionTreeClassifier(compute_importances=None, criterion='gini',\n",
        "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
        "            min_density=None, min_samples_leaf=1, min_samples_split=40,\n",
        "            random_state=None, splitter='best')"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pred = clf.predict(features_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "accuracy = accuracy_score(pred, labels_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "accuracy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 42,
       "text": [
        "0.97724687144482369"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So there is something a little odd going on. The accuracy changed from 0.978 to 0.977 here. And the 0.977 seems to be, if I recall correctly, what I had gotten the first time I ran the model. This is a slight decrease in accuracy and so would be consistent with the number of features decreasing but we already know that the number of features did not decrease. Is there some element of randomness in the decision tree process? \n",
      "\n",
      "I have deleted a lot of things that I tried to get the number of features reduced and now I am going to try the things suggested on the discussion forum. \n",
      "\n",
      "The first thing I am going to try is the set of steps suggested in response to [this post](https://discussions.udacity.com/t/python-import-question/33305), where a guy named Mark has exactly the same problems I have been having. \n",
      "\n",
      "The first suggestion is to get rid of the email_preprocess.pyc file. The file extension pyc indicates a script processed by Python into bytecode so that it will run faster. By deleting this code the nb_author_id.py script (he is working on mini-project 1 but I assume the same will work for mini-project 3) will be forced to call it rather than use the bytecode file. Another bytecode script will be created by Python for future use."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%ls"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "dt_author_id.py\r\n"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%cd .."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/Users/michaelreinhard/nano/machineLearning/ud120-projects\n"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%ls"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "email_authors.pkl          parse_out_email_text.py    startup.py\r\n",
        "email_preprocess.py        python2_lesson06_keys.pkl  word_data.pkl\r\n",
        "email_preprocess.pyc       python2_lesson13_keys.pkl\r\n",
        "feature_format.py          python2_lesson14_keys.pkl\r\n"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%rm email_preprocess.pyc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%ls"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "email_authors.pkl          parse_out_email_text.py    python2_lesson14_keys.pkl\r\n",
        "email_preprocess.py        python2_lesson06_keys.pkl  startup.py\r\n",
        "feature_format.py          python2_lesson13_keys.pkl  word_data.pkl\r\n"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%cd .."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/Users/michaelreinhard/nano/machineLearning/ud120-projects\n"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%ls"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "README.md           \u001b[34mevaluation\u001b[m\u001b[m/         \u001b[34mnaive_bayes\u001b[m\u001b[m/        \u001b[34msvm\u001b[m\u001b[m/\r\n",
        "\u001b[34mchoose_your_own\u001b[m\u001b[m/    \u001b[34mfeature_selection\u001b[m\u001b[m/  \u001b[34moutliers\u001b[m\u001b[m/           \u001b[34mtext_learning\u001b[m\u001b[m/\r\n",
        "\u001b[34mdatasets_questions\u001b[m\u001b[m/ \u001b[34mfinal_project\u001b[m\u001b[m/      \u001b[34mpca\u001b[m\u001b[m/                \u001b[34mtools\u001b[m\u001b[m/\r\n",
        "\u001b[34mdecision_tree\u001b[m\u001b[m/      \u001b[34mk_means\u001b[m\u001b[m/            \u001b[34mregression\u001b[m\u001b[m/         \u001b[34mvalidation\u001b[m\u001b[m/\r\n"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%cd decision_tree"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/Users/michaelreinhard/nano/machineLearning/ud120-projects/decision_tree\n"
       ]
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%ls"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "dt_author_id.py\r\n"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%run dt_author_id.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "no. of Chris training emails: 7936\n",
        "no. of Sara training emails: 7884\n"
       ]
      }
     ],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(features_train[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 60,
       "text": [
        "3785"
       ]
      }
     ],
     "prompt_number": 60
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, the first suggestion did not work. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(It just occurred to me that I have been editing the python scripts in vim from the command line. There must be a way to do that from within the iPython notebook, no?)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The next suggestion that Sheng makes is that if you are running the commands from the notebook it may require the 'reload(\\___) function'. So, first I will trying running it from the command line. Then, if that does not work, I will try the reload function that Sheng suggests. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ok, so I just ran the dt_author_id.py script from the command line. I will now check and see if it worked. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(features_train[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 63,
       "text": [
        "3785"
       ]
      }
     ],
     "prompt_number": 63
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It does not appear to have worked. I will try to use the reload(\\__)function, though I am not quite sure how to use it from inside the notebook. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%ls"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "dt_author_id.py\r\n"
       ]
      }
     ],
     "prompt_number": 64
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%reload(dt_author_id.py)function"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "ERROR: Line magic function `%reload` not found.\n"
       ]
      }
     ],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%reload('dt_author_id.py')function"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "ERROR: Line magic function `%reload` not found.\n"
       ]
      }
     ],
     "prompt_number": 66
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%reload('dt_author_id.py')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "ERROR: Line magic function `%reload` not found.\n"
       ]
      }
     ],
     "prompt_number": 67
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ok, so I don't quite get how to use the reload function anyway. Maybe it can be done from the command line? And maybe it should be used on the email_preprocess.py file, since that is the file that gets the changes? I am going to try to navigate to the tools folder to see if I can get reload to work there. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%cd /../tools"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[Errno 2] No such file or directory: '/../tools'\n",
        "/Users/michaelreinhard/nano/machineLearning/ud120-projects/decision_tree\n"
       ]
      }
     ],
     "prompt_number": 68
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That should have worked. the .. should have gotten me to ud120-projects and the /tools should have gotten me back down to the tools folder. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%cd .."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/Users/michaelreinhard/nano/machineLearning/ud120-projects\n"
       ]
      }
     ],
     "prompt_number": 69
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%ls"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "README.md           \u001b[34mevaluation\u001b[m\u001b[m/         \u001b[34mnaive_bayes\u001b[m\u001b[m/        \u001b[34msvm\u001b[m\u001b[m/\r\n",
        "\u001b[34mchoose_your_own\u001b[m\u001b[m/    \u001b[34mfeature_selection\u001b[m\u001b[m/  \u001b[34moutliers\u001b[m\u001b[m/           \u001b[34mtext_learning\u001b[m\u001b[m/\r\n",
        "\u001b[34mdatasets_questions\u001b[m\u001b[m/ \u001b[34mfinal_project\u001b[m\u001b[m/      \u001b[34mpca\u001b[m\u001b[m/                \u001b[34mtools\u001b[m\u001b[m/\r\n",
        "\u001b[34mdecision_tree\u001b[m\u001b[m/      \u001b[34mk_means\u001b[m\u001b[m/            \u001b[34mregression\u001b[m\u001b[m/         \u001b[34mvalidation\u001b[m\u001b[m/\r\n"
       ]
      }
     ],
     "prompt_number": 70
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cd tools"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/Users/michaelreinhard/nano/machineLearning/ud120-projects/tools\n"
       ]
      }
     ],
     "prompt_number": 71
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that I am in the tools directory I can maybe use the reload thing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "No combination of relaod(email_preprocess.py)function with our without the % sign. So, now I am on to the next thing. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(features_train[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 76,
       "text": [
        "3785"
       ]
      }
     ],
     "prompt_number": 76
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import email_preprocess"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 78
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from email_preprocess import preprocess"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 79
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reload(email_preprocess)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 80,
       "text": [
        "<module 'email_preprocess' from 'email_preprocess.pyc'>"
       ]
      }
     ],
     "prompt_number": 80
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from email_preprocess import preprocess"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 81
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(features_train[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 82,
       "text": [
        "3785"
       ]
      }
     ],
     "prompt_number": 82
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Lesson 4: Choose Your Own Algorithm**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We choose our own algorithm from among k-nearest neighbors, adaboost, and random forest.\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pwd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "u'/Users/michaelreinhard/nano/machineLearning'"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%cd ud120-projects/choose_your_own/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[Errno 2] No such file or directory: 'ud120-projects/choose_your_own/'\n",
        "/Users/michaelreinhard/nano/machineLearning/ud120-projects/choose_your_own\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%run your_algorithm.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%cat your_algorithm.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%ls"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}