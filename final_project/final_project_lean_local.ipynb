{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the final project with all of the experiments stripped out.\n",
    "\n",
    "### load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.preprocessing import Imputer\n",
    "                                                \n",
    "from pprint import pprint\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import data into Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bonus</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>email_address</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>expenses</th>\n",
       "      <th>from_messages</th>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <th>...</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>other</th>\n",
       "      <th>poi</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>salary</th>\n",
       "      <th>shared_receipt_with_poi</th>\n",
       "      <th>to_messages</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>total_stock_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ALLEN PHILLIP K</th>\n",
       "      <td>4175000</td>\n",
       "      <td>2869717</td>\n",
       "      <td>-3081055</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phillip.allen@enron.com</td>\n",
       "      <td>1729541</td>\n",
       "      <td>13868</td>\n",
       "      <td>2195</td>\n",
       "      <td>47</td>\n",
       "      <td>65</td>\n",
       "      <td>...</td>\n",
       "      <td>304805</td>\n",
       "      <td>152</td>\n",
       "      <td>False</td>\n",
       "      <td>126027</td>\n",
       "      <td>-126027</td>\n",
       "      <td>201955</td>\n",
       "      <td>1407</td>\n",
       "      <td>2902</td>\n",
       "      <td>4484442</td>\n",
       "      <td>1729541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BADUM JAMES P</th>\n",
       "      <td>NaN</td>\n",
       "      <td>178980</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>257817</td>\n",
       "      <td>3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>182466</td>\n",
       "      <td>257817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BANNANTINE JAMES M</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-5104</td>\n",
       "      <td>NaN</td>\n",
       "      <td>james.bannantine@enron.com</td>\n",
       "      <td>4046157</td>\n",
       "      <td>56301</td>\n",
       "      <td>29</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>864523</td>\n",
       "      <td>False</td>\n",
       "      <td>1757552</td>\n",
       "      <td>-560222</td>\n",
       "      <td>477</td>\n",
       "      <td>465</td>\n",
       "      <td>566</td>\n",
       "      <td>916197</td>\n",
       "      <td>5243487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BAXTER JOHN C</th>\n",
       "      <td>1200000</td>\n",
       "      <td>1295738</td>\n",
       "      <td>-1386055</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6680544</td>\n",
       "      <td>11200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1586055</td>\n",
       "      <td>2660303</td>\n",
       "      <td>False</td>\n",
       "      <td>3942714</td>\n",
       "      <td>NaN</td>\n",
       "      <td>267102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5634343</td>\n",
       "      <td>10623258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BAY FRANKLIN R</th>\n",
       "      <td>400000</td>\n",
       "      <td>260455</td>\n",
       "      <td>-201641</td>\n",
       "      <td>NaN</td>\n",
       "      <td>frank.bay@enron.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>129142</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69</td>\n",
       "      <td>False</td>\n",
       "      <td>145796</td>\n",
       "      <td>-82782</td>\n",
       "      <td>239671</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>827696</td>\n",
       "      <td>63014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      bonus deferral_payments deferred_income director_fees  \\\n",
       "ALLEN PHILLIP K     4175000           2869717        -3081055           NaN   \n",
       "BADUM JAMES P           NaN            178980             NaN           NaN   \n",
       "BANNANTINE JAMES M      NaN               NaN           -5104           NaN   \n",
       "BAXTER JOHN C       1200000           1295738        -1386055           NaN   \n",
       "BAY FRANKLIN R       400000            260455         -201641           NaN   \n",
       "\n",
       "                                 email_address exercised_stock_options  \\\n",
       "ALLEN PHILLIP K        phillip.allen@enron.com                 1729541   \n",
       "BADUM JAMES P                              NaN                  257817   \n",
       "BANNANTINE JAMES M  james.bannantine@enron.com                 4046157   \n",
       "BAXTER JOHN C                              NaN                 6680544   \n",
       "BAY FRANKLIN R             frank.bay@enron.com                     NaN   \n",
       "\n",
       "                   expenses from_messages from_poi_to_this_person  \\\n",
       "ALLEN PHILLIP K       13868          2195                      47   \n",
       "BADUM JAMES P          3486           NaN                     NaN   \n",
       "BANNANTINE JAMES M    56301            29                      39   \n",
       "BAXTER JOHN C         11200           NaN                     NaN   \n",
       "BAY FRANKLIN R       129142           NaN                     NaN   \n",
       "\n",
       "                   from_this_person_to_poi        ...         \\\n",
       "ALLEN PHILLIP K                         65        ...          \n",
       "BADUM JAMES P                          NaN        ...          \n",
       "BANNANTINE JAMES M                       0        ...          \n",
       "BAXTER JOHN C                          NaN        ...          \n",
       "BAY FRANKLIN R                         NaN        ...          \n",
       "\n",
       "                   long_term_incentive    other    poi restricted_stock  \\\n",
       "ALLEN PHILLIP K                 304805      152  False           126027   \n",
       "BADUM JAMES P                      NaN      NaN  False              NaN   \n",
       "BANNANTINE JAMES M                 NaN   864523  False          1757552   \n",
       "BAXTER JOHN C                  1586055  2660303  False          3942714   \n",
       "BAY FRANKLIN R                     NaN       69  False           145796   \n",
       "\n",
       "                   restricted_stock_deferred  salary shared_receipt_with_poi  \\\n",
       "ALLEN PHILLIP K                      -126027  201955                    1407   \n",
       "BADUM JAMES P                            NaN     NaN                     NaN   \n",
       "BANNANTINE JAMES M                   -560222     477                     465   \n",
       "BAXTER JOHN C                            NaN  267102                     NaN   \n",
       "BAY FRANKLIN R                        -82782  239671                     NaN   \n",
       "\n",
       "                   to_messages total_payments total_stock_value  \n",
       "ALLEN PHILLIP K           2902        4484442           1729541  \n",
       "BADUM JAMES P              NaN         182466            257817  \n",
       "BANNANTINE JAMES M         566         916197           5243487  \n",
       "BAXTER JOHN C              NaN        5634343          10623258  \n",
       "BAY FRANKLIN R             NaN         827696             63014  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "df = pd.DataFrame(data_dict)\n",
    "df = df.transpose()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Select Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop('email_address', axis=1)\n",
    "df = df.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Eliminate Outliers/Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.drop('TOTAL')\n",
    "df = df.drop(\"THE TRAVEL AGENCY IN THE PARK\")\n",
    "df = df.drop(\"loan_advances\", axis=1)\n",
    "#letting the imputer deal with missing data\n",
    "#df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Create New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['deferred_ratio'] = df['deferred_income']/(df['total_payments'] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sklearn prep: features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_list = list(df.columns)\n",
    "features_list.remove('poi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Try a variety of classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I am going to try the GridSearchCV tutorial with the data in the machine learning class. \n",
    "\n",
    "From page 1, I start with the data in a pandas data frame. \n",
    "\n",
    "We don't have any categorical variables besides the dependent variable so I will not have to use the sklearn.preprocessing.LabelEncoder() or the pandas.get_dummies() functions. \n",
    "\n",
    "The data is in a pandas data frame and has not been split into training or testing data sets. So what we have to do is create the features and labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = df['poi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = df[features_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    cross_validation.train_test_split(features, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search Cross Validation\n",
    "So we make lists of the parameters of the model that we want to search through. Then there are parameters of the GridSearchCV model itself. Then we fit the model, test its predictions and print out the best estimator and its parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, I deleted the grid searches because they were working and I thought I was ready to combine them with pipelines, but now I am messing up how to combine them with pipelines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines\n",
    "The next step is to use all of this grid searching and pipelines working on the Gaussian Naive Bayes model. That will get me the best parameters.\n",
    "\n",
    "First, I look up what the possible parameters are for the GaussianNB. Of course, this will also require that I normalize the data and do the Principle Components analysis on the data before it goes to the model so I will also have to do some data preparation and build a pipeline. \n",
    "\n",
    "The Gaussian Naive Bayes model takes the parameters of--ok, well it doesn't really have any parameters to take. It just wants the normalized data and the principle components if you are doing that. So, we will skip the Gaussian for now and, following the tutorial, add in the pipeline for the decision tree classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so I have gotten rid of all the experiments and tests done using the data with the missing values cleaned by the Pandas functions and am now going to do all the data cleaning in the pipeline so I can compare different imputation strategies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Accuracy Score:\n",
      "0.795454545455\n",
      "\n",
      "Pipeline parameters:\n",
      "{'clf': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=53, splitter='best'),\n",
      " 'clf__class_weight': None,\n",
      " 'clf__criterion': 'gini',\n",
      " 'clf__max_depth': None,\n",
      " 'clf__max_features': None,\n",
      " 'clf__max_leaf_nodes': None,\n",
      " 'clf__min_samples_leaf': 1,\n",
      " 'clf__min_samples_split': 2,\n",
      " 'clf__min_weight_fraction_leaf': 0.0,\n",
      " 'clf__presort': False,\n",
      " 'clf__random_state': 53,\n",
      " 'clf__splitter': 'best',\n",
      " 'imp': Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0),\n",
      " 'imp__axis': 0,\n",
      " 'imp__copy': True,\n",
      " 'imp__missing_values': 'NaN',\n",
      " 'imp__strategy': 'median',\n",
      " 'imp__verbose': 0,\n",
      " 'pca': PCA(copy=True, n_components=None, whiten=False),\n",
      " 'pca__copy': True,\n",
      " 'pca__n_components': None,\n",
      " 'pca__whiten': False,\n",
      " 'std': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
      " 'std__copy': True,\n",
      " 'std__with_mean': True,\n",
      " 'std__with_std': True,\n",
      " 'steps': [('imp',\n",
      "            Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)),\n",
      "           ('std', StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
      "           ('pca', PCA(copy=True, n_components=None, whiten=False)),\n",
      "           ('clf',\n",
      "            DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=53, splitter='best'))]}\n",
      "\n",
      "\n",
      "Recall Score: 0.4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# imp = Imputer(missing_values='NaN', strategy='0')\n",
    "# features_train = imp.fit_transform(features_train)\n",
    "# features_test = imp.fit_transform(features_test)\n",
    "\n",
    "Pipeline = Pipeline([\n",
    "        ('imp', Imputer(missing_values='NaN', strategy='median')),\n",
    "        ('std', StandardScaler()),\n",
    "        ('pca', PCA()),\n",
    "        ('clf', DecisionTreeClassifier(random_state = 53))\n",
    "    ])\n",
    "\n",
    "pipeline = Pipeline.fit(features_train, labels_train)\n",
    "\n",
    "print \"Pipeline Accuracy Score:\"\n",
    "print pipeline.score(features_test, labels_test)\n",
    "\n",
    "print \"\\nPipeline parameters:\"\n",
    "pprint(pipeline.get_params())\n",
    "\n",
    "clf_pipeline_pred = pipeline.predict(features_test)\n",
    "print \"\\n\\nRecall Score:\", recall_score(labels_test, clf_pipeline_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to run the grid search with the pipeline put in as the estimator. The things we want to change in the pipeline will be put in through the grid search. \n",
    "\n",
    "One thing that is a little confusing is that we specify one imputation strategy in in the pipeline and another in the param_grid. I can't do both, can I? Is that part of my problem? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('std', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=None, whiten=False)), ('clf', DecisionTreeClassifier(class_weight=None, criterion='g...lit=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=53, splitter='best'))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'pca__n_components': [2, 3, 4, 5, 6, 7], 'clf__max_depth': [4, 5, 6, 7, 8], 'clf__min_samples_split': [1, 2, 3, 4, 5, 6, 10, 15, 20, 25, 30], 'imp__strategy': ['median', 'most_frequent']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Massive Accuracy Drop, Don't understand missing values\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "Pipeline = Pipeline([\n",
    "        ('imp', Imputer()),\n",
    "        ('std', StandardScaler()),\n",
    "        ('pca', PCA()),\n",
    "        ('clf', DecisionTreeClassifier(random_state = 53))\n",
    "    ])\n",
    "\n",
    "\n",
    "param_grid = {'imp__strategy':['median', 'most_frequent'],\n",
    "              'clf__min_samples_split': [1, 2, 3, 4, 5, 6, 10, 15, 20, 25, 30],\n",
    "              'clf__max_depth': [4, 5, 6, 7, 8],\n",
    "              'pca__n_components': [2,3,4,5,6,7]}\n",
    "\n",
    "#create the GridSearchCV object using param_grid\n",
    "gridCV_object = GridSearchCV(estimator = Pipeline, \n",
    "                                        param_grid = param_grid,\n",
    "                                        cv = 3)\n",
    "\n",
    "gridCV_object.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters from the grid search:{'clf__max_depth': 4,\n",
      " 'clf__min_samples_split': 15,\n",
      " 'imp__strategy': 'most_frequent',\n",
      " 'pca__n_components': 4}\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "print \"Best parameters from the grid search:\", pprint(gridCV_object.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_gridCV = gridCV_object.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best estimator accuracy: 0.840909090909\n"
     ]
    }
   ],
   "source": [
    "print \"\\nBest estimator accuracy:\", clf_gridCV.score(features_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Recall Score: 0.2\n"
     ]
    }
   ],
   "source": [
    "clf_gridCV_pred = clf_gridCV.predict(features_test)\n",
    "print \"\\n\\nRecall Score:\", recall_score(labels_test, clf_gridCV_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the recall score is 0? What did I do wrong? The accuracy is higher but the recall has dropped to 0. That doesn't seem possible. \n",
    "\n",
    "UPDATE: We had the coach chat and then the computer shut down so I lost a lot of what we said, but I think I got a lot of the idea. Now I ran the model again with slight changes. I took cv from 8 down to 3. Seemed to have a big effect. Also, he suggested that we switch off the numbers and use the stratified shuffle split instead. \n",
    "\n",
    "Now I am going to do it again and add the stratified shuffle split and make the search criteria recall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters from the grid search:{'clf__max_depth': 4,\n",
      " 'clf__min_samples_split': 4,\n",
      " 'imp__strategy': 'most_frequent',\n",
      " 'pca__n_components': 4}\n",
      " None\n",
      "\n",
      "Best estimator accuracy: 0.840909090909\n",
      "\n",
      "\n",
      "Recall Score: 0.2\n"
     ]
    }
   ],
   "source": [
    "#Now using stratified shuffle split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "cross_validator = StratifiedShuffleSplit(y = labels_train, random_state = 0)\n",
    "\n",
    "Pipeline = Pipeline([\n",
    "        ('imp', Imputer()),\n",
    "        ('std', StandardScaler()),\n",
    "        ('pca', PCA()),\n",
    "        ('clf', DecisionTreeClassifier(random_state = 53))\n",
    "    ])\n",
    "\n",
    "\n",
    "param_grid = {'imp__strategy':['median', 'most_frequent'],\n",
    "              'clf__min_samples_split': [1, 2, 3, 4, 5, 6, 10, 15, 20, 25, 30],\n",
    "              'clf__max_depth': [4, 5, 6, 7, 8],\n",
    "              'pca__n_components': [2,3,4,5,6,7]}\n",
    "\n",
    "#create the GridSearchCV object using param_grid\n",
    "gridCV_object = GridSearchCV(estimator = Pipeline, \n",
    "                                        param_grid = param_grid,\n",
    "                                        cv = cross_validator)\n",
    "\n",
    "#fit the model\n",
    "gridCV_object.fit(features_train, labels_train)\n",
    "\n",
    "#print the best model's parameters\n",
    "print \"Best parameters from the grid search:\", pprint(gridCV_object.best_params_)\n",
    "\n",
    "#instantiate the best model\n",
    "clf_gridCV = gridCV_object.best_estimator_\n",
    "\n",
    "#print out the best model's accuracy\n",
    "print \"\\nBest estimator accuracy:\", clf_gridCV.score(features_test, labels_test)\n",
    "\n",
    "#get predictions and print out recall score\n",
    "clf_gridCV_pred = clf_gridCV.predict(features_test)\n",
    "print \"\\n\\nRecall Score:\", recall_score(labels_test, clf_gridCV_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add the recall argument for the scoring function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters from the grid search:{'clf__max_depth': 4,\n",
      " 'clf__min_samples_split': 1,\n",
      " 'imp__strategy': 'median',\n",
      " 'pca__n_components': 2}\n",
      " None\n",
      "\n",
      "Best estimator accuracy: 0.840909090909\n",
      "\n",
      "\n",
      "Recall Score: 0.2\n"
     ]
    }
   ],
   "source": [
    "#Now using stratified shuffle split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "cross_validator = StratifiedShuffleSplit(y = labels_train, random_state = 0)\n",
    "\n",
    "Pipeline = Pipeline([\n",
    "        ('imp', Imputer()),\n",
    "        ('std', StandardScaler()),\n",
    "        ('pca', PCA()),\n",
    "        ('clf', DecisionTreeClassifier(random_state = 53))\n",
    "    ])\n",
    "\n",
    "\n",
    "param_grid = {'imp__strategy':['median', 'most_frequent'],\n",
    "              'clf__min_samples_split': [1, 2, 3, 4, 5, 6, 10, 15, 20, 25, 30],\n",
    "              'clf__max_depth': [4, 5, 6, 7, 8],\n",
    "              'pca__n_components': [2,3,4,5,6,7]}\n",
    "\n",
    "#create the GridSearchCV object using param_grid\n",
    "gridCV_object = GridSearchCV(estimator = Pipeline, \n",
    "                                        param_grid = param_grid,\n",
    "                                        scoring = 'recall',\n",
    "                                        cv = cross_validator)\n",
    "\n",
    "#fit the model\n",
    "gridCV_object.fit(features_train, labels_train)\n",
    "\n",
    "#print the best model's parameters\n",
    "print \"Best parameters from the grid search:\", pprint(gridCV_object.best_params_)\n",
    "\n",
    "#instantiate the best model\n",
    "clf_gridCV = gridCV_object.best_estimator_\n",
    "\n",
    "#print out the best model's accuracy\n",
    "print \"\\nBest estimator accuracy:\", clf_gridCV.score(features_test, labels_test)\n",
    "\n",
    "#get predictions and print out recall score\n",
    "clf_gridCV_pred = clf_gridCV.predict(features_test)\n",
    "print \"\\n\\nRecall Score:\", recall_score(labels_test, clf_gridCV_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can switch out the tree classifier for the random forest classifier. And we add in the 'clf__n_estimators': [2,5,10,15] which is a key parameter for the random forest model. I have also removed the min_samples_split parameter from the param_grid to speed things up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters from the grid search:{'clf__max_depth': 4,\n",
      " 'clf__n_estimators': 2,\n",
      " 'imp__strategy': 'most_frequent',\n",
      " 'pca__n_components': 6}\n",
      " None\n",
      "\n",
      "Best estimator accuracy: 0.863636363636\n",
      "\n",
      "\n",
      "Recall Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "#Now using stratified shuffle split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "cross_validator = StratifiedShuffleSplit(y = labels_train, random_state = 0)\n",
    "\n",
    "Pipeline = Pipeline([\n",
    "        ('imp', Imputer()),\n",
    "        ('std', StandardScaler()),\n",
    "        ('pca', PCA()),\n",
    "        ('clf', RandomForestClassifier(random_state = 53))\n",
    "    ])\n",
    "\n",
    "\n",
    "param_grid = {'imp__strategy':['median', 'most_frequent'],\n",
    "              #'clf__min_samples_split': [2, 3, 4, 5, 6, 10, 15, 20],\n",
    "              'clf__max_depth': [4, 5, 6, 7, 8],\n",
    "              'clf__n_estimators': [2,5,10,15],\n",
    "              'pca__n_components': [2,3,4,5,6,7]}\n",
    "\n",
    "#create the GridSearchCV object using param_grid\n",
    "gridCV_object = GridSearchCV(estimator = Pipeline, \n",
    "                                        param_grid = param_grid,\n",
    "                                        scoring = 'recall',\n",
    "                                        cv = cross_validator)\n",
    "\n",
    "#fit the model\n",
    "gridCV_object.fit(features_train, labels_train)\n",
    "\n",
    "#print the best model's parameters\n",
    "print \"Best parameters from the grid search:\", pprint(gridCV_object.best_params_)\n",
    "\n",
    "#instantiate the best model\n",
    "clf_gridCV = gridCV_object.best_estimator_\n",
    "\n",
    "#print out the best model's accuracy\n",
    "print \"\\nBest estimator accuracy:\", clf_gridCV.score(features_test, labels_test)\n",
    "\n",
    "#get predictions and print out recall score\n",
    "clf_gridCV_pred = clf_gridCV.predict(features_test)\n",
    "print \"\\n\\nRecall Score:\", recall_score(labels_test, clf_gridCV_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the accuracy has gone up but the recall has gone to zero. That seems almost unbelievable. After all, I have made recall the scoring function. \n",
    "\n",
    "Max depth has was very low at 4 so maybe we should truncate the parameter list there. And we should also reduce the number of estimators we look through. \n",
    "\n",
    "For some reason the imputation strategy has flipped over to 'most_frequent' rather than 'median'. I have no idea why. Anyway, the next step is to do what? I have lost the recall that I had. Where do I go from here? \n",
    "\n",
    "I am going to give up on the random forest classifiers for now and go to the Gaussian model that did well for me before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters from the grid search:{'imp__strategy': 'median', 'pca__n_components': 10}\n",
      " None\n",
      "\n",
      "Best estimator accuracy: 0.818181818182\n",
      "\n",
      "\n",
      "Recall Score: 0.2\n"
     ]
    }
   ],
   "source": [
    "#Now using stratified shuffle split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "cross_validator = StratifiedShuffleSplit(y = labels_train, random_state = 0)\n",
    "\n",
    "Pipeline = Pipeline([\n",
    "        ('imp', Imputer()),\n",
    "        ('std', StandardScaler()),\n",
    "        ('pca', PCA()),\n",
    "        ('clf', GaussianNB())\n",
    "    ])\n",
    "\n",
    "\n",
    "param_grid = {'imp__strategy':['median', 'most_frequent'],\n",
    "              #'clf__min_samples_split': [2, 3, 4, 5, 6, 10, 15, 20],\n",
    "              #'clf__max_depth': [4, 5, 6, 7, 8],\n",
    "              #'clf__n_estimators': [2,5,10,15],\n",
    "              'pca__n_components': [2,3,4,5,6,7]}\n",
    "\n",
    "#create the GridSearchCV object using param_grid\n",
    "gridCV_object = GridSearchCV(estimator = Pipeline, \n",
    "                                        param_grid = param_grid,\n",
    "                                        scoring = 'recall',\n",
    "                                        cv = cross_validator)\n",
    "\n",
    "#fit the model\n",
    "gridCV_object.fit(features_train, labels_train)\n",
    "\n",
    "#print the best model's parameters\n",
    "print \"Best parameters from the grid search:\", pprint(gridCV_object.best_params_)\n",
    "\n",
    "#instantiate the best model\n",
    "clf_gridCV = gridCV_object.best_estimator_\n",
    "\n",
    "#print out the best model's accuracy\n",
    "print \"\\nBest estimator accuracy:\", clf_gridCV.score(features_test, labels_test)\n",
    "\n",
    "#get predictions and print out recall score\n",
    "clf_gridCV_pred = clf_gridCV.predict(features_test)\n",
    "print \"\\n\\nRecall Score:\", recall_score(labels_test, clf_gridCV_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I am going to try to increase the number of dimensions we look through to 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters from the grid search:{'imp__strategy': 'median', 'pca__n_components': 10}\n",
      " None\n",
      "\n",
      "Best estimator accuracy: 0.818181818182\n",
      "\n",
      "\n",
      "Recall Score: 0.2\n"
     ]
    }
   ],
   "source": [
    "#Now using stratified shuffle split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "cross_validator = StratifiedShuffleSplit(y = labels_train, random_state = 0)\n",
    "\n",
    "Pipeline = Pipeline([\n",
    "        ('imp', Imputer()),\n",
    "        ('std', StandardScaler()),\n",
    "        ('pca', PCA()),\n",
    "        ('clf', GaussianNB())\n",
    "    ])\n",
    "\n",
    "\n",
    "param_grid = {'imp__strategy':['median', 'most_frequent'],\n",
    "              #'clf__min_samples_split': [2, 3, 4, 5, 6, 10, 15, 20],\n",
    "              #'clf__max_depth': [4, 5, 6, 7, 8],\n",
    "              #'clf__n_estimators': [2,5,10,15],\n",
    "              'pca__n_components': [2,3,4,5,6,7,8,9,10,11,12,13,14,15]}\n",
    "\n",
    "#create the GridSearchCV object using param_grid\n",
    "gridCV_object = GridSearchCV(estimator = Pipeline, \n",
    "                                        param_grid = param_grid,\n",
    "                                        scoring = 'recall',\n",
    "                                        cv = cross_validator)\n",
    "\n",
    "#fit the model\n",
    "gridCV_object.fit(features_train, labels_train)\n",
    "\n",
    "#print the best model's parameters\n",
    "print \"Best parameters from the grid search:\", pprint(gridCV_object.best_params_)\n",
    "\n",
    "#instantiate the best model\n",
    "clf_gridCV = gridCV_object.best_estimator_\n",
    "\n",
    "#print out the best model's accuracy\n",
    "print \"\\nBest estimator accuracy:\", clf_gridCV.score(features_test, labels_test)\n",
    "\n",
    "#get predictions and print out recall score\n",
    "clf_gridCV_pred = clf_gridCV.predict(features_test)\n",
    "print \"\\n\\nRecall Score:\", recall_score(labels_test, clf_gridCV_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, Just going to do 12 components to search through. Also, I am reporting both the recall and the precision on this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters from the grid search:{'imp__strategy': 'median', 'pca__n_components': 10}\n",
      " None\n",
      "\n",
      "Best estimator accuracy: 0.818181818182\n",
      "\n",
      "\n",
      "Recall Score: 0.2\n",
      "\n",
      "\n",
      "Precision Score: 0.2\n"
     ]
    }
   ],
   "source": [
    "#Now using stratified shuffle split and the GaussianNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "cross_validator = StratifiedShuffleSplit(y = labels_train, random_state = 0)\n",
    "\n",
    "Pipeline = Pipeline([\n",
    "        ('imp', Imputer()),\n",
    "        ('std', StandardScaler()),\n",
    "        ('pca', PCA()),\n",
    "        ('clf', GaussianNB())\n",
    "    ])\n",
    "\n",
    "\n",
    "param_grid = {'imp__strategy':['median', 'most_frequent'],\n",
    "              #'clf__min_samples_split': [2, 3, 4, 5, 6, 10, 15, 20],\n",
    "              #'clf__max_depth': [4, 5, 6, 7, 8],\n",
    "              #'clf__n_estimators': [2,5,10,15],\n",
    "              'pca__n_components': [2,3,4,5,6,7,8,9,10,11,12]}\n",
    "\n",
    "#create the GridSearchCV object using param_grid\n",
    "gridCV_object = GridSearchCV(estimator = Pipeline, \n",
    "                                        param_grid = param_grid,\n",
    "                                        scoring = 'recall',\n",
    "                                        cv = cross_validator)\n",
    "\n",
    "#fit the model\n",
    "gridCV_object.fit(features_train, labels_train)\n",
    "\n",
    "#print the best model's parameters\n",
    "print \"Best parameters from the grid search:\", pprint(gridCV_object.best_params_)\n",
    "\n",
    "#instantiate the best model\n",
    "clf_gridCV = gridCV_object.best_estimator_\n",
    "\n",
    "#print out the best model's accuracy\n",
    "print \"\\nBest estimator accuracy:\", clf_gridCV.score(features_test, labels_test)\n",
    "\n",
    "#get predictions and print out recall score\n",
    "clf_gridCV_pred = clf_gridCV.predict(features_test)\n",
    "print \"\\n\\nRecall Score:\", recall_score(labels_test, clf_gridCV_pred)\n",
    "print \"\\n\\nPrecision Score:\", precision_score(labels_test, clf_gridCV_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look at the preformance of this model. Now that we have fit the model we look at how well it does. First we print out the parameters of the model that was chosen by grid search as the best one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Tune precision and recall > 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardize by subtracting mean and dividing by std_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "# Create a minimum and maximum processor object\n",
    "standard_scaler = preprocessing.StandardScaler()\n",
    "\n",
    "# Create an object to transform the data to fit minmax processor\n",
    "features_train = standard_scaler.fit_transform(features_train)\n",
    "features_test = standard_scaler.fit_transform(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I am going to use the select k best method on the data and then see how it does with the models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "COACH QUESTION: Where do I do the k-best thing? Can it be added to a pipeline below? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB()\n",
      "\tAccuracy: 0.61633\tPrecision: 0.21514\tRecall: 0.70900\tF1: 0.33011\tF2: 0.48592\n",
      "\tTotal predictions: 15000\tTrue positives: 1418\tFalse positives: 5173\tFalse negatives:  582\tTrue negatives: 7827\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "test_classifier(clf, my_dataset, features_list, folds = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COACH QUESTION: Why doesn't the rbf kernel work? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got a divide by zero when trying out: SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Precision or recall may be undefined due to a lack of true positive predicitons.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(kernel='rbf',C=100) \n",
    "# tried gamma=0.001, C=100.\n",
    "# kernel = 'linear', kernel='rbf'\n",
    "test_classifier(clf, my_dataset, features_list, folds=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got a divide by zero when trying out: Pipeline(steps=[('reduce_dim', PCA(copy=True, n_components=7, whiten=False)), ('svm', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "Precision or recall may be undefined due to a lack of true positive predicitons.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import PCA\n",
    "estimators = [('reduce_dim', PCA(n_components=7)), ('svm', svm.SVC(kernel='rbf'))]\n",
    "clf = Pipeline(estimators)\n",
    "test_classifier(clf, my_dataset, features_list, folds = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Winner\n",
    "COACH QUESTION: So the Guassian Naive Bayes with the dimensions of the data set reduced to 8 through prinicple component analysis is the one that I got to break the 0.3 thresholds in Recall and Precision. I did it by just doing a manual search through the possible number of components. I have no idea which components were chosen and on what basis they worked, which 8 worked better than 9 or why 7 didn't make the 0.3 cut at all. I have no intuition for why any of this worked. And I don't know how to visualize it. Can I just submit this for the final project? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('reduce_dim', PCA(copy=True, n_components=8, whiten=False)), ('Gaussian', GaussianNB())])\n",
      "\tAccuracy: 0.85053\tPrecision: 0.42568\tRecall: 0.34650\tF1: 0.38203\tF2: 0.35989\n",
      "\tTotal predictions: 15000\tTrue positives:  693\tFalse positives:  935\tFalse negatives: 1307\tTrue negatives: 12065\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.decomposition import PCA\n",
    "estimators = [('reduce_dim', PCA(n_components=8)), ('Gaussian', GaussianNB())]\n",
    "clf = Pipeline(estimators)\n",
    "test_classifier(clf, my_dataset, features_list, folds = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
