{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final project restart\n",
    "Import the modules, assign variables and define functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "                                                \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from tester import test_classifier\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "df = pd.DataFrame(data_dict)\n",
    "df = df.transpose()\n",
    "df = df.drop('email_address', axis=1)\n",
    "df = df.astype(float)\n",
    "df = df.drop('TOTAL')\n",
    "df = df.drop(\"THE TRAVEL AGENCY IN THE PARK\")\n",
    "df = df.drop(\"loan_advances\", axis=1)\n",
    "features_list = list(df.columns)\n",
    "features_list.remove('poi')\n",
    "features = df[features_list]\n",
    "labels = df['poi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    cross_validation.train_test_split(features, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Pipeline: median imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier\n",
      "Imputer: median\n",
      "Normalize: StandardScaler\n",
      "PCA: dimensions unspecified\n",
      "\n",
      "Accuracy Score: 0.863636363636\n",
      "\n",
      "Recall: 0.6\n",
      "\n",
      "Precision: 0.428571428571\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "Pipeline = Pipeline([\n",
    "        ('imp', Imputer(missing_values='NaN', strategy='median')),\n",
    "        ('std', StandardScaler()),\n",
    "        ('pca', PCA()),\n",
    "        ('clf', DecisionTreeClassifier(random_state = 53))\n",
    "    ])\n",
    "\n",
    "pipeline = Pipeline.fit(features_train, labels_train)\n",
    "\n",
    "print \"\"\"DecisionTreeClassifier\\nImputer: median\\nNormalize: StandardScaler\\nPCA: dimensions unspecified\"\"\"\n",
    "print \"\\nAccuracy Score:\", pipeline.score(features_test, labels_test)\n",
    "\n",
    "clf_pipeline_pred = pipeline.predict(features_test)\n",
    "print \"\\nRecall:\", recall_score(labels_test, clf_pipeline_pred)\n",
    "print \"\\nPrecision:\", precision_score(labels_test, clf_pipeline_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Feature-Data Reimport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "df = pd.DataFrame(data_dict)\n",
    "df = df.transpose()\n",
    "df = df.drop('email_address', axis=1)\n",
    "df = df.astype(float)\n",
    "df = df.drop('TOTAL')\n",
    "df = df.drop(\"THE TRAVEL AGENCY IN THE PARK\")\n",
    "df = df.drop(\"loan_advances\", axis=1)\n",
    "#new feature\n",
    "df['deferred_ratio'] = df['deferred_income']/(df['total_payments'] + 1)\n",
    "features_list = list(df.columns)\n",
    "features_list.remove('poi')\n",
    "features = df[features_list]\n",
    "labels = df['poi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = \"final_project_dataset.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def import_data(data):\n",
    "    '''This are the things I will do to import the data everytime, \n",
    "    regardless of what variables I make.'''\n",
    "    with open(data, \"r\") as data_file:\n",
    "        data_dict = pickle.load(data_file)\n",
    "    df = pd.DataFrame(data_dict)\n",
    "    df = df.transpose()\n",
    "    df = df.drop('email_address', axis=1)\n",
    "    df = df.astype(float)\n",
    "    df = df.drop('TOTAL')\n",
    "    df = df.drop(\"THE TRAVEL AGENCY IN THE PARK\")\n",
    "    df = df.drop(\"loan_advances\", axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = import_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features_labels(df):\n",
    "    '''This is where the features and labels are extracted to use as arguments\n",
    "    for sklearn\\'s cross_validation function. It is also where I will do add\n",
    "    any new variables.'''\n",
    "    df['deferred_ratio'] = df['deferred_income']/(df['total_payments'] + 1)\n",
    "    df['pct_from_poi'] = df['from_poi_to_this_person']/(df['from_messages'] + 1)\n",
    "    df['pct_to_poi'] = df['from_this_person_to_poi']/(df['from_messages'] + 1)\n",
    "    df['to_from'] = df['pct_from_poi']*df['pct_from_poi']\n",
    "    features_list = list(df.columns)\n",
    "    features_list.remove('poi')\n",
    "    features = df[features_list]\n",
    "    labels = df['poi']\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features, labels = get_features_labels(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_outcomes(grid_object):\n",
    "    '''Gets the print out of all the outcomes from the grid_search. It prints out the \n",
    "    best parameters found by the model and the outcomes of the test of the model on \n",
    "    the test set.'''\n",
    "    print \"Best parameters from the grid search:\", pprint(gridCV_object.best_params_)\n",
    "    clf_gridCV = gridCV_object.best_estimator_\n",
    "    print \"\\nBest Estimator Accuracy:\", clf_gridCV.score(features_test, labels_test)\n",
    "    clf_gridCV_pred = clf_gridCV.predict(features_test)\n",
    "    print \"\\n\\nRecall Score:\", recall_score(labels_test, clf_gridCV_pred)\n",
    "    print \"\\n\\nPrecision Score:\", precision_score(labels_test, clf_gridCV_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    cross_validation.train_test_split(features, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters from the grid search:{'clf__max_depth': 7, 'imp__strategy': 'most_frequent', 'pca__n_components': 8}\n",
      " None\n",
      "\n",
      "Best Estimator Accuracy: 0.886363636364\n",
      "\n",
      "\n",
      "Recall Score: 0.2\n",
      "\n",
      "\n",
      "Precision Score: 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "Pipeline = Pipeline([\n",
    "        ('imp', Imputer(missing_values='NaN')),\n",
    "        ('std', StandardScaler()),\n",
    "        ('pca', PCA()),\n",
    "        ('clf', DecisionTreeClassifier(random_state = 53))\n",
    "    ])\n",
    "\n",
    "x = [x for x in range(2,19)]\n",
    "d = [d for d in range(6,12)]\n",
    "param_grid = {'pca__n_components': x,\n",
    "              'clf__max_depth': d,\n",
    "              'imp__strategy':['most_frequent','median']}\n",
    "\n",
    "gridCV_object = GridSearchCV(estimator = Pipeline, \n",
    "                             param_grid = param_grid,\n",
    "                             scoring = \"f1\",\n",
    "                             cv = StratifiedShuffleSplit(labels_train, n_iter=1000))\n",
    "\n",
    "gridCV_object.fit(features_train, labels_train)\n",
    "\n",
    "# print all the outcomes of interest\n",
    "get_outcomes(gridCV_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters from the grid search:{'clf__max_depth': 8, 'pca__n_components': 7}\n",
      " None\n",
      "\n",
      "Best Estimator Accuracy: 0.863636363636\n",
      "\n",
      "\n",
      "Recall Score: 0.2\n",
      "\n",
      "\n",
      "Precision Score: 0.333333333333\n"
     ]
    }
   ],
   "source": [
    "get_outcomes(gridCV_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results with 1000, 10 and 100 iterations\n",
    "\n",
    "### 1000 iterations\n",
    "So the first outcome from the model is pretty good. It is StratifiedShuffleSplit with 1000 iterations. The outcome was really good but it took forever. \n",
    "\n",
    "```\n",
    "Best parameters from the grid search:{'clf__max_depth': 9, 'pca__n_components': 10}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.886363636364\n",
    "\n",
    "\n",
    "Recall Score: 0.6\n",
    "\n",
    "\n",
    "Precision Score: 0.5\n",
    "```\n",
    "### 10 iterations\n",
    "This is great but I just can't use this for testing things out. It will take forever. So, I am going to try it with 10 iterations and see what happens. \n",
    "\n",
    "```\n",
    "\n",
    "Best parameters from the grid search:{'clf__max_depth': 9, 'pca__n_components': 12}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.863636363636\n",
    "\n",
    "\n",
    "Recall Score: 0.4\n",
    "\n",
    "\n",
    "Precision Score: 0.4\n",
    "```\n",
    "### 100\n",
    "Now we get the same depth but 12 instead of 10 principle components. The scores have gone down though. It is worth trying it at a higher number of iterations. I am going to try 100. Here is the outcome: \n",
    "```\n",
    "Best Estimator Accuracy: 0.818181818182\n",
    "\n",
    "\n",
    "Recall Score: 0.4\n",
    "\n",
    "\n",
    "Precision Score: 0.285714285714\n",
    "```\n",
    "Ok, so 100 iterations is worse than either 1000 or 10 iterations. That is kind of distressing. It would be nice if the behavior of the model was 'monotonic', that is, the more of one thing you do the more of something you are looking for you get. I was thinking that 100 iterations could serve as a good way to explore possibilities and narrow the search space while reserving 1000 iteration runs to make the final cut. Now I am not quite sure what to do. \n",
    "\n",
    "And here is another thing. I just ran the model again with 1000 iterations with the single addition of the clf's criterion being 'gini' or 'entropy'. It came back with gini as the better criterion. And the model had performance that was just as good as before on the 1000 iterations. In fact, the three scores of interest--accuracy, precision and recall--were exactly the same. But it found a max depth of 7 instead of 9. \n",
    "```\n",
    "Best parameters from the grid search:{'clf__criterion': 'gini', 'clf__max_depth': 7, 'pca__n_components': 10}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.886363636364\n",
    "\n",
    "\n",
    "Recall Score: 0.6\n",
    "\n",
    "\n",
    "Precision Score: 0.5\n",
    "```\n",
    "The reason that is so puzzling to me is that gini is the default scoring criterion for the model. So it was using gini before when it found the best preformance was at max depth of 9 instead of 7. How does that happen?\n",
    "\n",
    "### Adding variables\n",
    "\n",
    "In this iteration I try adding some new variables and the choice between the median and most_frequent imputation for the missing values. I also limited the search space for 'max_depth' and the number of components to 6-8 and 8-12 respectively. That may have been a mistake because I had found that the most effective max depth was 9. I don't know why I did that. \n",
    "```\n",
    "\n",
    "Best parameters from the grid search:{'clf__max_depth': 7, 'imp__strategy': 'most_frequent', 'pca__n_components': 8}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.886363636364\n",
    "\n",
    "\n",
    "Recall Score: 0.2\n",
    "\n",
    "\n",
    "Precision Score: 0.5\n",
    "```\n",
    "The results are strange in that adding more variables decreased the preformance, but I can't be sure that it wasn't because of my odd choice of cutting off the max_depth at 8 instead of 9. \n",
    "\n",
    "Also, I don't know why the max_depth is a useful parameter since the default is None. How could limiting the parameter improve preformance? \n",
    "\n",
    "## New Variables with modified parameter search space\n",
    "\n",
    "Just to satisfy my curiosity I am going to increase the max_depth parameter to 10 and raise the maximum number of principle components to 16 to account for the four newly added variables.\n",
    "\n",
    "#### Civil Libertarian Improvement\n",
    "\n",
    "So this turns out to have been a good idea. I have had a slight loss in recall but big gains in accuracy and precision. Given that I am a civil libertarian I personally feel this is an improvement. \n",
    "\n",
    "```\n",
    "\n",
    "Best parameters from the grid search:{'clf__max_depth': 8, 'imp__strategy': 'most_frequent', 'pca__n_components': 8}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.909090909091\n",
    "\n",
    "\n",
    "Recall Score: 0.4\n",
    "\n",
    "\n",
    "Precision Score: 0.666666666667\n",
    "```\n",
    "Now I have one problem with these results in that the number of principle components chosen was at the bottom of the range I had specified. So now I have to face the possibility that there was better preforming model with a smaller dimensional principle component space that was over looked by the paramters to which I limited the grid search. So, as a double check, I am going to let the space searced go down. I am also going to keep the 'most_frequent' strategy and not search that space anymore. \n",
    "\n",
    "### Fewer dimensions: Insanity\n",
    "\n",
    "Ok, this is the kind of insane behavior that is driving me insane. \n",
    "```\n",
    "\n",
    "Best parameters from the grid search:{'clf__max_depth': 8, 'pca__n_components': 7}\n",
    " None\n",
    "\n",
    "Best Estimator Accuracy: 0.863636363636\n",
    "\n",
    "\n",
    "Recall Score: 0.2\n",
    "\n",
    "\n",
    "Precision Score: 0.333333333333\n",
    "```\n",
    "Now, all I did was specify 'most_frequent' as the method of imputation and offered it the choice of finding few dimensions in the principle component analysis. Everything else was the same. So how could the model get worse? It had the choice of keeping the model that had preformed better in the last specificaiton of the model, so how could it get worse? \n",
    "## Showdown\n",
    "Ok, I am just going to test everything. I am going to have a model of 200 different possible parameter configurations, but I am going to know once and for all what's what. Since I have never had entropy come up as the better scoring criterion I am going to leave that out but I am going to try everything else I have tried with the broades possible ranges. \n",
    "\n",
    "I am doing this essentially because I have to know once and for all whether more variables improve things or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "df = pd.DataFrame(data_dict)\n",
    "df = df.transpose()\n",
    "df = df.drop('email_address', axis=1)\n",
    "df = df.astype(float)\n",
    "df = df.drop('TOTAL')\n",
    "df = df.drop(\"THE TRAVEL AGENCY IN THE PARK\")\n",
    "df = df.drop(\"loan_advances\", axis=1)\n",
    "#new feature\n",
    "# df['deferred_ratio'] = df['deferred_income']/(df['total_payments'] + 1)\n",
    "features_list = list(df.columns)\n",
    "features_list.remove('poi')\n",
    "features = df[features_list]\n",
    "labels = df['poi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    cross_validation.train_test_split(features, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier\n",
      "Imputer: median\n",
      "Normalize: StandardScaler\n",
      "PCA: dimensions unspecified\n",
      "\n",
      "Accuracy Score: 0.863636363636\n",
      "\n",
      "Recall: 0.6\n",
      "\n",
      "Precision: 0.428571428571\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "Pipeline = Pipeline([\n",
    "        ('imp', Imputer(missing_values='NaN', strategy='median')),\n",
    "        ('std', StandardScaler()),\n",
    "        ('pca', PCA()),\n",
    "        ('clf', DecisionTreeClassifier(random_state = 53))\n",
    "    ])\n",
    "\n",
    "pipeline = Pipeline.fit(features_train, labels_train)\n",
    "\n",
    "print \"\"\"DecisionTreeClassifier\\nImputer: median\\nNormalize: StandardScaler\\nPCA: dimensions unspecified\"\"\"\n",
    "print \"\\nAccuracy Score:\", pipeline.score(features_test, labels_test)\n",
    "\n",
    "clf_pipeline_pred = pipeline.predict(features_test)\n",
    "print \"\\nRecall:\", recall_score(labels_test, clf_pipeline_pred)\n",
    "print \"\\nPrecision:\", precision_score(labels_test, clf_pipeline_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so now I can get the thing to work. The best thing so far is using pca with no actual reduction in the number of dimensions and no new features added. Now I am going to try to submit that to the udacity grader. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Submitting to the Udacity Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "df = pd.DataFrame(data_dict)\n",
    "df = df.transpose()\n",
    "df = df.drop('email_address', axis=1)\n",
    "df = df.astype(float)\n",
    "df = df.drop('TOTAL')\n",
    "df = df.drop(\"THE TRAVEL AGENCY IN THE PARK\")\n",
    "df = df.drop(\"loan_advances\", axis=1)\n",
    "features_list = list(df.columns)\n",
    "features_list.remove('poi')\n",
    "features = df[features_list]\n",
    "labels = df['poi']\n",
    "\n",
    "from sklearn import cross_validation\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    cross_validation.train_test_split(features, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier\n",
      "Imputer: median\n",
      "Normalize: StandardScaler\n",
      "PCA: dimensions unspecified\n",
      "\n",
      "Accuracy Score: 0.863636363636\n",
      "\n",
      "Recall: 0.6\n",
      "\n",
      "Precision: 0.428571428571\n"
     ]
    }
   ],
   "source": [
    "from tester import test_classifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "Pipeline = Pipeline([\n",
    "        ('imp', Imputer(missing_values='NaN', strategy='median')),\n",
    "        ('std', StandardScaler()),\n",
    "        ('pca', PCA()),\n",
    "        ('clf', DecisionTreeClassifier(random_state = 53))\n",
    "    ])\n",
    "\n",
    "pipeline = Pipeline.fit(features_train, labels_train)\n",
    "\n",
    "print \"\"\"DecisionTreeClassifier\\nImputer: median\\nNormalize: StandardScaler\\nPCA: dimensions unspecified\"\"\"\n",
    "print \"\\nAccuracy Score:\", pipeline.score(features_test, labels_test)\n",
    "\n",
    "clf_pipeline_pred = pipeline.predict(features_test)\n",
    "\n",
    "print \"\\nRecall:\", recall_score(labels_test, clf_pipeline_pred)\n",
    "print \"\\nPrecision:\", precision_score(labels_test, clf_pipeline_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra data prep to give to the grader. \n",
    "Put 'poi' at the top of the features_list and put the whole pandas data frame into a dictionary called 'data_dict'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_list[0] = 'poi'\n",
    "df_1 = df.transpose()\n",
    "data_dict = df_1.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Same model submitted to the grader: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "And these are the results going through the test classifier:\n",
      "\n",
      "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('std', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=None, whiten=False)), ('clf', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=N...plit=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=53, splitter='best'))])\n",
      "\tAccuracy: 0.79173\tPrecision: 0.27005\tRecall: 0.33000\tF1: 0.29703\tF2: 0.31597\n",
      "\tTotal predictions: 15000\tTrue positives:  660\tFalse positives: 1784\tFalse negatives: 1340\tTrue negatives: 11216\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tester import test_classifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "clf = Pipeline([\n",
    "        ('imp', Imputer(missing_values='NaN', strategy='median')),\n",
    "        ('std', StandardScaler()),\n",
    "        ('pca', PCA()),\n",
    "        ('clf', DecisionTreeClassifier(random_state = 53))\n",
    "    ])\n",
    "\n",
    "print \"\\n\\nAnd these are the results going through the test classifier:\\n\"\n",
    "test_classifier(clf, data_dict, features_list, folds = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
